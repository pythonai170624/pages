## 🧠 Logistic Regression – מציאת β₀ ו־β₁ בשלבים

### שלב 1: נגדיר את הבעיה
יש לנו נתונים (X, Y), כאשר:
- X הוא משתנה רציף (למשל: שעות למידה)
- Y הוא בינארי: 0 או 1 (למשל: כישלון או הצלחה)

המטרה שלנו:
> לחשב את ההסתברות ש־Y יהיה 1 בהינתן X.

---

### שלב 2: נגדיר את הפונקציה הלוגיסטית

זו הפונקציה שאיתה נבנה את המודל:

$$
P(Y = 1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

היא תמיד מחזירה ערכים בין 0 ל־1 – מושלם להסתברויות.

---

### שלב 3: נבנה את פונקציית ההסתברות הכוללת (Likelihood)

כדי לבדוק עד כמה המודל שלנו "מדויק", נחשב את ההסתברות שהוא חוזה נכון את כל הנתונים:

$$
L(\beta_0, \beta_1) = \prod_{i=1}^{n} \left( \hat{p}_i \right)^{y_i} \cdot \left(1 - \hat{p}_i \right)^{1 - y_i}
$$

כאשר:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

**🎯 דוגמה לחישוב פונקציית Likelihood**

נניח שיש לנו שתי דוגמאות בלבד:

- דוגמה ראשונה: x = 1, y = 1  
- דוגמה שנייה: x = 2, y = 0

נבחר מקדמים למודל:
- β₀ = 0  
- β₁ = 1

**שלב א: נחשב את ההסתברות (p-hat) שהמודל חוזה**

הנוסחה היא:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

##### עבור הדוגמה הראשונה (x = 1):

$$
\hat{p}_1 = \frac{1}{1 + e^{-(0 + 1 \cdot 1)}} = \frac{1}{1 + e^{-1}} \approx 0.731
$$

##### עבור הדוגמה השנייה (x = 2):

$$
\hat{p}_2 = \frac{1}{1 + e^{-(0 + 1 \cdot 2)}} = \frac{1}{1 + e^{-2}} \approx 0.881
$$

אבל – בגלל ש־y = 0 במקרה הזה, נשתמש ב־(1 − p̂):

$$
1 - \hat{p}_2 \approx 1 - 0.881 = 0.119
$$


**שלב ב: נחשב את פונקציית ה־Likelihood הכוללת**

נשתמש בנוסחה:

$$
L = (\hat{p}_1)^{y_1} \cdot (1 - \hat{p}_2)^{1 - y_2}
$$

נציב את הערכים:

$$
L = 0.731 \cdot 0.119 \approx 0.087
$$


#### 🧠 מה זה אומר?

- המודל חזה טוב יחסית בדוגמה הראשונה (73% עבור y=1)
- אבל טעה בדוגמה השנייה — חזה הסתברות גבוהה ל־y=1, למרות שהתשובה הייתה y=0
- לכן, פונקציית ה־Likelihood הסופית היא בערך 8.7%

המסקנה: נרצה למצוא β₀ ו־β₁ אחרים שייתנו ערך גבוה יותר ל־L.


---

### שלב 4: ניקח לוגריתם של פונקציית הסבירות

כדי לפשט את החישוב, לוקחים את הלוגריתם של פונקציית הסבירות (log-likelihood):

$$
\log L(\beta_0, \beta_1) = \sum_{i=1}^{n} \left[ y_i \cdot \log(\hat{p}_i) + (1 - y_i) \cdot \log(1 - \hat{p}_i) \right]
$$

---

### שלב 5: נגדיר את בעיית האופטימיזציה

המטרה היא **למקסם** את פונקציית הלוג-לייקלי-הוד:

$$
\max_{\beta_0, \beta_1} \ \log L(\beta_0, \beta_1)
$$

---

### שלב 6: נחשב נגזרות ונחפש מקסימום

- מחשבים נגזרות של הפונקציה לפי β₀ ו־β₁
- משווים ל־0 כדי למצוא נקודת קיצון
- אבל... אי אפשר לפתור את זה ידנית כמו ברגרסיה רגילה

#### מה המטרה?
אנחנו רוצים למצוא את הערכים של לפי β₀ ו־β₁ שמביאים את פונקציית הלוג־לייקלי למקסימום

לפי מתמטיקה:
- נקודת מקסימום נמצאת כאשר **הנגזרת = 0**
- כלומר: הפונקציה לא עולה ולא יורדת – זו פסגה (או תחתית, אבל כאן מדובר על מקסימום)

#### מהי הפונקציה שלנו?

זוהי פונקציית **הלוג־לייקלי** (log-likelihood):

$$
\log L(\beta_0, \beta_1) = \sum_{i=1}^{n} \left[ y_i \cdot \log(\hat{p}_i) + (1 - y_i) \cdot \log(1 - \hat{p}_i) \right]
$$

כאשר:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

#### נגזרות:

הנגזרות של הפונקציה לפי β₀ ו־β₁ הן:

- לפי $$\beta_1$$:

$$
\frac{\partial}{\partial \beta_1} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i) \cdot x_i
$$

- לפי $$\beta_0$$:

$$
\frac{\partial}{\partial \beta_0} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i)
$$


#### למה משווים ל־0?

כדי למצוא מקסימום, משווים את הנגזרות ל־0:

$$
\frac{\partial}{\partial \beta} \log L = 0
$$

אבל... לא ניתן לפתור את זה ביד, לכן נעבור לפתרון נומרי (כמו Gradient Descent).

### 🧸 דוגמה 

נניח שיש לנו 2 תצפיות:

| x  | y |
|----|---|
| 1  | 1 |
| 2  | 0 |

ונניח שכרגע:
- $$\beta_0 = 0$$
- $$\beta_1 = 0$$

#### שלב 1: חישוב ההסתברויות (p-hat)

$$
\hat{p}_1 = \frac{1}{1 + e^{-(0 + 0 \cdot 1)}} = 0.5 \\
\hat{p}_2 = \frac{1}{1 + e^{-(0 + 0 \cdot 2)}} = 0.5
$$

#### שלב 2: חישוב נגזרת לפי $$\beta_1$$

$$
\sum (y_i - \hat{p}_i) \cdot x_i =
(1 - 0.5) \cdot 1 + (0 - 0.5) \cdot 2 =
0.5 - 1 = -0.5
$$

מאחר והנגזרת שלילית, המסקנה היא:
> **צריך להגדיל את** $$\beta_1$$ כדי לשפר את המודל.

### 🎯 סיכום:
- אם הנגזרת חיובית → נעלה את המקדם
- אם הנגזרת שלילית → נוריד את המקדם
- נמשיך כך עד שהנגזרת תהיה ≈ 0 – ונמצא את מקסימום הפונקציה 💛


---

### שלב 7: משתמשים בפתרון נומרי

בגלל שהפונקציה מסובכת, המחשב פותר אותה על ידי ניסוי וטעייה חכמה:

- Gradient Descent
- Newton-Raphson
- שיטות אחרות שנמצאות במודלים כמו `LogisticRegression` בסקיקיט־לרן

כל פעם המחשב משנה קצת את β₀ ו־β₁, בודק אם ההסתברויות נהיו יותר טובות,  
וממשיך עד שהוא מוצא את הערכים הכי טובים.

---

### ✅ סיכום:
- אין נוסחה סגורה למציאת β₀ ו־β₁
- הפתרון מתקבל בעזרת אופטימיזציה מתמטית (נומרית)
- הפונקציה שאנחנו ממקסמים נקראת log-likelihood
