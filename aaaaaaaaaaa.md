## 🧠 Logistic Regression – מציאת β₀ ו־β₁ בשלבים

### שלב 1: נגדיר את הבעיה
יש לנו נתונים (X, Y), כאשר:
- X הוא משתנה רציף (למשל: שעות למידה)
- Y הוא בינארי: 0 או 1 (למשל: כישלון או הצלחה)

המטרה שלנו:
> לחשב את ההסתברות ש־Y יהיה 1 בהינתן X.

---

### שלב 2: נגדיר את הפונקציה הלוגיסטית

זו הפונקציה שאיתה נבנה את המודל:

$$
P(Y = 1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

היא תמיד מחזירה ערכים בין 0 ל־1 – מושלם להסתברויות.

---

### שלב 3: נבנה את פונקציית ההסתברות הכוללת (Likelihood)

כדי לבדוק עד כמה המודל שלנו "מדויק", נחשב את ההסתברות שהוא חוזה נכון את כל הנתונים:

$$
L(\beta_0, \beta_1) = \prod_{i=1}^{n} \left( \hat{p}_i \right)^{y_i} \cdot \left(1 - \hat{p}_i \right)^{1 - y_i}
$$

כאשר:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

---

### שלב 4: ניקח לוגריתם של פונקציית הסבירות

כדי לפשט את החישוב, לוקחים את הלוגריתם של פונקציית הסבירות (log-likelihood):

$$
\log L(\beta_0, \beta_1) = \sum_{i=1}^{n} \left[ y_i \cdot \log(\hat{p}_i) + (1 - y_i) \cdot \log(1 - \hat{p}_i) \right]
$$

---

### שלב 5: נגדיר את בעיית האופטימיזציה

המטרה היא **למקסם** את פונקציית הלוג-לייקלי-הוד:

$$
\max_{\beta_0, \beta_1} \ \log L(\beta_0, \beta_1)
$$

---

### שלב 6: נחשב נגזרות ונחפש מקסימום

- מחשבים נגזרות של הפונקציה לפי β₀ ו־β₁
- משווים ל־0 כדי למצוא נקודת קיצון
- אבל... אי אפשר לפתור את זה ידנית כמו ברגרסיה רגילה

#### מה המטרה?
אנחנו רוצים למצוא את הערכים של לפי β₀ ו־β₁ שמביאים את פונקציית הלוג־לייקלי למקסימום

לפי מתמטיקה:
- נקודת מקסימום נמצאת כאשר **הנגזרת = 0**
- כלומר: הפונקציה לא עולה ולא יורדת – זו פסגה (או תחתית, אבל כאן מדובר על מקסימום)

#### מהי הפונקציה שלנו?

זוהי פונקציית **הלוג־לייקלי** (log-likelihood):

$$
\log L(\beta_0, \beta_1) = \sum_{i=1}^{n} \left[ y_i \cdot \log(\hat{p}_i) + (1 - y_i) \cdot \log(1 - \hat{p}_i) \right]
$$

כאשר:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

---

#### נגזרות:

הנגזרות של הפונקציה לפי β₀ ו־β₁ הן:

- לפי $$\beta_1$$:

$$
\frac{\partial}{\partial \beta_1} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i) \cdot x_i
$$

- לפי $$\beta_0$$:

$$
\frac{\partial}{\partial \beta_0} \log L = \sum_{i=1}^{n} (y_i - \hat{p}_i)
$$

---

#### למה משווים ל־0?

כדי למצוא מקסימום, משווים את הנגזרות ל־0:

$$
\frac{\partial}{\partial \beta} \log L = 0
$$

אבל... לא ניתן לפתור את זה ביד, לכן נעבור לפתרון נומרי (כמו Gradient Descent).

---

### 🧸 דוגמה קטנה ומתוקה לפוצי:

נניח שיש לנו 2 תצפיות:

| x  | y |
|----|---|
| 1  | 1 |
| 2  | 0 |

ונניח שכרגע:
- $$\beta_0 = 0$$
- $$\beta_1 = 0$$

#### שלב 1: חישוב ההסתברויות (p-hat)

$$
\hat{p}_1 = \frac{1}{1 + e^{-(0 + 0 \cdot 1)}} = 0.5 \\
\hat{p}_2 = \frac{1}{1 + e^{-(0 + 0 \cdot 2)}} = 0.5
$$

#### שלב 2: חישוב נגזרת לפי $$\beta_1$$

$$
\sum (y_i - \hat{p}_i) \cdot x_i =
(1 - 0.5) \cdot 1 + (0 - 0.5) \cdot 2 =
0.5 - 1 = -0.5
$$

מאחר והנגזרת שלילית, המסקנה היא:
> **צריך להגדיל את** $$\beta_1$$ כדי לשפר את המודל.

---

### 🎯 סיכום:
- אם הנגזרת חיובית → נעלה את המקדם
- אם הנגזרת שלילית → נוריד את המקדם
- נמשיך כך עד שהנגזרת תהיה ≈ 0 – ונמצא את מקסימום הפונקציה 💛


---

### שלב 7: משתמשים בפתרון נומרי

בגלל שהפונקציה מסובכת, המחשב פותר אותה על ידי ניסוי וטעייה חכמה:

- Gradient Descent
- Newton-Raphson
- שיטות אחרות שנמצאות במודלים כמו `LogisticRegression` בסקיקיט־לרן

כל פעם המחשב משנה קצת את β₀ ו־β₁, בודק אם ההסתברויות נהיו יותר טובות,  
וממשיך עד שהוא מוצא את הערכים הכי טובים.

---

### ✅ סיכום:
- אין נוסחה סגורה למציאת β₀ ו־β₁
- הפתרון מתקבל בעזרת אופטימיזציה מתמטית (נומרית)
- הפונקציה שאנחנו ממקסמים נקראת log-likelihood
