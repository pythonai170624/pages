## 🧠 Logistic Regression – מציאת β₀ ו־β₁ בשלבים

### שלב 1: נגדיר את הבעיה
יש לנו נתונים (X, Y), כאשר:
- X הוא משתנה רציף (למשל: שעות למידה)
- Y הוא בינארי: 0 או 1 (למשל: כישלון או הצלחה)

המטרה שלנו:
> לחשב את ההסתברות ש־Y יהיה 1 בהינתן X.

---

### שלב 2: נגדיר את הפונקציה הלוגיסטית

זו הפונקציה שאיתה נבנה את המודל:

$$
P(Y = 1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

היא תמיד מחזירה ערכים בין 0 ל־1 – מושלם להסתברויות.

---

### שלב 3: נבנה את פונקציית ההסתברות הכוללת (Likelihood)

כדי לבדוק עד כמה המודל שלנו "מדויק", נחשב את ההסתברות שהוא חוזה נכון את כל הנתונים:

$$
L(\beta_0, \beta_1) = \prod_{i=1}^{n} \left( \hat{p}_i \right)^{y_i} \cdot \left(1 - \hat{p}_i \right)^{1 - y_i}
$$

כאשר:

$$
\hat{p}_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}
$$

---

### שלב 4: ניקח לוגריתם של פונקציית הסבירות

כדי לפשט את החישוב, לוקחים את הלוגריתם של פונקציית הסבירות (log-likelihood):

$$
\log L(\beta_0, \beta_1) = \sum_{i=1}^{n} \left[ y_i \cdot \log(\hat{p}_i) + (1 - y_i) \cdot \log(1 - \hat{p}_i) \right]
$$

---

### שלב 5: נגדיר את בעיית האופטימיזציה

המטרה היא **למקסם** את פונקציית הלוג-לייקלי-הוד:

$$
\max_{\beta_0, \beta_1} \ \log L(\beta_0, \beta_1)
$$

---

### שלב 6: נחשב נגזרות ונחפש מקסימום

- מחשבים נגזרות של הפונקציה לפי β₀ ו־β₁
- משווים ל־0 כדי למצוא נקודת קיצון
- אבל... אי אפשר לפתור את זה ידנית כמו ברגרסיה רגילה

---

### שלב 7: משתמשים בפתרון נומרי

בגלל שהפונקציה מסובכת, המחשב פותר אותה על ידי ניסוי וטעייה חכמה:

- Gradient Descent
- Newton-Raphson
- שיטות אחרות שנמצאות במודלים כמו `LogisticRegression` בסקיקיט־לרן

כל פעם המחשב משנה קצת את β₀ ו־β₁, בודק אם ההסתברויות נהיו יותר טובות,  
וממשיך עד שהוא מוצא את הערכים הכי טובים.

---

### ✅ סיכום:
- אין נוסחה סגורה למציאת β₀ ו־β₁
- הפתרון מתקבל בעזרת אופטימיזציה מתמטית (נומרית)
- הפונקציה שאנחנו ממקסמים נקראת log-likelihood
