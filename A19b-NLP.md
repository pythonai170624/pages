### × ×™×ª×•×— ×˜×§×¡×˜ ×¢× SpaCy â€“ ×¤×•× ×§×¦×™×•× ×œ×™×•×ª ×‘×¡×™×¡×™×ª

#### ×˜×¢×™× ×ª ×”××•×“×œ ×•× ×™×ª×•×— ××©×¤×˜

×›×“×™ ×œ× ×ª×— ×˜×§×¡×˜ ×¢× **spaCy**, ×¦×¨×™×š ×§×•×“× ×œ×˜×¢×•×Ÿ ××ª ×”××•×“×œ ×œ×©×¤×” ×”×× ×’×œ×™×ª â€“ `en_core_web_sm`
×”××•×“×œ ××‘×¦×¢ **×˜×•×§× ×™×–×¦×™×”** (×¤×™×¨×•×§ ×©×œ ×”×˜×§×¡×˜ ×œ××™×œ×™× ××• ×‘×™×˜×•×™×™×) ×•××—×–×™×¨ ××•×‘×™×™×§×˜ ××¡×•×’ `Doc`, ×©××›×™×œ ××ª ×”××™×“×¢ ×”×œ×©×•× ×™ ×¢×œ ×›×œ ××™×œ×”

```python
import spacy
nlp = spacy.load("en_core_web_sm")

# The model response is a SpaCy Doc object
doc = nlp("Tesla is looking at buying U.S. startup for $6 million")

for token in doc:
    print(f'[{token.pos:3}]', token.text, token.pos_, token.dep_)
```

Output:
```
[ 96] Tesla PROPN nsubj
[ 87] is AUX aux
[100] looking VERB ROOT
[ 85] at ADP prep
[100] buying VERB pcomp
[ 96] U.S. PROPN dobj
[100] startup VERB advcl
[ 85] for ADP prep
[ 99] $ SYM quantmod
[ 93] 6 NUM compound
[ 93] million NUM pobj
```

×”×¤×œ×˜ ×›×•×œ×œ ×˜×•×§× ×™× ×¢× ×××¤×™×™× ×™× ×›××•:

* `text` â†’ ×”×˜×§×¡×˜ ×©×œ ×”××™×œ×”
* `pos` part-of-speech tag as an integer â†’ ×ª×’ ×—×œ×§ ×”×“×™×‘×¨ ×›××¡×¤×¨ (int)
* `pos_`string representation of the POS tag (more human) â†’ ×¡×•×’ ×”××™×œ×” (×©× ×¢×¦×, ×¤×•×¢×œ, ×ª×•××¨ ×•×›×•â€™) ×›×ª×™××•×¨ ××™×œ×•×œ×™
* `dep_` dependency label â†’ ×”×§×©×¨ ×”×ª×—×‘×™×¨×™ ×©×œ ×”××™×œ×” ×‘××©×¤×˜ (× ×•×©×, × ×©×•×, ××•×©× ×™×©×™×¨ ×•×›×•â€™)


| ×˜×•×§×Ÿ (text) | POS\_ |                | DEP\_    |                         |
| ----------- | ----- | ------------------------- | -------- | --------------------------------- |
| Tesla       | PROPN | ×©× ×¢×¦× ×¤×¨×˜×™ (Proper Noun) | nsubj    | × ×•×©× ×”××©×¤×˜                        |
| is          | AUX   | ×¤×•×¢×œ ×¢×–×¨ (Auxiliary Verb) | aux      | ×¤×•×¢×œ ×¢×–×¨ ×©×œ ×”×¤×•×¢×œ ×”××¨×›×–×™          |
| looking     | VERB  | ×¤×•×¢×œ (Verb)               | ROOT     | ×”×¤×•×¢×œ ×”××¨×›×–×™ ×‘××©×¤×˜                |
| at          | ADP   | ××™×œ×ª ×™×—×¡ (Adposition)     | prep     | ××™×œ×ª ×™×—×¡ ×©××§×©×¨×ª ×œ×¤×•×¢×œ             |
| buying      | VERB  | ×¤×•×¢×œ (Verb)               | pcomp    | ××©×œ×™× ×©×œ ×”×¤×•×¢×œ ×‘×××¦×¢×•×ª ××™×œ×ª ×™×—×¡   |
| U.S.        | PROPN | ×©× ×¢×¦× ×¤×¨×˜×™               | dobj     | ××•×©× ×™×©×™×¨ ×©×œ ×”×¤×•×¢×œ                |
| startup     | VERB  | ×¤×•×¢×œ                      | advcl    | ×¤×¡×•×§×™×ª × ×¡×™×‘×ª×™×ª ×©××•×¡×™×¤×” ××™×“×¢ ×œ×¤×•×¢×œ |
| for         | ADP   | ××™×œ×ª ×™×—×¡                  | prep     | ××™×œ×ª ×™×—×¡ ×©××•×‘×™×œ×” ×œ××•×©× × ×•×¡×£       |
| \$          | SYM   | ×¡×™××Ÿ (Symbol)             | quantmod | ×ª×™××•×¨ ×›××•×ª ×©××§×“×™× ××¡×¤×¨            |
| 6           | NUM   | ××¡×¤×¨ (Numeral)            | compound | ×ª×™××•×¨ ××§×“×™× ×©×œ ×©× ×¢×¦× ××—×¨         |
| million     | NUM   | ××¡×¤×¨ (Numeral)            | pobj     | ××•×©× ×©×œ ××™×œ×ª ×”×™×—×¡ "for"           |

In linguistic terms, a **proper noun** is a specific type of noun that names a particular
person, place, organization, or sometimes a thing

#### ×¨×•××™× ×›××Ÿ ×˜×•×§× ×™×–×¦×™×” ××ª×§×“××ª

* ×”××•×“×œ ××–×”×” **U.S.** ×›×˜×•×§×Ÿ ××—×“ ×•×œ× ××¤×¦×œ ××•×ª×•
* ××–×”×” **\$** ×›×¡××œ (SYM)
* ××–×”×” **million** ×›××¡×¤×¨ (NUM)

## ×“×•×’×× × ×•×¡×¤×ª

```python
import spacy
nlp = spacy.load("en_core_web_sm")

# The model response is a SpaCy Doc object
doc = nlp("Tesla isn't looking into startups anymore")

for token in doc:
    print(f'[{token.pos:3}]', token.text, token.pos_, token.dep_)
```

Output:

```
[ 96] Tesla PROPN nsubj
[ 87] is AUX aux
[ 94] n't PART neg
[100] looking VERB ROOT
[ 85] into ADP prep
[ 92] startups NOUN pobj
[ 86] anymore ADV advmod
```

We can see that the model **splitted the 'isn't' word into 2 separated tokens**

First for the 'is' (AUX - auxiliary verbs) Second for 'n't' (neg - negativity word)

### ×¤×™×¨×•×§ ×©×œ ××™×œ×™× ××¡×•×‘×›×•×ª

×”××™×œ×” **isn't** ××ª×¤×¦×œ×ª ×œ×©× ×™ ×˜×•×§× ×™×:

* `is` (×¤×•×¢×œ ×¢×–×¨)
* `n't` (××™×œ×ª ×©×œ×™×œ×”)

×›×“×™ ×œ× ×œ×¤×¦×œ ××ª ×”××™×œ×” ×”××§×•×¦×¨×ª isn't ×œ×˜×•×§× ×™× is ×•Ö¾n't, ×¦×¨×™×š ×œ×©× ×•×ª ××ª ×—×•×§×™ ×”×˜×•×§× ×™×–×¦×™×” ×©×œ SpaCy

×‘×¨×™×¨×ª ×”××—×“×œ ×©×œ SpaCy ××¤×¨×™×“×” ×§×•× ×˜×¨×§×¦×™×•×ª, ××‘×œ × ×™×ª×Ÿ ×œ×‘×˜×œ ×–××ª ×¢"×™ ×”×—×œ×¤×ª ×‘×¨×™×¨×ª ×”××—×“×œ ×©×œ ×”Ö¾Tokenizer ××• ×¢"×™ ×©×™× ×•×™ ×©×œ infixes ×•Ö¾suffixes

### ×¤×•× ×§×¦×™×™×ª Exaplin

×× ×× ×—× ×• ×¨×•×¦×™× ×œ×”×‘×™×Ÿ ××” ×”××©××¢×•×ª ×©×œ ×ª×’ ××¡×•×™× ×©××—×–×™×¨×” ×œ× ×• spaCy, ××¤×©×¨ ×œ×”×©×ª××© ×‘×¤×•× ×§×¦×™×” `spacy.explain` ×©××¡×¤×§×ª ×”×¡×‘×¨ ×˜×§×¡×˜×•××œ×™ ×‘×¨×•×¨ ×œ×›×œ ×ª×’ ×ª×—×‘×™×¨×™ ××• ×“×§×“×•×§×™

×œ×“×•×’××”:

```python
import spacy
import spacy.explain

print(spacy.explain("AUX"))    # auxiliary - Auxiliary verb
print(spacy.explain("nsubj"))  # nominal subject
print(spacy.explain("ROOT"))   # root - Root of the sentence
```

### ××” ×–×” Doc?

××•×‘×™×™×§×˜ `Doc` ×”×•× ×ª×•×¦××” ×©×œ ×¢×™×‘×•×“ ×˜×§×¡×˜ ×‘×¢×–×¨×ª ××•×“×œ ×©×œ spaCy
×›××©×¨ ××¤×¢×™×œ×™× ××ª ×”××•×“×œ ×¢×œ ×˜×§×¡×˜, ××§×‘×œ×™× ××•×‘×™×™×§×˜ `Doc` ×©××™×™×¦×’ ××ª ×›×œ ×”×˜×§×¡×˜ ×•×›×•×œ×œ ××ª ×›×œ ×”××™×“×¢ ×”×œ×©×•× ×™ ×¢×‘×•×¨×•: ×˜×•×§× ×™×, ××‘× ×” ×ª×—×‘×™×¨×™, ×™×©×•×™×•×ª, ××©×¤×˜×™× ×•×¢×•×“

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying a startup in the U.K.")
```

×”××•×‘×™×™×§×˜ `doc` ×”×•× ×¨×¦×£ ×©×œ ×˜×•×§× ×™×. × ×™×ª×Ÿ ×œ×¢×‘×•×¨ ×¢×œ×™×• ×›××• ×¢×œ ×¨×©×™××”:

```python
for token in doc:
    print(token.text, token.pos_, token.dep_)
```

### ××” ×–×” Span?

×”××•× ×— `Span` ×”×•× ×ª×ªÖ¾×§×˜×¢ ×©×œ ×”××•×‘×™×™×§×˜ `Doc`. ×›×œ×•××¨, ×¨×¦×£ ×©×œ ×˜×•×§× ×™× ×©××™×™×¦×’ ×—×œ×§ ××”×˜×§×¡×˜ ×”××§×•×¨×™ â€“ ××™×œ×” ××—×ª, ×‘×™×˜×•×™, ××©×¤×˜ ××• ××¤×™×œ×• ×¤×¡×§×”

```python
span = doc[3:6]  # ×™×•×¦×¨ Span ××”××™×œ×™× ×”×¨×‘×™×¢×™×ª ×¢×“ ×”×©×™×©×™×ª
print(span.text)
```

×‘×“×•×’××” ×”×–×• `span.text` ××—×–×™×¨ ××ª ×”×—×œ×§ "at buying a"

### ×”×”×‘×“×œ ×‘×™×Ÿ Doc ×œ-Span

| ×ª×›×•× ×”       | Doc                           | Span                              |
| ----------- | ----------------------------- | --------------------------------- |
| ×”×’×“×¨×”       | ××™×™×¦×’ ××ª ×›×œ ×”×˜×§×¡×˜ ×©× ×™×ª×Ÿ ×œ××•×“×œ | ×ª×ªÖ¾×§×˜×¢ ××ª×•×š ×“×•×§                   |
| ×›×•×œ×œ ×˜×•×§× ×™× | ×›×Ÿ                            | ×›×Ÿ                                |
| ×›×•×œ×œ ××©×¤×˜×™× | ×›×Ÿ                            | ×œ× ×‘×”×›×¨×—                          |
| ×›×•×œ×œ ×™×©×•×™×•×ª | ×›×Ÿ                            | ×œ× ×‘×”×›×¨×—                          |
| ×©×™××•×© ×¢×™×§×¨×™ | × ×™×ª×•×— ×›×•×œ×œ ×©×œ ×”×˜×§×¡×˜           | × ×™×ª×•×— ×××•×§×“ ×©×œ ×‘×™×˜×•×™ ××• ×§×˜×¢ ×‘×˜×§×¡×˜ |

×œ×¡×™×›×•×: `Doc` ×”×•× ×”××¡××š ×”×©×œ×, ×•Ö¾`Span` ×”×•× ×—×ª×™×›×” ××ª×•×›×• ×©××¤×©×¨ ×œ× ×ª×— ×‘× ×¤×¨×“

××•×‘×™×™×§×˜ ××¡×•×’ `Span` ××™×™×¦×’ ×§×˜×¢ (×¨×¦×™×£) ××ª×•×š `Doc` â€“ ×›×œ×•××¨ ×ª×ª-×¨×¦×£ ×©×œ ×˜×•×§× ×™× ×©× ×•×¦×¨×• ×œ××—×¨ ×¢×™×‘×•×“ ×”×˜×§×¡×˜. ×”-Span ×©×•××¨ ×¢×œ ×”×”×§×©×¨ ××ª×•×š ×”××¡××š ×”××§×•×¨×™ ×•×××¤×©×¨ ×œ×‘×¦×¢ ×¢×œ×™×• × ×™×ª×•×— ×ª×—×‘×™×¨×™, ×¡×× ×˜×™ ××• ××—×¨, ×‘×“×™×•×§ ×›××• ×¢×œ `Doc`

```python
import spacy
nlp = spacy.load("en_core_web_sm")

# ××¡××š ×©×œ×
doc3 = nlp(u"Although commonly attributed to John Lennon from his song \"Beautiful Boy\", the phrase\
\"Life is what happens to us while we are making other plans\" was written by cartoonist Allen Saunders\
 and published in Reader's Digest in 1957, when Lennon was 17.")

# Span ××ª×•×š ×”××™×œ×™× ×‘××™×§×•××™× 16 ×¢×“ 30
life_quote = doc3[16:30]

print(life_quote)
print(type(life_quote))
```

×¤×œ×˜:

```
Life is what happens to us while we are making other plans
<class 'spacy.tokens.span.Span'>
```

### ××” ×–×” Sents ×‘-SpaCy?

`Doc.sents` ×”×•× generator ×©×œ ××©×¤×˜×™× â€“ ×›×œ×•××¨ SpaCy ××–×”×” ××•×˜×•××˜×™×ª ××ª ×”×’×‘×•×œ×•×ª ×‘×™×Ÿ ××©×¤×˜×™× ×‘×˜×§×¡×˜ ×•××—×–×™×¨ ×›×œ ××©×¤×˜ ×›××•×‘×™×™×§×˜ `Span`

```python
doc4 = nlp('This is the first sentence. This is another sentence. This is the last sentence.')

for sentence in doc4.sents:
    print(sentence)
```

×¤×œ×˜:

```
This is the first sentence.
This is another sentence.
This is the last sentence.
```

ğŸ–¼ï¸ \*×›××Ÿ ×œ×”×•×¡×™×£ ××ª ×”×ª××•× ×” ××”×©×§×£ (×©×•×¨×” ×©× ×™×™×” ×‘×ª××•× ×”)

### ××” ×–×” is\_sent\_start?

`token.is_sent_start` ××—×–×™×¨ `True` ×× ×”×˜×•×§×Ÿ ×”×•× ×”×˜×•×§×Ÿ ×”×¨××©×•×Ÿ ×‘××©×¤×˜, ×•Ö¾`False` ××—×¨×ª

```python
print(f'word: {doc4[6]}, start a sentence: {doc4[6].is_sent_start}')
print(f'word: {doc4[8]}, start a sentence: {doc4[8].is_sent_start}')
```

×¤×œ×˜:

```
word: This, start a sentence: True
word: another, start a sentence: False
```

ğŸ–¼ï¸ \*×›××Ÿ ×œ×”×•×¡×™×£ ××ª ×”×ª××•× ×” ××”×©×§×£ (×©×•×¨×” ×©×œ×™×©×™×ª ×‘×ª××•× ×”)

### ×¡×™×›×•×:

* `Span` = ×§×˜×¢ ××•×’×“×¨ ××ª×•×š Doc
* `sents` = ××©×¤×˜×™× ×©-SpaCy ×—×™×œ×§×” ××•×˜×•××˜×™×ª ××ª×•×š Doc
* `is_sent_start` = ×”×× ×”×˜×•×§×Ÿ ×”×•× ×ª×—×™×œ×ª ××©×¤×˜

×©×œ×•×©×ª× ×©×•××¨×™× ×¢×œ ×”×”×§×©×¨ ×©×œ ×”×˜×§×¡×˜ ×”××§×•×¨×™ ×•×××¤×©×¨×™× × ×™×ª×•×—×™× ××ª×§×“××™×


### ×›×™×¦×“ spaCy ××¤×¨×§ ×˜×•×§× ×™× ×‘×©×¤×”

spaCy ××©×ª××© ×‘×—×•×§×™ ×˜×•×§× ×™×–×¦×™×” ×©××ª××™××™× ×œ×©×¤×” (×œ××©×œ ×× ×’×œ×™×ª) ×©××•×’×“×¨×™× ××¨××©

* **prefixes** â€“ ×ª×•×•×™× ×‘×”×ª×—×œ×” ×›××• \$, (
* **suffixes** â€“ ×ª×•×•×™× ×‘×¡×•×£ ×›××• ., !
* **infixes** â€“ ×ª×•×•×™× ×‘×××¦×¢ ×›××• ×§×• ××¤×¨×™×“ ×‘Ö¾"e-mail"

