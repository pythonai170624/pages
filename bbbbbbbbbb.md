# 🤖 SVM – יסודות והבנה מתמטית

## מה זה SVM?

SVM (Support Vector Machine) היא שיטת למידה מונחית (Supervised Learning) המשמשת ל:
- סיווג (Classification)
- רגרסיה (Regression – חיזוי ערכים רציפים)

המטרה העיקרית:
> למצוא את הגבול (Hyperplane) שמפריד בצורה המיטבית בין קבוצות שונות של דוגמאות.

---

## מה המטרה של SVM?

למצוא את ההיפר־פליין (קו או מישור) שמפריד בין הקבוצות עם **מרווח מקסימלי** מהנקודות הקרובות ביותר – הנקראות **וקטורים תומכים (Support Vectors)**.

📌 וקטורים תומכים:
- הנקודות הקרובות ביותר למישור ההפרדה.
- הן קובעות את מיקום ההיפר־פליין.
- הזזתן תשנה את המישור, בניגוד לשאר הנקודות.

---

## מה זה Hyperplane?

- ב־2D: קו ישר.
- ב־3D: משטח.
- ב־4D ומעלה: "Hyperplane" כללי.

---

## משוואת ההיפר־פליין

ההיפר־פליין מיוצג כך:

$$
f(x) = w^T x + b
$$

כאשר:
- $w$ הוא וקטור המשקלים (Weights)
- $x$ הוא וקטור הקלט
- $b$ הוא ההיסט (Bias)

---

## תנאי ההפרדה

כדי שהקווים יפרידו נכון בין הקבוצות, מתקיים:

$$
y_i (w^T x_i + b) \geq 1
$$

כאשר:
- $x_i$ – דוגמת קלט
- $y_i$ – תווית הקלט (+1 או -1)

---

## למה ממזערים את \( \|w\| \) ב־SVM?

המטרה היא להגדיל את ה־**Margin**:

$$
\text{Margin} = \frac{1}{\|w\|}
$$

ככל ש־$\|w\|$ קטן → המרווח גדול יותר → הפרדה טובה יותר בין הקבוצות.

בפועל ממזערים:

$$
\min \left( \frac{1}{2} \|w\|^2 \right)
$$

🔹 למה דווקא \( \frac{1}{2} \|w\|^2 \)?
- נגזרת פשוטה יותר (הנגזרת היא $w$)
- לא משנה את תוצאת האופטימיזציה

---

## איך מוצאים את הפתרון?

1. מנסחים בעיית אופטימיזציה עם תנאים.
2. משתמשים בשיטת לגראנז' (Lagrange Multipliers).
3. פותרים את הבעיה הכפולה (Dual Problem).
4. הפתרון תלוי רק ב־Support Vectors:

$$
w = \sum_i \alpha_i y_i x_i
$$

כאשר:
- רק ל־support vectors יש \( \alpha_i > 0 \)

---

## מיקום הקווים ב־SVM

- **היפר־פליין מרכזי**:
  $$w^T x + b = 0$$
- **גבולות הרווח (Margin Boundaries)**:
  - גבול עליון: $$w^T x + b = +1$$
  - גבול תחתון: $$w^T x + b = -1$$

✅ נקודות התמיכה יושבות **בדיוק על גבולות הרווח**, לא על ההיפר־פליין עצמו.

---

## סיכום טבלה: מיקום הקווים

| קו | עובר דרך נקודות התמיכה? |
|----|---------------------------|
| היפר־פליין מרכזי ($w^T x + b = 0$) | ❌ לא |
| גבולות רווח ($w^T x + b = \pm1$)   | ✅ כן |

---

## למה רק נקודות התמיכה משפיעות על המודל?

- רק להן יש ערך $\alpha_i > 0$
- הן קובעות את המיקום של ההיפר־פליין
- שאר הנקודות לא משפיעות (\( \alpha_i = 0 \))

---

## איך SVM מתמודד עם יותר משתי קבוצות?

SVM קלאסי עובד עם שתי מחלקות בלבד. מה עושים כשיש יותר?

### פתרונות נפוצים:
- **One-vs-Rest (OvR)** – כל קבוצה מול שאר הקבוצות.
- **One-vs-One (OvO)** – כל זוג קבוצות מופרד ע"י SVM משלו.

ב־`sklearn`:
- `SVC` תומך ב־OvO כברירת מחדל – אין צורך לטפל בזה ידנית.

---

> המשך: דף 2 – גרעינים, Kernel Trick וטרנספורמציות 🚀
