# הסבר על הפתרון של רגרסיה פולינומיאלית מדרגה שנייה

## מהנוסחה לפתרון

ברגרסיה פולינומיאלית מדרגה שנייה, אנו מנסים למצוא את המקדמים $\beta_0$, $\beta_1$, ו-$\beta_2$ במשוואה:

$$y = \beta_0 + \beta_1 x + \beta_2 x^2$$

כמו ברגרסיה לינארית, אנו רוצים למצוא את הערכים שממזערים את סכום ריבועי השגיאות (SSE):

$$SSE = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i + \beta_2 x_i^2))^2$$

## צעד 1: גזירת ה-SSE לפי כל אחד מהפרמטרים

כדי למצוא את הפרמטרים האופטימליים, נגזור את ה-SSE לפי כל אחד מהפרמטרים ונשווה לאפס:

**גזירה לפי $\beta_0$**:
$$\frac{\partial SSE}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i + \beta_2 x_i^2)) = 0$$

**גזירה לפי $\beta_1$**:
$$\frac{\partial SSE}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i(y_i - (\beta_0 + \beta_1 x_i + \beta_2 x_i^2)) = 0$$

**גזירה לפי $\beta_2$**:
$$\frac{\partial SSE}{\partial \beta_2} = -2 \sum_{i=1}^{n} x_i^2(y_i - (\beta_0 + \beta_1 x_i + \beta_2 x_i^2)) = 0$$

## צעד 2: פישוט המשוואות

**מהגזירה לפי $\beta_0$**:
$$\sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i - \beta_2 x_i^2) = 0$$

נפשט:
$$\sum_{i=1}^{n} y_i - \beta_0 \sum_{i=1}^{n} 1 - \beta_1 \sum_{i=1}^{n} x_i - \beta_2 \sum_{i=1}^{n} x_i^2 = 0$$

מכיוון ש-$\sum_{i=1}^{n} 1 = n$ (מספר הנקודות), נקבל:
$$\sum_{i=1}^{n} y_i - n\beta_0 - \beta_1 \sum_{i=1}^{n} x_i - \beta_2 \sum_{i=1}^{n} x_i^2 = 0$$

מכאן:
$$\sum_{i=1}^{n} y_i = n\beta_0 + \beta_1 \sum_{i=1}^{n} x_i + \beta_2 \sum_{i=1}^{n} x_i^2$$

**מהגזירה לפי $\beta_1$**:
$$\sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i - \beta_2 x_i^2) = 0$$

נפשט:
$$\sum_{i=1}^{n} x_iy_i - \beta_0 \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 - \beta_2 \sum_{i=1}^{n} x_i^3 = 0$$

מכאן:
$$\sum_{i=1}^{n} x_iy_i = \beta_0 \sum_{i=1}^{n} x_i + \beta_1 \sum_{i=1}^{n} x_i^2 + \beta_2 \sum_{i=1}^{n} x_i^3$$

**מהגזירה לפי $\beta_2$**:
$$\sum_{i=1}^{n} x_i^2(y_i - \beta_0 - \beta_1 x_i - \beta_2 x_i^2) = 0$$

נפשט:
$$\sum_{i=1}^{n} x_i^2y_i - \beta_0 \sum_{i=1}^{n} x_i^2 - \beta_1 \sum_{i=1}^{n} x_i^3 - \beta_2 \sum_{i=1}^{n} x_i^4 = 0$$

מכאן:
$$\sum_{i=1}^{n} x_i^2y_i = \beta_0 \sum_{i=1}^{n} x_i^2 + \beta_1 \sum_{i=1}^{n} x_i^3 + \beta_2 \sum_{i=1}^{n} x_i^4$$

## צעד 3: ייצוג במערכת משוואות

קיבלנו מערכת של שלוש משוואות ליניאריות עם שלושה נעלמים ($\beta_0$, $\beta_1$, $\beta_2$):

1. $\sum_{i=1}^{n} y_i = n\beta_0 + \beta_1 \sum_{i=1}^{n} x_i + \beta_2 \sum_{i=1}^{n} x_i^2$
2. $\sum_{i=1}^{n} x_iy_i = \beta_0 \sum_{i=1}^{n} x_i + \beta_1 \sum_{i=1}^{n} x_i^2 + \beta_2 \sum_{i=1}^{n} x_i^3$
3. $\sum_{i=1}^{n} x_i^2y_i = \beta_0 \sum_{i=1}^{n} x_i^2 + \beta_1 \sum_{i=1}^{n} x_i^3 + \beta_2 \sum_{i=1}^{n} x_i^4$

## צעד 4: פתרון בצורה מטריצית

אפשר לייצג את המערכת בצורה מטריצית:

$$\begin{bmatrix} 
n & \sum x_i & \sum x_i^2 \\
\sum x_i & \sum x_i^2 & \sum x_i^3 \\
\sum x_i^2 & \sum x_i^3 & \sum x_i^4
\end{bmatrix}
\begin{bmatrix} 
\beta_0 \\
\beta_1 \\
\beta_2
\end{bmatrix} = 
\begin{bmatrix} 
\sum y_i \\
\sum x_i y_i \\
\sum x_i^2 y_i
\end{bmatrix}$$

זוהי בדיוק משוואת המטריצה הנורמלית $X^TX\beta = X^Ty$ שמתקבלת כאשר מיישמים רגרסיה לינארית על נתונים שעברו טרנספורמציה פולינומיאלית.

בצורה מקוצרת:
$$A\beta = b$$

והפתרון:
$$\beta = A^{-1}b$$

## צעד 5: פתרון מספרי עבור הדוגמה

עבור הדוגמה של שעות אימון וזמני ריצה, אנו צריכים לחשב את הסכומים:

- $n = 10$ (מספר הנקודות)
- $\sum x_i = 129$ (סכום שעות האימון)
- $\sum y_i = 661$ (סכום זמני הריצה)
- $\sum x_i^2 = 2,493$ (סכום ריבועי שעות האימון)
- $\sum x_i y_i = 7,860$ (סכום מכפלות שעות וזמנים)
- $\sum x_i^2 y_i = 152,110$ (סכום מכפלות ריבועי שעות וזמנים)
- $\sum x_i^3$ = יש לחשב
- $\sum x_i^4$ = יש לחשב

אם נחשב את הסכומים החסרים ונציב במטריצה, נוכל למצוא את הפתרון המספרי:

$$\beta_0 \approx 110.5$$
$$\beta_1 \approx -5.3$$
$$\beta_2 \approx 0.13$$

מכאן, משוואת הרגרסיה הפולינומיאלית היא:
$$y = 110.5 - 5.3x + 0.13x^2$$

## הסבר גרפי של המשמעות

משוואה זו מייצגת פרבולה (עקומה בצורת U) שבה:
- $\beta_0 = 110.5$ הוא החיתוך עם ציר ה-y (הערך כאשר x=0)
- $\beta_1 = -5.3$ מייצג את השיפוע הראשוני (שלילי, כלומר הביצועים משתפרים בהתחלה)
- $\beta_2 = 0.13$ מייצג את מידת הקמירות של הפרבולה (חיובי, כלומר הפרבולה פונה כלפי מעלה - הביצועים מתחילים להידרדר לאחר נקודה מסוימת)

פרבולה זו מתארת את היחס בין שעות אימון לזמן ריצה, כאשר ניתן לראות שהיא מגיעה למינימום (הזמן הטוב ביותר) ואז עולה שוב.

## חישוב הנקודה האופטימלית

כדי למצוא את נקודת המינימום של הפרבולה, נגזור את המשוואה לפי x ונשווה לאפס:

$$\frac{dy}{dx} = -5.3 + 2 \cdot 0.13 \cdot x = 0$$
$$-5.3 + 0.26x = 0$$
$$0.26x = 5.3$$
$$x = \frac{5.3}{0.26} \approx 20.4$$

לכן, מספר שעות האימון האופטימלי הוא כ-20.4 שעות בשבוע. כדי למצוא את הזמן הטוב ביותר, נציב את הערך הזה במשוואה המקורית:

$$y = 110.5 - 5.3 \cdot 20.4 + 0.13 \cdot 20.4^2$$
$$y = 110.5 - 108.12 + 0.13 \cdot 416.16$$
$$y = 110.5 - 108.12 + 54.1$$
$$y \approx 56.48 \textrm{ שניות}$$

זהו הזמן המינימלי הצפוי לפי המודל שלנו.

## יתרונות השימוש במודל פולינומיאלי

המודל הפולינומיאלי מאפשר לנו:
1. לתאר מערכות יחסים לא-לינאריות
2. למצוא נקודות אופטימום (מינימום או מקסימום)
3. לחזות ביצועים בצורה מדויקת יותר במקרים שבהם היחס אינו לינארי

חשוב לזכור שמודלים פולינומיאליים מדרגה גבוהה עלולים לסבול מ"אובר-פיטינג" - התאמת יתר לנתונים הקיימים, שעשויה לפגוע ביכולת ההכללה על נתונים חדשים. לכן, בדרך כלל מתחילים עם מודל ריבועי (דרגה 2) ומעלים את הדרגה רק אם יש הצדקה לכך מבחינת הנתונים והתיאוריה.
