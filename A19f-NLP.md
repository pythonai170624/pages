# NLP FETAURES

## מהו 🧠 Text Classification – סיווג טקסטים

### מה זה סיווג טקסטים

סיווג טקסטים (Text Classification) הוא משימה מרכזית בתחום עיבוד שפה טבעית (NLP)  
במשימה זו מקצים לכל טקסט תגית אחת או יותר מתוך רשימת קטגוריות ידועה מראש

דוגמאות לקטגוריות נפוצות:
- חיובי / שלילי (Sentiment Analysis)
- ספאם / לא ספאם
- נושאים: ספורט, פוליטיקה, טכנולוגיה ועוד

### רכיבים עיקריים בתהליך

- פ- **Labels / Categories** – הקבוצות האפשריות שהטקסט יכול להשתייך אליהן  
  דוגמה: אימייל → `spam` או `not spam`  
  דוגמה: סקירת סרט → `positive` או `negative`  
  דוגמה: חדשות → `sports`, `politics`, `entertainment`

- פ- **Features** – מאפיינים המופקים מהטקסט לצורך סיווג  
  דוגמה: תדירות מילים, האם הטקסט מכיל סימני קריאה, מספר מילים חיוביות או שליליות  
  דוגמה: האם המילים כתובות באותיות גדולות, האם יש אזכור של ישות מסוימת (כמו שם של מוצר)

- פ- **Model** – אלגוריתם לומד שמקבל את המאפיינים ומחזיר תחזית לסיווג  
  דוגמה:  
  - Naive Bayes – פשוט ומהיר, מתאים לבסיסים טקסטואליים  
  - SVM – מתאים למקרים עם הפרדה חדה בין קטגוריות  
  - מודלים עמוקים (LSTM, BERT) – לטקסטים ארוכים ומורכבים

## ✨ Feature Extraction – הפקת מאפיינים מהטקסט

כדי שנוכל להשתמש בטקסט במודל למידת מכונה, עלינו להמיר אותו לייצוג מספרי  
נשתמש בטכניקות שונות ל־Feature Extraction

### טכניקות נפוצות:

- Tokenization – חלוקה למילים
- Bag of Words (BoW) – מונה כמה פעמים כל מילה מופיעה
- TF-IDF – מדגיש מילים נדירות יחסית למסמך
- POS Tagging – זיהוי חלקי דיבר
- NER – זיהוי ישויות

## 🧰 BoW – Bag of Words

BoW היא שיטה פשוטה שבה בונים אוסף של כל המילים הייחודיות מהמסמכים  
לאחר מכן יוצרים מטריצה בה כל שורה היא מסמך, וכל עמודה היא מילה מהאוסף  
התאים מציינים כמה פעמים מילה הופיעה במסמך

דוגמה:

- Document 1: "I love NLP"  
- Document 2: "NLP is fun"

Vocabulary: ["I", "love", "NLP", "is", "fun"]

|       | I | love | NLP | is | fun |
|-------|---|------|-----|----|-----|
| Doc1  | 1 | 1    | 1   | 0  | 0   |
| Doc2  | 0 | 0    | 1   | 1  | 1   |

📌 שיטה זו מתעלמת מסדר המילים ומתמקדת רק בספירה

## 🧠 TF-IDF – Term Frequency-Inverse Document Frequency

השיטה הזו משפרת את BoW על ידי הדגשת מילים נדירות  
היא כוללת 2 שלבים:

1. TF – תדירות המילה במסמך  
2. IDF – הופכיות תדירות המילה בכלל המסמכים  
3. TF * IDF – נותן ציון סופי למילה במסמך

דוגמה:

- Document 1: "I love NLP"  
- Document 2: "NLP is fun"

TF:
- NLP מופיעה פעם אחת מתוך 3 מילים בכל מסמך → TF = 1/3

IDF:
- NLP מופיעה בשני המסמכים → IDF = log(2/2) = 0  
- I מופיעה רק במסמך 1 → IDF = log(2/1) = 0.301

TF-IDF:
- NLP → 1/3 * 0 = 0  
- I → 1/3 * 0.301 ≈ 0.1003

|       | I      | love   | NLP | is     | fun    |
|-------|--------|--------|-----|--------|--------|
| Doc1  | 0.1003 | 0.1003 | 0   | 0      | 0      |
| Doc2  | 0      | 0      | 0   | 0.1003 | 0.1003 |

