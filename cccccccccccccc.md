# Support Vector Machines (SVM) - ×”×‘× ×” ××§×™×¤×”

## ğŸ§  ××” ×–×” SVM?

SVM (Support Vector Machine) ×”×™× ×©×™×˜×ª **×œ××™×“×ª ××›×•× ×” ××•× ×—×™×ª (Supervised Learning)**, ×©× ×•×¢×“×”:
- **×œ×¡×•×•×’** ×“×•×’×××•×ª (Classification)
- ××• ×œ×‘×¦×¢ **×¨×’×¨×¡×™×”** (Regression â€“ ×—×™×–×•×™ ×¢×¨×›×™× ×¨×¦×™×¤×™×)

×”××˜×¨×” ×”×¢×™×§×¨×™×ª:  
**×œ××¦×•× ××ª ×”×’×‘×•×œ ×©××¤×¨×™×“ ×‘×¦×•×¨×” ×”×›×™ ×˜×•×‘×” ×‘×™×Ÿ ×§×‘×•×¦×•×ª ×©×•× ×•×ª ×©×œ ×“×•×’×××•×ª**

SVM ×”×•× ××œ×’×•×¨×™×ª× ×œ××™×“×” ××•× ×—×™×ª ×”××—×¤×© ×”×™×¤×¨×¤×œ×Ÿ ××•×¤×˜×™××œ×™ (××™×©×•×¨ ×”×¤×¨×“×”) ×©×™×¤×¨×™×“ ×‘×™×Ÿ ×§×‘×•×¦×•×ª ×©×•× ×•×ª ×©×œ × ×ª×•× ×™×. ×”×™×¤×¨×¤×œ×Ÿ ××•×¤×˜×™××œ×™ ×”×•× ×–×” ×©×™×•×¦×¨ ××ª ×”××¨×•×•×— (margin) ×”××¨×‘×™ ×‘×™×Ÿ ×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ××›×œ ×§×‘×•×¦×”, ×”×™×“×•×¢×•×ª ×›×•×•×§×˜×•×¨×™ ×ª××™×›×” (support vectors).

××ª××˜×™×ª, ×¢×‘×•×¨ × ×ª×•× ×™× ×œ×™× ××¨×™×™×, ×”××•×“×œ ××™×•×¦×’ ×¢×œ ×™×“×™:
- $f(x) = w^T x + b$
- ×›××©×¨ $w$ ×”×•× ×•×§×˜×•×¨ ×”××©×§×œ×•×ª
- x ×”×•× ×•×§×˜×•×¨ ×ª×›×•× ×•×ª ×”×§×œ×˜
- b ×”×•× ×¢×¨×š ×”×”×¡×˜×” (bias)
- ×”× ×•×¡×—× ××ª××™××” ×œ×”×¨×‘×” ××™××“×™× ×•×œ× ×¨×§ ×œ×“×•-××™××“

![SVM Basic Illustration](svm1.png)

## ğŸ¯ ××” ×”××˜×¨×” ×©×œ SVM?

×œ××¦×•× ××ª **×”×§×•/××™×©×•×¨ (Hyperplane)** ×©××¤×¨×™×“ ×‘×¦×•×¨×” ××§×¡×™××œ×™×ª ×‘×™×Ÿ ×§×‘×•×¦×•×ª. 
×”××˜×¨×” ×”×™× ×œ×”×’×“×™×œ ××ª ×”××¨×—×§ ××”×§×• ××œ ×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ â€“ ×”× ×§×¨××•×ª **×•×§×˜×•×¨×™× ×ª×•××›×™×**

### **×•×§×˜×•×¨×™× ×ª×•××›×™× â€“ Support Vectors**

×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ×œ×§×• ×”×”×¤×¨×“×”:

- ×”×Ÿ ××œ×• ×©×§×•×‘×¢×•×ª ××ª ××™×§×•× ×”×§×•
- ×× ×ª×–×™×– × ×§×•×“×” ××—×¨×ª â€“ ×”×§×• ×œ× ×™×–×•×–
- ×× ×ª×–×™×– ×•×§×˜×•×¨ ×ª×•××š â€“ ×”×§×• ×™×©×ª× ×”

ğŸ“Œ ××œ×” "×”× ×§×•×“×•×ª ×”×—×©×•×‘×•×ª ×‘×™×•×ª×¨" ×‘××™××•×Ÿ ×©×œ SVM

![Support Vectors Illustration](svm2.png)

### **×œ××” "×•×§×˜×•×¨ ×ª×•××š" ×•×œ× "× ×§×•×“×” ×ª×•××›×ª"?**

×”×”×¡×‘×¨ ×”××ª××˜×™:

×‘××ª××˜×™×§×”, ×‘×¤×¨×˜ ×‘×’×™××•××˜×¨×™×” ×•×œ××™×“×ª ××›×•× ×”:

×›×œ × ×§×•×“×” ×‘××¨×—×‘ ××™×•×¦×’×ª ×›Ö¾×•×§×˜×•×¨

×œ×“×•×’××”: [3, 4] ×–×• × ×§×•×“×”, ××‘×œ ×’× ×•×§×˜×•×¨ ××”××•×¦× (0,0) ××œ [3, 4]

×›×œ×•××¨: × ×§×•×“×” = ×•×§×˜×•×¨ ×©××¨××” ×œ××Ÿ "×œ×”×’×™×¢" ××”×¨××©×™×ª

××– ×‘××•× ×—×™× ×©×œ ×œ××™×“×ª ××›×•× ×”:

×”× ×ª×•× ×™× ×©×œ× ×• ×”× ×•×§×˜×•×¨×™× ×‘××¨×—×‘

×•×œ×›×Ÿ ×’× ×”Ö¾Support Vectors ×”× ×•×§×˜×•×¨×™× ×©× ××¦××™× ×”×›×™ ×§×¨×•×‘ ×œ××™×©×•×¨ ×”×”×¤×¨×“×”

×•×œ××” "×ª×•××š"?
×›×™ ×”× ××œ×• ×©:

×ª×•××›×™× ×‘××™×§×•× ×©×œ ××™×©×•×¨ ×”×”×¤×¨×“×”

×›×œ×•××¨: ×”× ××œ×• ×©×§×•×‘×¢×™× ××•×ª×•

×× ×ª×–×™×– ××—×“ ××”× â€” ×”××™×©×•×¨ ×™×–×•×–!

## ğŸ›¤ï¸ ××” ×–×” Hyperplane?

- ×‘Ö¾2D: ×§×• ×™×©×¨
- ×‘Ö¾3D: ××©×˜×—
- ×‘Ö¾4D ×•××¢×œ×”: ×¤×©×•×˜ × ×§×¨× "Hyperplane"

![Hyperplane Illustration](svm3.png)

## âœ… ××” ×”×Ÿ × ×§×•×“×•×ª ×”×ª××™×›×”?

- × ×§×•×“×•×ª ×”×ª××™×›×” ×”×Ÿ **×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ×œ××™×©×•×¨ ×”××¤×¨×™×“**.
- ×”×Ÿ ×™×•×©×‘×•×ª ×‘×“×™×•×§ **×¢×œ ×’×‘×•×œ×•×ª ×”×¨×•×•×—** (margin boundaries).
- ×”×Ÿ ××§×™×™××•×ª ××ª ×”×ª× ××™:

$$
y_i(w^T x_i + b) = 1 \quad \text{××•} \quad -1
$$

## ğŸ“ ×”×× ×”×§×• ×—×™×™×‘ ×œ×¢×‘×•×¨ ×“×¨×›×Ÿ?

### ğŸ”¹ ×”××™×©×•×¨ ×”××¨×›×–×™ (×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ):

$$
w^T x + b = 0
$$

- **×œ× ×¢×•×‘×¨ ×“×¨×š × ×§×•×“×•×ª ×”×ª××™×›×”**.
- ×”×•× ×¢×•×‘×¨ **×‘×××¦×¢** ×‘×™×Ÿ ×©×ª×™ ×”×§×‘×•×¦×•×ª.

### ğŸ”¹ ×’×‘×•×œ×•×ª ×”×¨×•×•×— (margin boundaries):

- ×’×‘×•×œ ×¢×œ×™×•×Ÿ:

$$
w^T x + b = +1
$$

- ×’×‘×•×œ ×ª×—×ª×•×Ÿ:

$$
w^T x + b = -1
$$

âœ… **×›×Ÿ! ×’×‘×•×œ×•×ª ×”×¨×•×•×— ×—×™×™×‘×™× ×œ×¢×‘×•×¨ ×“×¨×š × ×§×•×“×•×ª ×”×ª××™×›×”.**

## ğŸ’¡ ×œ××” × ×§×•×“×•×ª ×”×ª××™×›×” ×”×›×™ ×—×©×•×‘×•×ª?

- ×›×™ ×¨×§ × ×§×•×“×•×ª ×”×ª××™×›×” ××©×¤×™×¢×•×ª ×¢×œ ×”×¤×ª×¨×•×Ÿ.
- ×›×œ ×©××¨ ×”× ×§×•×“×•×ª **×œ× ××©× ×•×ª ××ª ×”××™×§×•× ×©×œ ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ**.
- ×”×¤×ª×¨×•×Ÿ ×©×œ \( w \) ××‘×•×¡×¡ ××š ×•×¨×§ ×¢×œ×™×”×Ÿ:

$$
w = \sum_i \alpha_i y_i x_i
$$

×›××©×¨ ×¨×§ ×œÖ¾support vectors ×™×© \( \alpha_i \neq 0 \)

### ğŸ§  ×¡×™×›×•× ×‘×˜×‘×œ×”:

| ××¨×›×™×‘ | ×¢×•×‘×¨ ×“×¨×š × ×§×•×“×•×ª ×”×ª××™×›×”? |
|--------|---------------------------|
| ×”××™×©×•×¨ ×”××¨×›×–×™ \( w^T x + b = 0 \) | âŒ ×œ× ×—×™×™×‘ |
| ×’×‘×•×œ×•×ª ×”×¨×•×•×— \( w^T x + b = \pm1 \) | âœ… ×›×Ÿ, ×—×™×™×‘ |

×•×œ×›×Ÿ ×”×Ÿ × ×§×¨××•×ª **×•×§×˜×•×¨×™ ×ª××™×›×”** â€“ ×”×Ÿ ×××© **×ª×•××›×•×ª** ×‘××™×§×•× ×©×œ ×”×§×•! ğŸ’™

## ğŸ”“ ××¨×•×•×— ×¨×š â€“ Soft Margin ×•××¨×•×•×— ×§×©×” - Hard Margin

×‘×¢×•×œ× ×”×××™×ª×™ ×”× ×ª×•× ×™× ×œ× ×ª××™×“ ××•×¤×¨×“×™× ×‘×¦×•×¨×” ××•×©×œ××ª.

**Soft Margin:**
- ×××¤×©×¨ ×›××” ×˜×¢×•×™×•×ª ×§×˜× ×•×ª
- × ×•×ª×Ÿ ×œ××•×“×œ ×œ×”×™×•×ª **×’××™×© ×™×•×ª×¨**
- ×¢×•×–×¨ ×œ×× ×•×¢ **Overfitting**

**Hard Margin:**

×× ×™×—×™× ×©×”× ×ª×•× ×™× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×‘×¦×•×¨×” ××•×©×œ××ª â€“ ××™×Ÿ ×©×’×™××•×ª!

×›×œ×•××¨: ××¤×©×¨ ×œ××¦×•× ×§×• ×©××¤×¨×™×“ 100% × ×›×•×Ÿ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª.

×××¤×™×™× ×™×:

×œ× ×××¤×©×¨ ××£ ×˜×¢×•×ª (××™×Ÿ × ×§×•×“×•×ª ×‘×¦×“ ×”×œ× × ×›×•×Ÿ)

×“×•×¨×© ×©×”× ×ª×•× ×™× ×™×”×™×• ×œ×™× ×™××¨×™×ª × ×¤×¨×“×™× (linearly separable)

×××•×“ ×¨×’×™×© ×œ×¨×¢×© â€” × ×§×•×“×” ××—×ª ×©×’×•×™×” ×™×›×•×œ×” ×œ×”×¨×•×¡ ×”×›×•×œ

ğŸ“‰ ××ª×™ ×œ×”×©×ª××©:

×¨×§ ×›×©××ª×” ×‘×˜×•×— ×©××™×Ÿ ×—×¤×™×¤×” ×‘×™×Ÿ ×”××—×œ×§×•×ª

×œ× ××ª××™× ×œ×¨×•×‘ ×”××§×¨×™× ×”×××™×ª×™×™×

![Hard vs Soft Margin](svm4.png)

## âš™ï¸ ×¤×¨××˜×¨ C

×¤×¨××˜×¨ ×—×©×•×‘ ×××•×“ ×‘Ö¾SVM ×©××—×œ×™×˜ **×›××” × ××¤×©×¨ ×˜×¢×•×™×•×ª**:

| ×¢×¨×š C | ××” ×–×” ×¢×•×©×”? |
|-------|---------------|
| ×’×‘×•×”  | ×¤×—×•×ª ×¡×œ×—× ×™ ×œ×˜×¢×•×™×•×ª (××•×“×œ ×§×©×™×—, ×¤×—×•×ª ×’××™×©) |
| × ××•×š  | ×¡×œ×—× ×™ ×™×•×ª×¨ â€“ ×××¤×©×¨ ×©×’×™××•×ª ×§×˜× ×•×ª (××•×“×œ ×›×œ×œ×™ ×™×•×ª×¨) |

![Parameter C Illustration](svm5.png)

## ğŸŒŒ ×’×¨×¢×™×Ÿ â€“ Kernel

×›××©×¨ ×”× ×ª×•× ×™× **×œ× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×‘×§×• ×™×©×¨**, × ×©×ª××© ×‘×§×¨× ×œ×™× ×›×“×™ ×œ×”×¤×•×š ××ª ×”××¨×—×‘:

- × ×‘×¦×¢ **××™×¤×•×™ ×œÖ¾××¨×—×‘ ×—×“×©** (×œ×¨×•×‘ ×’×‘×•×” ×™×•×ª×¨)
- ×‘××¨×—×‘ ×”×—×“×© â€“ ×›×Ÿ × ×™×ª×Ÿ ×œ×”×¤×¨×™×“ ×‘×™× ×™×”× ×‘×§×• ×™×©×¨!

![Kernel Transformation](svm6.png)

## ğŸ© Kernel Trick

"×˜×¨×™×§ ××ª××˜×™" ×©×××¤×©×¨:
- ×œ×—×©×‘ ××ª **×”××›×¤×œ×” ×”×¤× ×™××™×ª ×‘××¨×—×‘ ×”×—×“×©**
- ×‘×œ×™ ×‘×××ª ×œ×—×©×‘ ××ª ×”××™×§×•× ×”×—×“×© ×©×œ ×›×œ × ×§×•×“×”!

×–×” ×—×•×¡×š **×”×¨×‘×” ×××•×“ ×–××Ÿ ×•×–×™×›×¨×•×Ÿ**.

×”-Kernel Trick ×”×•× ×”×˜×›× ×™×§×” ×©×××¤×©×¨×ª ×œ-SVM ×œ×”×ª××•×“×“ ×¢× ×‘×¢×™×•×ª ×¡×™×•×•×’ ×œ×-×œ×™× ××¨×™×•×ª ××‘×œ×™ ×œ×—×©×‘ ×‘××¤×•×¨×© ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” ×œ××¨×—×‘ ×’×‘×•×”-×××“×™. ×”×¨×¢×™×•×Ÿ ×”×‘×¡×™×¡×™ ×”×•×:

1. ×‘××§×•× ×œ×”×¤×¢×™×œ ×˜×¨× ×¡×¤×•×¨××¦×™×” $\phi$ ×¢×œ ×›×œ ×•×§×˜×•×¨ ×§×œ×˜ $x$ ×•-$y$ ×‘× ×¤×¨×“
2. ×•××– ×œ×—×©×‘ ××ª ××›×¤×œ×ª ×”× ×§×•×“×•×ª ×©×œ×”× $\phi(x) \cdot \phi(y)$
3. ×× ×—× ×• ××—×©×‘×™× ×™×©×™×¨×•×ª ××ª $K(x, y)$ ×©× ×•×ª×Ÿ ××ª ××•×ª×” ×ª×•×¦××”

×–×” ×—×•×¡×š ×–××Ÿ ×—×™×©×•×‘ ××©××¢×•×ª×™, ×‘××™×•×—×“ ×›××©×¨ ××¨×—×‘ ×”×ª×›×•× ×•×ª ×”×’×‘×•×”-×××“×™ ×™×›×•×œ ×œ×”×™×•×ª ××™× ×¡×•×¤×™ (×›××• ×‘-RBF Kernel).

### ×”×™×ª×¨×•× ×•×ª ×©×œ Kernel Trick:
- ×××¤×©×¨ ×œ-SVM ×œ×”×ª××•×“×“ ×¢× × ×ª×•× ×™× ×œ× ×œ×™× ××¨×™×™×
- ×—×•×¡×š ×‘×–××Ÿ ×—×™×©×•×‘ ×•×‘×©×™××•×© ×‘×–×™×›×¨×•×Ÿ
- ×××¤×©×¨ ×¢×‘×•×“×” ×‘××¨×—×‘×™ ×ª×›×•× ×•×ª ××™× ×¡×•×¤×™×™×
- ××©×¤×¨ ××ª ×”×“×™×•×§ ×‘×‘×¢×™×•×ª ×¡×™×•×•×’ ××•×¨×›×‘×•×ª

×”-Kernel Trick ×××¤×©×¨ ×œ× ×• ×œ×¢×‘×•×“ ×¢× ××¨×—×‘×™× ×‘×¢×œ×™ ×××“×™× ×’×‘×•×”×™×, ×œ×¢×ª×™× ××¤×™×œ×• ××™× ×¡×•×¤×™×™×, ××‘×œ×™ ×œ×©×œ× ××ª ×”××—×™×¨ ×”×—×™×©×•×‘×™ ×©×œ ×¢×‘×•×“×” ×‘××¨×—×‘×™× ××œ×•.

### ğŸ§  ××™×š ×§×¨× ×œ ××©×™×’ ××ª ××•×ª×” ×ª×•×¦××” ××‘×œ×™ ×œ×—×©×‘ ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×”?

#### âœ¨ ××” ×¢×•×©×” ×”×§×¨× ×œ?

×”×§×¨× ×œ ×¢×•×§×£ ××ª ×”×¦×•×¨×š ×œ×—×©×‘ ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” ×¢×¦××”. ×‘××§×•× ×–×”, ×”×•× ×¢×•×©×” ××©×”×• ×—×›×:

#### ×©×œ×‘×™× ×¤×©×•×˜×™×:

1. **×œ× ××—×•×©×‘ ××ª ×”××™×§×•× ×”×—×“×© ×©×œ ×›×œ × ×§×•×“×”** (×œ× ××—×©×‘ ××ª phi(x))
2. **×‘×•×“×§ ×¨×§ ××ª ×”×“××™×•×Ÿ** ×‘×™×Ÿ × ×§×•×“×•×ª:
   ×©×•××œ: "×›××” ×“×•××” x ×œ× ×§×•×“×” ××—×¨×ª x_i ×× ×”×™×™× ×• ××¢×‘×™×¨×™× ××ª ×©×ª×™×”×Ÿ ×œ××¨×—×‘ ×”×’×‘×•×”?"
3. ×”×§×¨× ×œ × ×•×ª×Ÿ ××ª ×”×ª×©×•×‘×” ×œ×©××œ×” ×”×–×• â€“ ×”×•× ××—×©×‘ ××ª ×”×“××™×•×Ÿ **×›××™×œ×• ×‘×•×¦×¢×” ×˜×¨× ×¡×¤×•×¨××¦×™×”**, ×‘×œ×™ ×œ×¢×©×•×ª ××•×ª×” ×‘×¤×•×¢×œ.
4. ×”××œ×’×•×¨×™×ª× ×©×œ SVM ××©×ª××© ×¨×§ ×‘×“××™×•×Ÿ ×”×–×” ×›×“×™ ×œ×”×—×œ×™×˜ ×× × ×§×•×“×” ×—×“×©×” ×©×™×™×›×ª ×œ×§×‘×•×¦×” ×”×—×™×•×‘×™×ª ××• ×”×©×œ×™×œ×™×ª.

#### ğŸ“¦ ×“×™××•×™ ×¤×©×•×˜:

×–×” ×›××™×œ×• ××ª×” ×¨×•×¦×” ×œ×“×¢×ª ×›××” ×“×•××•×ª ×©×ª×™ ×ª××•× ×•×ª â€”  
×‘××§×•× ×œ×¢×‘×•×¨ ×¢×œ ×›×œ ×”×¤×™×§×¡×œ×™× ×•×œ×—×©×‘ ××—×“ ××—×“, ××ª×” ×©×•××œ ××™×©×”×• ×©××‘×™×Ÿ ×—×–×•×ª×™×ª ×•×”×•× ×¤×©×•×˜ ××•××¨ ×œ×š "×××•×“ ×“×•××•×ª" ××• "×œ× ×“×•××•×ª ×‘×›×œ×œ".

#### âœ… ×•××– ×‘××” ×”× ×•×¡×—×”:

×”××•×“×œ ×©×œ SVM ××©×ª××© ×‘×“××™×•×Ÿ ×©×”×§×¨× ×œ × ×•×ª×Ÿ (×‘×™×Ÿ × ×§×•×“×•×ª) ×›×“×™ ×œ×—×–×•×ª ×œ××™×–×• ×§×‘×•×¦×” ×©×™×™×›×ª × ×§×•×“×” ×—×“×©×”.

## ğŸ“Œ ×¡×•×’×™ Kernels × ×¤×•×¦×™×:

- **Linear** â€“ ××ª××™× ×›×©××¤×©×¨ ×œ×”×¤×¨×™×“ ×‘×§×• ×™×©×¨
- **Polynomial** â€“ ××ª××™× ×›×©×™×© ×§×©×¨×™× ××•×¨×›×‘×™×
- **RBF (Gaussian)** â€“ ×‘×¨×™×¨×ª ××—×“×œ, ××ª××™× ×œ×”×¨×‘×” ×‘×¢×™×•×ª
- **Sigmoid** â€“ ×›××• × ×•×™×¨×•× ×™× ×‘×¨×©×ª ×¢×¦×‘×™×ª

![Different Kernel Types](https://scikit-learn.org/stable/_images/sphx_glr_plot_svm_kernels_001.png)

### Common types of Kernel Functions:

![Kernel Functions](svm10.jpg)

1. **Linear Kernel**:
   $K(x, y) = x \cdot y$

2. **Polynomial Kernel**:
   $K(x, y) = (\gamma x \cdot y + c)^d$
   where $\gamma > 0$, $c \geq 0$, and $d$ is an integer representing the degree of the polynomial.

3. **Radial Basis Function (RBF) or Gaussian Kernel**:
   $K(x, y) = \exp(-\gamma \|x - y\|^2)$
   where $\gamma > 0$, typically $\gamma = \frac{1}{2\sigma^2}$.

4. **Sigmoid Kernel**:
   $K(x, y) = \tanh(\gamma x \cdot y + c)$
   where $\gamma > 0$ and $c \geq 0$.

## Kernel Functions with Examples

**What is Î³ (Gamma)?**

Gamma is a hyperparameter that appears in several kernel functions, including the Polynomial kernel, RBF/Gaussian kernel, and Sigmoid kernel. It controls different aspects of the kernel's behavior.

**The "exp"** in the RBF/Gaussian kernel formula refers to the exponential function, which is commonly written as "exp" in mathematics and programming.

The exponential function exp(x) is equivalent to e^x, where "e" is Euler's number (approximately 2.71828...), a mathematical constant that forms the base of natural logarithms.

### 1. Linear Kernel
**Formula**: $K(x, y) = x \cdot y$

**Example**:
For two 2D vectors $x = [1, 2]$ and $y = [3, 4]$:

$K(x, y) = x \cdot y = 1 \times 3 + 2 \times 4 = 3 + 8 = 11$

**Use case**: Linear kernels work well when the data is already linearly separable. They're computationally efficient but cannot handle non-linear relationships in data.

### 2. Polynomial Kernel
**Formula**: $K(x, y) = (\gamma x \cdot y + c)^d$

where $\gamma > 0$, $c \geq 0$, and $d$ is the polynomial degree.

**Example**:
For vectors $x = [1, 2]$ and $y = [3, 4]$, with $\gamma = 1$, $c = 1$, and $d = 2$:

$K(x, y) = (1 \times (1 \times 3 + 2 \times 4) + 1)^2 = (11 + 1)^2 = 12^2 = 144$

**Use case**: Polynomial kernels are useful for problems where training data is not linearly separable. The degree $d$ determines the flexibility of the decision boundary. Common choices are $d = 2$ (quadratic) or $d = 3$ (cubic).

### 3. Radial Basis Function (RBF) / Gaussian Kernel
**Formula**: $K(x, y) = \exp(-\gamma \|x - y\|^2)$

where $\gamma > 0$, typically $\gamma = \frac{1}{2\sigma^2}$.

**Example**:
For vectors $x = [1, 2]$ and $y = [3, 4]$ with $\gamma = 0.5$:

1. Calculate the squared Euclidean distance: 
   $\|x - y\|^2 = (1-3)^2 + (2-4)^2 = 4 + 4 = 8$

2. Apply the RBF formula:
   $K(x, y) = \exp(-0.5 \times 8) = \exp(-4) \approx 0.018$

**Use case**: RBF kernels are versatile and work well for most types of data. They're especially effective when the relationship between classes is non-linear. The parameter $\gamma$ controls the "reach" of a single training example's influence.

### 4. Sigmoid Kernel
**Formula**: $K(x, y) = \tanh(\gamma x \cdot y + c)$

tanh = Hyperbolic Tangent 

where $\gamma > 0$ and $c \geq 0$.

**Example**:
For vectors $x = [1, 2]$ and $y = [3, 4]$, with $\gamma = 0.1$ and $c = 0$:

$K(x, y) = \tanh(0.1 \times (1 \times 3 + 2 \times 4)) = \tanh(0.1 \times 11) = \tanh(1.1) \approx 0.8$

**Use case**: The sigmoid kernel comes from neural networks (it's similar to using a neural network with one hidden layer). It's less commonly used in SVMs than RBF kernels but can be effective for specific problems.

### Choosing the Right Kernel

The choice of kernel depends on the specific problem:

- **Linear kernel**: When data is linearly separable
- **Polynomial kernel**: When you need a more flexible decision boundary with clear degree of separation
- **RBF kernel**: Most versatile, works well for most datasets when properly tuned
- **Sigmoid kernel**: Works for specific types of problems, often related to neural networks

In practice, it's common to try different kernels and use cross-validation to determine which one performs best for your specific dataset.

### Visual Intuition

To understand how kernels transform data:

1. **Linear**: Data remains in the same space, separated by a straight line
2. **Polynomial**: Data is mapped to a higher-dimensional space where curved boundaries in original space become linear boundaries
3. **RBF**: Essentially creates a "bump" around each data point, with the width controlled by $\gamma$
4. **Sigmoid**: Creates a decision boundary similar to that of a neural network

The kernel trick allows us to compute these separations without explicitly transforming the data to higher dimensions, making SVMs computationally efficient even for complex decision boundaries.

## ğŸ“š ×˜×¨× ×¡×¤×•×¨××¦×™×•×ª ×‘××›×•× ×•×ª ×œ×•××“×•×ª

### ğŸ§  ××” ×–×” ×˜×¨× ×¡×¤×•×¨××¦×™×”?

**×˜×¨× ×¡×¤×•×¨××¦×™×”** (Transformation) ×”×™× ×¤×¢×•×œ×” ××ª××˜×™×ª ×©××¢×‘×™×¨×” × ×§×•×“×” ×××§×•× ××—×“ ×œ××§×•× ××—×¨ â€“ ×œ×¨×•×‘ ×××™××“ ××¡×•×™× **×œ××™××“ ×’×‘×•×” ×™×•×ª×¨**.

×‘××™×œ×™× ×¤×©×•×˜×•×ª:  
×–×” ×›××• ×œ×§×—×ª × ×§×•×“×” ×‘××¨×—×‘ ×¤×©×•×˜ (×œ××©×œ ×§×• ×™×©×¨ â€“ ×—×“Ö¾××™××“×™), ×•×œ×”×¢×‘×™×¨ ××•×ª×” ×œ××¨×—×‘ ×¢×©×™×¨ ×™×•×ª×¨ (×œ××©×œ ××™×©×•×¨ ××• ××¨×—×‘ ×ª×œ×ªÖ¾××™××“×™), ×›×“×™ ×©×™×”×™×” ×œ× ×• **×™×•×ª×¨ ×§×œ ×œ×¢×©×•×ª ×“×‘×¨×™× ×›××• ×”×¤×¨×“×” ×‘×™×Ÿ ×§×‘×•×¦×•×ª**

![Transformation](svm11.png)

### âœ¨ ×œ××” ×¦×¨×™×š ×˜×¨× ×¡×¤×•×¨××¦×™×”?

×œ×¤×¢××™×, ×”×§×‘×•×¦×•×ª ×©×× ×—× ×• ×¨×•×¦×™× ×œ×”×¤×¨×™×“ **×œ× × ×™×ª× ×•×ª ×œ×”×¤×¨×“×” ×œ×™× ×™××¨×™×ª** ×‘××¨×—×‘ ×”××§×•×¨×™.  
××‘×œ ×× × ×¢×œ×” ×œ××™××“ ×’×‘×•×” ×™×•×ª×¨ â€“ ××•×œ×™ ×›×Ÿ × ×¦×œ×™×— ×œ×”×¤×¨×™×“ ×‘×™× ×™×”×Ÿ ×¢× ×§×• ×™×©×¨ (××• ××™×©×•×¨).

### ğŸ“Œ ×“×•×’××” ×¤×©×•×˜×”:

#### ×‘×¢×™×”:
×™×© ×œ× ×• × ×§×•×“×•×ª ×¢×œ ×¦×™×¨ \( x \), ×‘××™××“ ××—×“ (1D):

- × ×§×•×“×•×ª ×©×œ×™×œ×™×•×ª: \( x = -2, -1, 0, 1, 2 \)
- × ×§×•×“×” ×—×™×•×‘×™×ª ××—×ª: \( x = 0 \)

××™×Ÿ ×“×¨×š ×œ×©×™× **×§×• ××—×“ ×‘×¦×™×¨** ×©×™×¤×¨×™×“ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×‘×¦×•×¨×” ×˜×•×‘×”.

#### ×˜×¨× ×¡×¤×•×¨××¦×™×”:

× ×¢×©×” ×˜×¨× ×¡×¤×•×¨××¦×™×” ×¤×©×•×˜×”:

$$
\phi(x) = x^2
$$

×›×œ×•××¨ â€“ ×××¤×™× ××ª ×›×œ ×”× ×§×•×“×•×ª ×Ö¾1D ×œÖ¾2D, ×›×š ×©×”××™××“ ×”×—×“×© ×”×•× \( x^2 \)

#### ××” ×§×•×¨×” ×¢×›×©×™×•?

x = -2  â†’  phi(x) = 4  
x = -1  â†’  phi(x) = 1  
x =  0  â†’  phi(x) = 0  
x =  1  â†’  phi(x) = 1  
x =  2  â†’  phi(x) = 4  

×¤×ª××•×, ×›×œ ×”× ×§×•×“×•×ª ×©×œ×™×œ×™×•×ª ×”×Ÿ ×¨×—×•×§×•×ª ××”××¤×¡, ×•×”× ×§×•×“×” ×”×—×™×•×‘×™×ª \( x = 0 \) × ×©×œ×—×ª ×œÖ¾0.

âœ… ×¢×›×©×™×• ××¤×©×¨ ×œ×”×¤×¨×™×“ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×‘×¢×–×¨×ª ×§×• ×¤×©×•×˜ ×‘Ö¾2D (××• ××¤×™×œ×• ×‘Ö¾1D ×©×œ \( x^2 \)).

### ğŸ’¬ ×¡×™×›×•×:

- ×˜×¨× ×¡×¤×•×¨××¦×™×” ×”×™× ×©×™× ×•×™ ×™×™×¦×•×’ ×©×œ ×”× ×ª×•× ×™×.
- ×× ×—× ×• ××©×ª××©×™× ×‘×” ×›×“×™ ×œ×”×¤×•×š ×‘×¢×™×” ×©×§×©×” ×œ×¤×ª×•×¨ (×œ× ×œ×™× ×™××¨×™×ª) â€“ ×œ×‘×¢×™×” ×©×§×œ ×œ×¤×ª×•×¨ (×œ×™× ×™××¨×™×ª).
- ×œ× ×ª××™×“ ×¦×¨×™×š ×œ×××© ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” â€“ ×œ×¤×¢××™× × ×©×ª××© ×‘×§×¨× ×œ ×©×™×—×©×‘ ××ª ××” ×©×¦×¨×™×š ×‘×œ×™ ×œ×¢×‘×•×¨ ×œ××¨×—×‘ ×‘×¤×•×¢×œ.

## ğŸ” ×“×•×’×× - ×‘×¢×™×™×ª XOR

×‘×¢×™×™×ª XOR ×”×™× ×“×•×’×× ×§×œ××¡×™×ª ×œ× ×ª×•× ×™× ×©×œ× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×‘×§×• ×™×©×¨:

![XOR Problem](https://miro.medium.com/max/1400/1*_7OPgojau8hkiPUiHoGK_w.png)

×¢× ×§×¨× ×œ RBF × ×™×ª×Ÿ ×œ×¤×ª×•×¨ ××ª ×‘×¢×™×™×ª XOR ×‘×§×œ×•×ª.

## ğŸ“ˆ ×©×™××•×©×™× × ×¤×•×¦×™× ×©×œ SVM

- **×–×™×”×•×™ ×˜×§×¡×˜ ×•×›×ª×‘ ×™×“**
- **×¡×™×•×•×’ ×ª××•× ×•×ª**
- **×–×™×”×•×™ ×¤× ×™×**
- **×—×™×–×•×™ ×‘××“×¢×™ ×”×¨×¤×•××”**
- **× ×™×ª×•×— ×¨×’×©×•×ª ×‘×˜×§×¡×˜**

## ğŸ§  ××™×š SVM ××ª××•×“×“ ×¢× ×™×•×ª×¨ ××©×ª×™ ×§×‘×•×¦×•×ª?

### ğŸ¯ ×”×‘×¢×™×”:
SVM "×§×œ××¡×™" × ×•×¢×“ ×œ×‘×¢×™×” ×©×œ **×©× ×™ ×¡×•×’×™× ×‘×œ×‘×“**:
- ××—×œ×§×” ×—×™×•×‘×™×ª: \( +1 \)
- ××—×œ×§×” ×©×œ×™×œ×™×ª: \( -1 \)

××‘×œ ××” ×¢×•×©×™× ×›×©×™×© **×©×œ×•×© ×§×‘×•×¦×•×ª ××• ×™×•×ª×¨**? (×œ××©×œ A, B, C)

### âœ… ×¤×ª×¨×•× ×•×ª × ×¤×•×¦×™×:

#### 1. One-vs-Rest (OvR) â€“ "××—×“ ××•×œ ×›×œ ×”×©××¨"

- ×× ×™×© 3 ×§×‘×•×¦×•×ª (A, B, C) â†’ × ×‘× ×” 3 ××•×“×œ×™×:
  - ××•×“×œ 1: A ××•×œ (B ×•Ö¾C)
  - ××•×“×œ 2: B ××•×œ (A ×•Ö¾C)
  - ××•×“×œ 3: C ××•×œ (A ×•Ö¾B)

- ×›×œ ××•×“×œ ××××Ÿ SVM ×‘×™× ××¨×™.
- ×›×©×‘×•×“×§×™× ×“×•×’××” ×—×“×©×”:
  - ××¨×™×¦×™× ××ª ×©×œ×•×©×ª ×”××•×“×œ×™×.
  - ×‘×•×—×¨×™× ××ª ×”×§×‘×•×¦×” ×¢× ×”×¦×™×•×Ÿ ×”×’×‘×•×” ×‘×™×•×ª×¨.

#### 2. One-vs-One (OvO) â€“ "×›×œ ×–×•×’ ××•×œ ×–×•×’"

- × ×‘× ×” SVM ×¢×‘×•×¨ ×›×œ **×–×•×’ ×§×‘×•×¦×•×ª**.
- ×œ×“×•×’××”, ×¢×‘×•×¨ ×§×‘×•×¦×•×ª A, B, C â†’ × ×‘× ×”:
  - A ××•×œ B
  - A ××•×œ C
  - B ××•×œ C
- ×¢×‘×•×¨ \( k \) ×§×‘×•×¦×•×ª ×™×©:

$$
\frac{k(k - 1)}{2}
$$

  ××•×“×œ×™× ×©×•× ×™×.

- ×›×©×‘×•×“×§×™× ×“×•×’××” ×—×“×©×”:
  - ×›×œ ××•×“×œ × ×•×ª×Ÿ "×”×¦×‘×¢×”".
  - ×”×§×‘×•×¦×” ×©×–×•×›×” ×‘×”×›×™ ×”×¨×‘×” ×”×¦×‘×¢×•×ª ×”×™× ×”×–×•×›×”.

### ğŸ¤– ×‘×¤×•×¢×œ â€“ ×¢× Scikit-learn:
- ×× ×ª×©×ª××© ×‘Ö¾`SVC` (×¡×¤×¨×™×™×ª `sklearn.svm`) â€” ××™×Ÿ ×¦×•×¨×š ×œ×˜×¤×œ ×‘×–×” ×™×“× ×™×ª!
- ×›×‘×¨×™×¨×ª ××—×“×œ, ×”××œ×’×•×¨×™×ª× ××¤×¢×™×œ **One-vs-One** ×‘××•×¤×Ÿ ××•×˜×•××˜×™.

### ğŸ’¡ ×¡×™×›×•×:

| ××¡' ×§×‘×•×¦×•×ª | ×¤×ª×¨×•×Ÿ SVM               |
|------------|-------------------------|
| 2          | SVM ×¨×’×™×œ                |
| >2         | One-vs-Rest ××• One-vs-One |

## ğŸ© ×›×™×¦×“ ××•×¦××™× ××ª ×•×§×˜×•×¨ ×”××©×§×œ×™× \( w \) ×‘Ö¾SVM

### ğŸ¯ ×”××˜×¨×” ×©×œ SVM
×œ××¦×•× ××ª ×”×§×• (××• ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ) ×©××¤×¨×™×“ ×”×›×™ ×˜×•×‘ ×‘×™×Ÿ ×©×ª×™ ×§×‘×•×¦×•×ª, ×›×š ×©×”××¨×—×§ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ××›×œ ×¦×“ (×”Ö¾**support vectors**) ××œ ×”×§×• ×™×”×™×” **×”×›×™ ×’×“×•×œ ×©××¤×©×¨**.

### ğŸ”¢ ××™×š ××•×¦××™× ××ª \( w \)?
×”××•×“×œ ××’×“×™×¨ ×‘×¢×™×” ××ª××˜×™×ª ×©×œ **××•×¤×˜×™××™×–×¦×™×”** (××¦×™××ª ××§×¡×™××•×/××™× ×™××•×)

#### 1. × ×•×¡×—×ª ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ:
$$
f(x) = w^T x + b
$$

××:

$$f(x) \geq 1$$

â†’ ×”×“×’×™××” ×©×™×™×›×ª ×œ××—×œ×§×” ×”×—×™×•×‘×™×ª (label = +1)

$$f(x) \leq -1$$ 

â†’ ×”×“×’×™××” ×©×™×™×›×ª ×œ××—×œ×§×” ×”×©×œ×™×œ×™×ª (label = -1)

#### 2. ×ª× ××™ ×”×”×¤×¨×“×”:
×œ×›×œ ×“×•×’××” Xi Yi:

$$
y_i (w^T x_i + b) \geq 1
$$

**××” ×–×” Yi**

×”- Yi ×–×” ×”×ª×•×•×™×ª (label) ×©×œ ×”×“×•×’××” ×”Ö¾ i

×›×œ Xi ×”×•× ×•×§×˜×•×¨

×›×œ Yi ×”×•× ××¡×¤×¨ ×©××•××¨ ×œ××™×–×” ×§×‘×•×¦×” ×©×™×™×›×ª ×”×“×•×’××, ×œ×§×‘×•×¦×” ×”×—×™×•×‘×™×ª ××• ×œ×§×‘×•×¦×” ×”×©×œ×™×œ×™×ª

#### 3. ×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” (Objective Function):
×›×“×™ ×œ××§×¡× ××ª ×”××¨×—×§ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª, × ××–×¢×¨ ××ª ×’×•×“×œ \( w \):

$$
\min \left( \frac{1}{2} \|w\|^2 \right)
$$

×ª×—×ª ×”×”×’×‘×œ×”:

$
y_i (w^T x_i + b) \geq 1 \quad \forall i
$

### â“ ×œ××” ×¦×¨×™×š ×œ×”×§×˜×™×Ÿ ××ª \( \|w\| \) ×‘Ö¾SVM?

#### âœ¨ ×”×”×¡×‘×¨: ×›×œ ×”×¡×•×“ × ××¦× ×‘Ö¾**Margin** â€“ ×”××¨×•×•×— ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª

##### ×”××¨×—×§ ×©×œ × ×§×•×“×” ××”××™×©×•×¨:
×œ×¤×™ ×”× ×•×¡×—×”:

$
\text{Distance from hyperplane} = \frac{|w^T x + b|}{\|w\|}
$

##### ×”××˜×¨×” ×©×œ SVM:
×œ××¦×•× ××™×©×•×¨ ×©××¤×¨×™×“ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×¢× **×”××¨×—×§ ×”×›×™ ×’×“×•×œ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨** â€” ×›×œ×•××¨, ××¨×•×•×— (margin) ××§×¡×™××œ×™.

##### ×ª× ××™ ×”×”×¤×¨×“×”:

$
y_i(w^T x_i + b) \geq 1
$

×”× ×§×•×“×•×ª ×”×›×™ ×§×¨×•×‘×•×ª ×œ××™×©×•×¨ ×”×Ÿ ×”Ö¾**Support Vectors**, ×©××§×™×™××•×ª:

$
y_i(w^T x_i + b) = 1
$

##### âœ… ×”××¨×—×§ ×©×œ×”×Ÿ ××”××™×©×•×¨:

$
\text{margin} = \frac{1}{\|w\|}
$

##### ×•×œ×›×Ÿ:
- ×›×›×œ ×©Ö¾\( \|w\| \) **×§×˜×Ÿ ×™×•×ª×¨**, ×”××¨×•×•×— **×’×“×•×œ ×™×•×ª×¨**.
- ×›×œ×•××¨: ×× × ×§×˜×™×Ÿ ××ª \( \|w\| \), ×× ×—× ×• **××¨×—×™×§×™×** ××ª ×”××™×©×•×¨ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ â€” ×•×–×” ×‘×“×™×•×§ ××” ×©×× ×—× ×• ×¨×•×¦×™×!

##### ğŸ§  ×•×œ×›×Ÿ ×‘×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” ×©×œ SVM:
×× ×—× ×• **×××–×¢×¨×™×** ××ª:

$
\frac{1}{2} \|w\|^2
$

×›×“×™ ×œ××¦×•× ××ª ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ ×¢× **margin ××§×¡×™××œ×™** ×•×œ×•×•×“× ×”×¤×¨×“×” ×˜×•×‘×” ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª.

### â“ ×œ××” ×××–×¢×¨×™× ××ª 

$ 
\frac{1}{2} \|w\|^2 
$

×•×œ× ×¤×©×•×˜ ××ª 

$ 
\|w\| 
$

×‘Ö¾SVM?

#### âœ… ×”××˜×¨×” ×”××§×•×¨×™×ª:
×× ×—× ×• ×¨×•×¦×™× ×œ××§×¡× ××ª ×”××¨×•×•×— (**margin**) ×‘×™×Ÿ ×©×ª×™ ×”×§×‘×•×¦×•×ª.

#### ×”××¨×•×•×— ××•×’×“×¨ ×›:

$
\text{margin} = \frac{1}{\|w\|}
$

×›×“×™ **×œ××§×¡×** ××ª ×”××¨×•×•×— â€” ×¦×¨×™×š **×œ××–×¢×¨** ××ª:

$
\|w\|
$

××– ×œ××” ×××–×¢×¨×™× ×“×•×•×§× ××ª 

$ 
\frac{1}{2} \|w\|^2 
$

?

#### ×¡×™×‘×•×ª ××ª××˜×™×•×ª:

**× ×’×–×¨×•×ª ×¤×©×•×˜×•×ª ×™×•×ª×¨**:

×× × ×’×“×™×¨ ××ª ×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” ×›×š:

$
\frac{1}{2} \|w\|^2
$

××– ×”× ×’×–×¨×ª ×©×œ×” ×”×™× ×¤×©×•×˜ 

$ 
w 
$

×•××™×Ÿ ×¦×•×¨×š ×‘×©×•×¨×©×™× ××• × ×’×–×¨×•×ª ××•×¨×›×‘×•×ª

×” 1/2 ×”×•× ×¨×§ ×§×™×¦×•×¨ ×“×¨×š ×”×•× ×œ× ××©×¤×™×¢ ×¢×œ ×¤×ª×¨×•×Ÿ ×”××•×¤×˜×™××™×–×¦×™×” â€” ×–×” ×¨×§ ××§×œ ×¢×œ ×”×—×™×©×•×‘

#### ğŸ’¡ ×¡×™×›×•×:
- **××–×¢×•×¨** 

$ 
\|w\| 
$ 

= ×”×’×“×œ×ª ×”××¨×•×•×—.

- **××‘×œ ×‘×¤×•×¢×œ ×××–×¢×¨×™×** 

$ 
\frac{1}{2} \|w\|^2 
$

×›×™ ×–×” ×”×¨×‘×” ×™×•×ª×¨ × ×•×— ××‘×—×™× ×” ××ª××˜×™×ª.

- ×•×”×¤×ª×¨×•×Ÿ ×©××ª×§×‘×œ ×–×”×” â€” ××– ×–×” ×—×›× ×•×™×¢×™×œ ×™×•×ª×¨ ğŸ’™

## ğŸ¤– ××™×š ×¤×•×ª×¨×™× ××ª ×–×” ×‘×¤×•×¢×œ?

1. ××©×ª××©×™× ×‘×©×™×˜×” ××ª××˜×™×ª ×‘×©× **Lagrange Multipliers**.
2. ×¤×•×ª×¨×™× ××ª ×”×‘×¢×™×” ×”×›×¤×•×œ×” (Dual Problem).
3. ×”×¤×ª×¨×•×Ÿ ××‘×•×¡×¡ ×¨×§ ×¢×œ ×”Ö¾**Support Vectors** (×”× ×§×•×“×•×ª ×”×›×™ ×§×¨×•×‘×•×ª ×œ×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ).
4. ××”× ××—×©×‘×™× ××ª \( w \) ×›×š:

$
w = \sum_i \alpha_i y_i x_i
$

×›××©×¨:

$
\alpha_i
$

-  ×”× ×¤×¨××˜×¨×™× ×©×§×•×‘×¢×™× ××ª ×—×©×™×‘×•×ª ×›×œ ×“×•×’××”
- ×¨×§ ×¢×‘×•×¨ ×”Ö¾support vectors ×™×© alpha ×©×•× ×” ×××¤×¡

### ğŸ’¡ ×¡×™×›×•×
- ×× ×—× ×• ×œ× ××—×©×‘×™× ××ª \( w \) ×™×©×™×¨×•×ª, ××œ× ×¤×•×ª×¨×™× ×‘×¢×™×™×ª ××•×¤×˜×™××™×–×¦×™×”.
- ×”××˜×¨×” ×”×™× ×œ××¦×•× ××ª ×”×§×• ×©××¤×¨×™×“ ×”×›×™ ×˜×•×‘ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×¢× **margin** ××§×¡×™××œ×™.
- ×”×ª×•×¦××”: ××©×•×•××” ×©××‘×•×¡×¡×ª ×¨×§ ×¢×œ ×”Ö¾support vectors.

## ğŸ“˜ ××™×š SVM ×‘×•×—×¨ ××ª × ×§×•×“×•×ª ×”×ª××™×›×” (Support Vectors)

### 1. ×”××˜×¨×” ×©×œ SVM

×‘×©×œ×‘ ×”××™××•×Ÿ, ×”××•×“×œ ×× ×¡×” ×œ××¦×•× **××ª ×”××™×©×•×¨ ×©××¤×¨×™×“ ×”×›×™ ×˜×•×‘** ×‘×™×Ÿ ×©×ª×™ ×”×§×‘×•×¦×•×ª, ×›×š ×©×”××¨×—×§ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ (×”Ö¾margin) ×™×”×™×” ×”×›×™ ×’×“×•×œ ×©××¤×©×¨.

### 2. ×”× ×•×¡×—×” ×”××ª××˜×™×ª

×”××œ×’×•×¨×™×ª× ×¤×•×ª×¨ ×‘×¢×™×™×ª ××•×¤×˜×™××™×–×¦×™×”:

- ×××–×¢×¨ ××ª:
  
  (1/2) * ||w||Â²

- ×ª×—×ª ×ª× ××™× ×©×œ ×”×¤×¨×“×”:

  ×œ×›×œ × ×§×•×“×” ××”××™××•×Ÿ:  
  yáµ¢ * (wáµ€ * xáµ¢ + b) â‰¥ 1
- **x_i** â€” × ×§×•×“×” ××”×¡×˜ ×©×œ ×”××™××•×Ÿ
- **y_i** â€” ×”×¡×™×•×•×’ ×©×œ x_i (××• +1 ××• -1)

### 3. ×©×™××•×© ×‘Ö¾Lagrange Multipliers

×›×“×™ ×œ×¤×ª×•×¨ ××ª ×–×”, SVM ××©×ª××© ×‘×©×™×˜×” ×©× ×§×¨××ª *×©×™×˜×ª ×œ×’×¨×× ×–'* (Lagrange multipliers).  
×”×™× ××•×¡×™×¤×” ××©×ª× ×™× ×—×“×©×™×: Î±áµ¢ (alpha_i) â€” ××—×“ ×œ×›×œ × ×§×•×“×”.

×”×¤×ª×¨×•×Ÿ ×©×œ ×”×‘×¢×™×” ××•×‘×™×œ ×œ× ×•×¡×—×” ×¢×‘×•×¨ ×•×§×˜×•×¨ w:

w = Î£ (Î±áµ¢ * yáµ¢ * xáµ¢)

××‘×œ â€” ×‘×¨×•×‘ ×”× ×§×•×“×•×ª ×™×•×¦×:

Î±áµ¢ = 0

- **x_i** â€” × ×§×•×“×” ××”×¡×˜ ×©×œ ×”××™××•×Ÿ
- **y_i** â€” ×”×¡×™×•×•×’ ×©×œ x_i (××• +1 ××• -1)
- **alpha_i** â€” ×›××” ×—×©×•×‘×” ×”× ×§×•×“×” ×”×–×• ×‘××•×“×œ, × ×§×‘×¢ ×‘××™××•×Ÿ
  
×¨×§ × ×§×•×“×•×ª ×©Ö¾**alpha_i > 0** ××©×ª×ª×¤×•×ª ×‘×¤×•×¢×œ â€” ××œ×• ×”×Ÿ × ×§×•×“×•×ª ×”×ª××™×›×” (Support Vectors).

### 4. ××™ × ×§×•×“×ª ×ª××™×›×”?

× ×§×•×“×” ×”×•×¤×›×ª ×œ×”×™×•×ª Support Vector ××:

- ×™×© ×œ×” Î±áµ¢ > 0
- ×›×œ×•××¨ â€” ×”×™× **×™×•×©×‘×ª ×‘×“×™×•×§ ×¢×œ ×’×‘×•×œ ×”×¨×•×•×— (margin)**

×¨×§ ×”× ×§×•×“×•×ª ×©× ××¦××•×ª ×‘×“×™×•×§ "×¢×œ ×”×§×¦×”" â€” ×”×›×™ ×§×¨×•×‘×•×ª ×œ××™×©×•×¨ â€” ×”×Ÿ ××œ×• ×©×‘×××ª ××©×¤×™×¢×•×ª ×¢×œ ×¦×•×¨×ª ×”×”×¤×¨×“×”.

### 5. ×›×œ ×”×©××¨? × ×–×¨×§×•×ª ×”×¦×™×“×”

×× × ×§×•×“×” ×¨×—×•×§×” ××”××™×©×•×¨, SVM ××–×”×” ×©×”×™× ×œ× ×‘×××ª ××©× ×” ××ª ×”×”×—×œ×˜×” â€“  
××– ×”×•× × ×•×ª×Ÿ ×œ×” Î±áµ¢ = 0, ×•×”×™× ×œ× ×ª×©×ª×ª×£ ×‘×—×™×©×•×‘×™× ×©×œ ×”××•×“×œ.

### ğŸ’¡ ×œ×›×Ÿ:

- **Support Vectors ×”×Ÿ ×”× ×§×•×“×•×ª ×©×§×•×‘×¢×•×ª ××ª ×”××™×§×•× ×©×œ ×”××™×©×•×¨**
- ×•×”×Ÿ ×”×™×—×™×“×•×ª ×©"××•×©×›×•×ª" ××• "×“×•×—×¤×•×ª" ××ª ×”×§×• ×‘×¢×ª ×”×—×™×–×•×™

## ğŸ§  ××™×¤×” ×¢×•×‘×¨ ×”×§×• ×”××¤×¨×™×“ ×‘-SVM?

### âœ¨ ×”××˜×¨×” ×©×œ SVM

SVM ××—×¤×© ××™×©×•×¨ (×§×•) ×©××¤×¨×™×“ ×‘×™×Ÿ ×©×ª×™ ×§×‘×•×¦×•×ª â€” ××‘×œ ×œ× ×¡×ª× ×›×œ ×§×•.  
××œ× ×›×–×” ×©× ××¦× **×‘×“×™×•×§ ×‘×××¦×¢** ×‘×™×Ÿ ×©×ª×™ ×”×§×‘×•×¦×•×ª, ×¢× **××¨×—×§ ××§×¡×™××œ×™** ×œ×©×ª×™ ×”×§×‘×•×¦×•×ª.

### âœ… ×©×œ×•×©×ª ×”×§×•×•×™× ×©-SVM ××©×ª××© ×‘×”×:

1. **×”××™×©×•×¨ ×”××¨×›×–×™ (×”×§×œ××¡×™):**

    ×–×” ×”×§×• ×©××¤×¨×™×“ ×‘×¤×•×¢×œ:
    ```
    wáµ€ x + b = 0
    ```

2. **×§×•×•×™ ×”-margin (×”×§×•×•×™× ×©× ×•×’×¢×™× ×‘× ×§×•×“×•×ª ×”×ª××™×›×”):**

    - ×¢×‘×•×¨ ×”×§×‘×•×¦×” ×”×—×™×•×‘×™×ª:  
      ```
      wáµ€ x + b = +1
      ```

    - ×¢×‘×•×¨ ×”×§×‘×•×¦×” ×”×©×œ×™×œ×™×ª:  
      ```
      wáµ€ x + b = -1
      ```

### ğŸ“ ××™×¤×” ×¢×•×‘×¨ ×”×§×• ×”××¤×¨×™×“?

×”×§×• ×”××¤×¨×™×“ ×¢×•×‘×¨ ×‘×“×™×•×§ ×‘×××¦×¢ ×‘×™×Ÿ ×©× ×™ ×§×•×•×™ ×”-margin.

## ×“×•×’×××•×ª ×§×•×“ ×‘×¤×™×™×ª×•×Ÿ

×œ×”×œ×Ÿ ××¡×¤×¨ ×“×•×’×××•×ª ×§×•×“ ×”××“×’×™××•×ª ××ª ×”×©×™××•×© ×‘-SVM ×‘×¡×¤×¨×™×™×ª scikit-learn:

### ×“×•×’××” ×‘×¡×™×¡×™×ª ×©×œ ×¡×™×•×•×’ ×‘×™× ××¨×™
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm

# Create a simple dataset for apples and bananas
# Using two features: sweetness (x-axis) and weight (y-axis)
apples = np.array([[3, 150], [4, 130], [2, 160], [3, 140], [3.5, 145]])
bananas = np.array([[7, 120], [6, 110], [8, 115], [7.5, 125], [6.5, 118]])

# Combine features and create labels (-1 for apples, 1 for bananas)
X = np.vstack([apples, bananas])
y = np.array([-1, -1, -1, -1, -1, 1, 1, 1, 1, 1])

# Create and train the SVM model
# Using a linear kernel for simplicity
clf = svm.SVC(kernel='linear', C=1000)  # clf=classifier
clf.fit(X, y)

# Extract the model parameters
w = clf.coef_[0]  # The weights (normal vector to the hyperplane)
b = clf.intercept_[0]  # The bias (b in wÂ·x + b = 0)
print(f"Model weights (w): {w}")
print(f"Model bias (b): {b}")

# Create a new test point
test_point = np.array([[5, 135]])  # A point with sweetness=5, weight=135
prediction = clf.predict(test_point)[0]
class_name = "Banana" if prediction == 1 else "Apple"
print(class_name)
```

![SVM Binary Classification](svm7.png)

### ×“×•×’××” ×©×œ ×¡×™×•×•×’ ×¨×‘-×§×˜×’×•×¨×™××œ×™ (×™×•×ª×¨ ××©×ª×™ ×§×‘×•×¦×•×ª)
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.preprocessing import StandardScaler
from itertools import combinations

# Create a dataset for apples, bananas, and oranges
# Features: sweetness (x-axis) and weight (y-axis)
apples = np.array([[3, 150], [4, 130], [2, 160], [3, 140], [3.5, 145]])
bananas = np.array([[7, 120], [6, 110], [8, 115], [7.5, 125], [6.5, 118]])
oranges = np.array([[5, 180], [4.5, 195], [5.5, 185], [6, 175], [4.8, 190]])

# Combine features and create labels (0 for apples, 1 for bananas, 2 for oranges)
X = np.vstack([apples, bananas, oranges])
y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])

# Scale the features (important for SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create and train the SVM model
# For multiclass problems, SVM creates multiple binary classifiers (one-vs-one by default)
clf = svm.SVC(kernel='linear', C=1000, decision_function_shape='ovr')
clf.fit(X_scaled, y)

# Create a test point
test_point = np.array([[5, 150]])  # A point with sweetness=5, weight=150
test_point_scaled = scaler.transform(test_point)
prediction = clf.predict(test_point_scaled)[0]
class_names = ["Apple", "Banana", "Orange"]
predicted_class = class_names[prediction]

print(f"Test point: Sweetness={test_point[0][0]}, Weight={test_point[0][1]}")
print(f"Predicted class: {predicted_class}")
```

![SVM Multiclass Classification](svm8.png)

### ×—×œ×•×§×” ×œ-train-test ×•×‘×“×™×§×ª ×“×™×•×§
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns

# Create a dataset with 20 points manually
# Class 0: Points forming a cluster on the left
# Class 1: Points forming a cluster on the right
X = np.array([
    # 10 points for Class 0
    [2, 3], [1, 2], [2, 2], [1, 3], [3, 2],
    [2, 1], [1, 1], [3, 3], [2.5, 2], [1.5, 2.5],
    
    # 10 points for Class 1
    [6, 5], [5, 6], [7, 6], [6, 7], [5, 5],
    [7, 5], [6, 6], [5, 7], [7, 7], [6, 4]
])

# Create labels (0 for first 10 points, 1 for last 10 points)
y = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

# Split the data into training and testing sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Scale the features for better SVM performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create and train the SVM model
clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test_scaled)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print performance metrics
print(f"Model Accuracy: {accuracy:.2f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.tight_layout()
plt.show()
```

![Train-Test Split Results](svm12.png)
![Confusion Matrix](svm13.png)

### ×©×™××•×© ×‘-GridSearchCV ×œ××¦×™××ª ×”×¤×¨××˜×¨×™× ×”××™×“×™××œ×™×™×
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from time import time

# Load the digits dataset (larger dataset with 1797 samples, 64 features, 10 classes)
digits = datasets.load_digits()
X = digits.data
y = digits.target

print(f"Dataset shape: {X.shape} - {X.shape[0]} samples, {X.shape[1]} features")
print(f"Number of classes: {len(np.unique(y))}")

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define a smaller parameter grid to make computation more manageable
# but still diverse enough to show differences between kernels
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 0.01, 0.1],  # scale gamma:  1 / (n_features * [variance of X.var])
    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],
    'degree': [2, 3]  # Only relevant for poly kernel
}

# Create an SVM classifier
svm_clf = svm.SVC(probability=True, random_state=42)

# Set up GridSearchCV
print("Starting grid search. This might take a few minutes with this larger dataset...")
start_time = time()
grid_search = GridSearchCV(
    estimator=svm_clf,
    param_grid=param_grid,
    cv=3,  # Reduce to 3-fold cross-validation for speed
    n_jobs=-1,  # Use all available cores
    verbose=1,
    scoring='accuracy',
    return_train_score=True
)

# Perform the grid search
grid_search.fit(X_train_scaled, y_train)
search_time = time() - start_time

# Print the best parameters
print(f"\nGrid search completed in {search_time:.2f} seconds")
print(f"Best parameters: {grid_search.best_params_}")
```

![Grid Search Results](svm14.png)

## ×”×©×•×•××” ×‘×™×Ÿ ×¡×•×’×™ ×§×¨× ×œ×™× ×©×•× ×™×

×›××©×¨ ××“×•×‘×¨ ×¢×œ ×¤×¨××˜×¨ ×”-gamma, ×”×•× ××©×¤×™×¢ ××©××¢×•×ª×™×ª ×¢×œ ×”×ª× ×”×’×•×ª ×”××•×“×œ:

**The gamma parameter** ×”×•× ×”×™×¤×¨-×¤×¨××˜×¨ ×—×©×•×‘ ×‘-SVM ×”×§×•×‘×¢ ××ª ×”×”×©×¤×¢×” ×©×œ ×“×•×’×××•×ª ××™××•×Ÿ ×‘×•×“×“×•×ª. ×”×•× ××©××© ×‘×¢×™×§×¨ ×‘×§×¨× ×œ×™× ×œ×-×œ×™× ××¨×™×™× ×›××• RBF, ×¤×•×œ×™× ×•××™××œ×™, ×•×¡×™×’××•××™×“.

- ×¢×¨×›×™ gamma ×’×‘×•×”×™× (×œ××©×œ 10, 100):
  - ×™×•×¦×¨×™× ×’×‘×•×œ ×”×—×œ×˜×” ×¢× ×¢×§×•××•×ª "×¦×¨×•×ª" ×¡×‘×™×‘ × ×§×•×“×•×ª × ×ª×•× ×™× ×‘×•×“×“×•×ª

- ×¢×¨×›×™ gamma × ××•×›×™× (×œ××©×œ 0.001, 0.01):
  - ×™×•×¦×¨×™× ×’×‘×•×œ ×”×—×œ×˜×” ×¢× ×¢×§×•××•×ª ×—×œ×§×•×ª ×•×¨×—×‘×•×ª ×™×•×ª×¨

![Gamma Parameter Effect](svm15.png)

### ×”×©×•×•××” ×—×–×•×ª×™×ª ×‘×™×Ÿ ×¡×•×’×™ ×§×¨× ×œ×™×

![Kernel Comparison](svm9.png)

## ×¡×™×›×•×

SVM ×”×•× ××œ×’×•×¨×™×ª× ×—×–×§ ×•× ×¤×•×¥ ×‘×œ××™×“×ª ××›×•× ×” ×”×××¤×©×¨:

1. **×¡×™×•×•×’ ×œ×™× ××¨×™** - ×›××©×¨ ×”× ×ª×•× ×™× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×‘×§×• ×™×©×¨
2. **×¡×™×•×•×’ ×œ×-×œ×™× ××¨×™** - ×‘×××¦×¢×•×ª ×©×™××•×© ×‘×§×¨× ×œ×™× ×©×•× ×™×
3. **×¨×’×¨×¡×™×”** - ×œ×—×™×–×•×™ ×¢×¨×›×™× ×¨×¦×™×¤×™×

×”×™×ª×¨×•× ×•×ª ×”×¢×™×§×¨×™×™× ×©×œ SVM ×›×•×œ×œ×™×:
- ×™×¢×™×œ×•×ª ×‘××¨×—×‘×™× ×’×‘×•×”×™ ××™××“
- ×–×™×›×¨×•×Ÿ ×™×¢×™×œ (×©×™××•×© ×¨×§ ×‘×ª×ª-×§×‘×•×¦×” ×©×œ × ×§×•×“×•×ª ×”××™××•×Ÿ - ×•×§×˜×•×¨×™ ×”×ª××™×›×”)
- ×©×™××•×© ×‘×§×¨× ×œ×™× ×©×•× ×™× ×”×××¤×©×¨×™× ×’××™×©×•×ª

×—×¡×¨×•× ×•×ª ×¢×™×§×¨×™×™×:
- ×§×•×©×™ ×‘×”×ª××•×“×“×•×ª ×¢× ××¡×¤×¨ ×’×“×•×œ ×©×œ ×“×•×’×××•×ª
- ×‘×—×™×¨×ª ×”×§×¨× ×œ ×•×”×¤×¨××˜×¨×™× ×”××ª××™××™× ×™×›×•×œ×” ×œ×”×™×•×ª ×××ª×’×¨×ª

SVM × ×©××¨ ××œ×’×•×¨×™×ª× ×¤×•×¤×•×œ×¨×™ ×œ×¡×™×•×•×’, ×‘××™×•×—×“ ×‘××§×¨×™× ×‘×”× ××¡×¤×¨ ×”×ª×›×•× ×•×ª ×’×“×•×œ ×‘×™×—×¡ ×œ××¡×¤×¨ ×”×“×•×’×××•×ª.