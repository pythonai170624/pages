# Topic Modeling 

## 🧠 מה זה Topic Modeling?

ה- Topic Modeling הוא שיטה ב־NLP שמטרתה **לגלות נושאים סמויים** מתוך טקסטים לא מתויגים  
(כלומר: Unsupervised Learning).  
זהו מודל סטטיסטי שמנסה להבין על מה מדברים בטקסט, מבלי שהתווינו לו תגיות מראש

## 📌 מה זה Topic?

ה- "Topic" (נושא) הוא אוסף מילים שמופיעות הרבה יחד ומרמזות על נושא משותף  
לדוגמה: נושא על פירות עשוי לכלול את המילים "apple", "banana", "orange"

## 🛠️ למה משתמשים בזה?

- **המלצות תוכן** – התאמת מאמרים דומים לפי נושאים
- **קיבוץ מסמכים** – על בסיס דמיון רעיוני
- **ניתוח מגמות** – מעקב אחרי שינויי נושאים לאורך זמן
- **שיפור מנועי חיפוש** – על ידי הבנת נושא המסמך

## 🔑 שלבים עיקריים ב־Topic Modeling:

**עיבוד מקדים של הטקסט**  
כולל טוקניזציה, הסרת stop words, סטמינג / למטיזציה, וקטוריזציה (TF-IDF / Count)
- פ- טוקניזציה פיצול הטקסט למילים בודדות
- פ- הסרת stop words
- פ- למטיזציה מחזירים את המילים לשורש שלהן
- פ- וקטוריזציה: סופרים כמה פעמים כל מילה מופיעה (Count), או מחשבים משקל לכל מילה לפי הופעתה במסמך ביחס לשאר המסמכים (TF-IDF)

**בחירת מספר נושאים (k)**  
כמו ב־Clustering – אנחנו קובעים כמה נושאים המודל ינסה למצוא

**אימון המודל**  
שתמשים באלגוריתמים כמו:
- פ- LDA – Latent Dirichlet Allocation - יוסבר בהמשך
- פ- NMF – Non-Negative Matrix Factorization - יוסבר בהמשך

**פרשנות והבנה של הנושאים**  
המודל מחזיר:
- עבור כל נושא → רשימת מילים אופייניות
- עבור כל טקסט → הסתברות לכל נושא

## מודל LDA Model

### מה זה LDA – Latent Dirichlet Allocation?

 Latent = סמוי, חבוי

 Dirichlet = שם של התפלגות סטטיסטית

 Allocation = הקצאה

Latent Dirichlet Allocation = הקצאה של נושאים סמויים למסמכים, בעזרת התפלגות דירישלה

ה- LDA הוא מודל הסתברותי גנרטיבי שמשמש ל־**Topic Modeling**  
המטרה שלו היא לגלות אילו נושאים סמויים מרכיבים אוסף של מסמכים

#### 💡 מה ההנחה של המודל?

- כל **מסמך** הוא תערובת של **כמה נושאים**
- כל **נושא** הוא תערובת של **כמה מילים**

המשימה של המודל היא:
> לגלות מהי תערובת הנושאים בכל מסמך, ומהי תערובת המילים בכל נושא

#### 🧪 איך זה עובד?

1. עבור כל מסמך, המודל מחזיר התפלגות הסתברותית – כמה הוא שייך לכל נושא  
2. עבור כל נושא, המודל מחזיר את המילים שמאפיינות אותו (עם הסתברויות)

#### 🔍 לדוגמה:

- מסמך 1: 70% נושא "טכנולוגיה", 30% נושא "ספורט"  
- נושא "טכנולוגיה": ["computer", "keyboard", "internet"]  
- נושא "ספורט": ["game", "score", "team"]

#### 🧩 למה זה דומה?

אפשר לחשוב על נושאים כעל קלאסטרים (clusters)  
המסמכים לא משתייכים רק לאחד – אלא לכל הנושאים, עם הסתברויות שונות

#### 🎯 מתי זה שימושי?

- כשאין תוויות למסמכים
- כשאנחנו רוצים להבין על מה מדברים טקסטים
- כשצריך לסכם או לקבץ טקסטים לפי רעיונות סמויים

#### 📊 ההבדל בין Topic Modeling לבין Clustering רגיל

| תכונה                      | Topic Modeling (LDA)           | Clustering רגיל (KMeans למשל) |
|----------------------------|--------------------------------|-------------------------------|
| מספר תוויות לכל מסמך       | כמה נושאים עם הסתברויות       | רק קלאסטר אחד                |
| למידה מונחית או לא?         | לא מונחית (Unsupervised)        | לרוב לא מונחית               |
| משקל לכל תכונה              | לפי הסתברות מילה לנושא         | לפי מיקום במרחב              |

#### 🧪 בפועל:

> נריץ LDA על טקסטים ונקבל:
> - עבור כל מסמך: לאיזה נושאים הוא שייך (ואחוז שייכות)
> - עבור כל נושא: אילו מילים מייצגות אותו הכי חזק

### 🧩 ההבדל בין LDA לבין Clustering רגיל

ב־**Clustering רגיל** (כמו K-Means):
- כל מסמך משויך **לאשכול אחד בלבד**

לעומת זאת, ב־**LDA**:
- כל מסמך משויך **לכמה נושאים שונים**, לפי **הסתברות**  
- כלומר, כל מסמך מורכב מתערובת של נושאים

#### דוגמה:

נניח שיש לנו 2 מסמכים והרצנו עליהם מודל LDA עם 5 נושאים (`Topics 1–5`)

#### עבור כל מסמך, נקבל:
- **DOC 1**: Mostly belongs to topic 2, and partially to topics 3 and 4  
- **DOC 2**: Mostly belongs to topic 4, and partially to topics 1, 2, and 5

🟩 כלומר:
> כל מסמך מיוצג כווקטור הסתברויות – כמה הוא שייך לכל Topic

<img src="lda1.jpg" style="width: 100%" />

### 🎯 התפלגות מילים לפי נושא (Topic)

בנוסף לשיוך נושאים למסמכים, המודל של LDA גם מספק **התפלגות הסתברותית של מילים בכל נושא**  
כלומר: עבור כל Topic, המודל מראה אילו מילים הן הנפוצות ביותר בו

#### 🐾 דוגמה:

ב־**Topic 1** המילים עם ההסתברות הכי גבוהה הן:

- **cat**
- **dog**

מכאן ניתן לשער שהנושא הזה מדבר על **חיות**

<img src="lda2.jpg" style="width: 80%" />

### ⚙️ איך המודל של LDA עובד?

כדי ליצור את הנושאים ולחשב הסתברויות, המודל משתמש בטכניקה שנקראת:

#### **Gibbs Sampling** (חלק ממשפחת **MCMC** – Markov Chain Monte Carlo)

מדובר באלגוריתם הסתברותי שמעריך:
- עד כמה מילה מסוימת שייכת לנושא מסוים
- עד כמה נושא מסוים מופיע במסמך

#### תהליך העבודה:

1. עובר מילה־מילה בכל מסמך  
2. מחשב:  
   - מה הסיכוי שהמילה שייכת לכל נושא  
   - מה הסיכוי שהנושא שייך למסמך  
3. מחשב הסתברות משותפת (joint probability)  

$$
P(\text{Topic X and word w}) = P(\text{w belongs to Topic X}) × P(\text{Topic X belongs to document})
$$

4. חוזר על התהליך שוב ושוב (איטרציות) כדי לדייק

💡 בסוף התהליך נקבל:
- שיוך מילים לנושאים
- התפלגות נושאים לכל מסמך

### 🔁 איך עובד מודל LDA שלב אחר שלב

מודל LDA הוא מודל איטרטיבי (חוזר על עצמו), שמשתמש באיטרציות כדי לחשב את ההתפלגות הסופית של:
- ההסתברות של כל מילה לשייך לנושא מסוים
- ההסתברות של כל נושא להופיע במסמך מסוים

#### שלבי העבודה של המודל:

- **Parameters selection**  
  בחירה של מספר הנושאים (K) 
  
  וכמה איטרציות לבצע (פרמטר אופציונלי)

- **Random Initialization**  
  בתחילת התהליך, כל מילה במסמכים משויכת **באופן רנדומלי** לאחד מהנושאים

- **Iterative Refinement**  
  המודל מעדכן בכל איטרציה את שיוך המילים לנושאים:
  - מחשב את ההסתברות המשותפת של מילה לנושא ושל נושא למסמך
  - אם מוצא נושא עם הסתברות גבוהה יותר, המודל מעדכן את שיוך המילה

- **Model prediction**  
  בסיום האימון, כל מסמך יקבל רשימה של נושאים עם הסתברויות מתאימות  
  בנוסף, לכל נושא תהיה רשימת מילים עם הסתברויות שמייצגות את הנושא

### דוגמא

#### נניח שיש לנו את המסמכים הבאים:

- Doc 1: `"cats eat fish"`
- Doc 2: `"dogs eat bones"`
- Doc 3: `"fish swim in water"`

#### מטרת המודל:
למצוא **נושאים (topics)** במסמכים האלו, מבלי לדעת מראש מה הם  
למשל, אולי אחד הנושאים הוא "חיות בית" והשני "מים/טבע"

### שלב 1: Parameters selection

- בוחרים `K = 2` → כלומר נרצה לגלות 2 נושאים
- נגדיר גם מספר איטרציות (נניח 10)

בהתחלה, כן – כל מילה בקורפוס מקבלת Topic אקראי  
לדוגמה: המילה "fish" תקבל למשל Topic 2, בלי קשר למהות שלה  
המילה "bones" אולי Topic 4, גם אם זה לא הגיוני  
אבל... אל תדאג – זה רק צעד ראשוני. כאן מתחיל הקסם של למידה איטרטיבית

### שלב 2: Random Initialization

- כל מילה בכל מסמך משויכת באופן **רנדומלי** לאחד מהנושאים  
  לדוגמה:
  - "cats" → Topic 1  
  - "eat" → Topic 2  
  - "fish" → Topic 2  
  (ללא משמעות, רק התחלה רנדומלית)

### שלב 3: Iterative Refinement

- עבור כל מילה, המודל מחשב:
  - עד כמה היא נפוצה בתוך כל Topic
  - עד כמה Topic מסוים נפוץ במסמך הנוכחי
- לפי זה המודל **מעדכן את שיוך המילה**
- חוזר על התהליך שוב ושוב

בשלב הזה, המודל עובר מילה־מילה בכל מסמך, ומנסה להבין:

כמה נפוצה המילה בתוך כל Topic
לדוגמה: המילה "cat" מופיעה הרבה ב־Topic 1 (חיות), אבל לא ב־Topic 2 (מים)

כמה Topic מסוים נפוץ במסמך הנוכחי
נניח שהמסמך מכיל כבר הרבה מילים שמצביעות על Topic 1 → סביר שגם המילה הנוכחית שייכת ל־Topic 1

דוגמה:

יש לנו את המסמך הבא:  
Doc = "cat eats fish"  
והמצב הנוכחי של המודל:  
המילה "cat" שייכת ל־Topic 1 (חיות)  
המילה "fish" שייכת ל־Topic 2 (טבע/מים)  
המילה "eats" עדיין לא שויכה  
המודל ישאל: לאן הכי הגיוני לשייך את המילה "eats"?  
אם רוב המילים במסמך שייכות ל־Topic 1 → סביר שגם "eats" תשתייך לשם  
אם המילה עצמה מופיעה הרבה ב־Topic 2 במסמכים אחרים → ייתכן שתשויך לשם  

🔁 המודל בודק את כל זה מחדש בכל איטרציה, ועד שהוא מתייצב על חלוקה סופית של מילים לטופיקים

### שלב 4: Model Prediction

ניתוח לפי נושאים:

Topic 1: חיות → ["cats", "dogs", "eat", "bones"]

Topic 2: טבע/מים → ["fish", "swim", "water"]

Doc 1: contains both "cats" and "fish" → it may belong to both topics, but likely more to Topic 1 (Animals)

Doc 2: contains "dogs", "eat", "bones" → all words match Topic 1 → will mostly belong to Topic 1

Doc 3: contains "fish", "swim", "water" → all words match Topic 2 → will mostly belong to Topic 2

- אחרי מספר איטרציות:
  - **Doc 1** ישויך בעיקר נושא 1 (חיות)
  - **Doc 3** ישויך נושא 2 (טבע/מים)
- המודל גם יודע אילו מילים מייצגות כל Topic:
  - Topic 1: cats, dogs, eat
  - Topic 2: fish, water, swim

### ⚡ התוצאה:

- לכל מסמך → הסתברות לכל נושא  
- לכל נושא → הסתברות לכל מילה

### 🟢 Final Summary:

| Document | Topic 1 (Animals) | Topic 2 (Nature/Water) | Main Assignment |
|----------|-------------------|------------------------|-----------------|
| Doc 1    | Medium–High       | Medium                 | Likely Topic 1  |
| Doc 2    | Very High         | Very Low               | Topic 1         |
| Doc 3    | Low               | Very High              | Topic 2         |


וזה עוזר לנו להבין על מה מדברים המסמכים גם בלי תיוגים מראש 🧩

  
  שקף הבא דוגמא בפייתון ...







