# NLP FETAURES

## ××”×• ×§×•×¨×¤×•×¡ (Corpus)

×§×•×¨×¤×•×¡ ×”×•× ××•×¡×£ ×’×“×•×œ ×©×œ ×˜×§×¡×˜×™× ×”××©××© ×œ×œ×™××•×“ ×•× ×™×ª×•×— ×©×¤×” ×˜×‘×¢×™×ª. ×–×” ×™×›×•×œ ×œ×”×™×•×ª ×˜×§×¡×˜×™× ×¡×¤×¨×•×ª×™×™×, ×—×“×©×•×ª, ×ª××œ×•×œ×™× ×©×œ ×“×™×‘×•×¨, ××ª×¨×™ ××™× ×˜×¨× ×˜ ×•×¢×•×“

×‘×§×•× ×˜×§×¡×˜ ×©×œ NLP, ×§×•×¨×¤×•×¡ ××©××© ×œ××™××•×Ÿ, ×‘×“×™×§×” ×•×”×¢×¨×›×” ×©×œ ××•×“×œ×™× â€“ ×œ×“×•×’××”, ××•×“×œ×™× ×©×œ ×—×œ×§×™ ×“×™×‘×¨ (POS) ××• ×–×™×”×•×™ ×™×©×•×™×•×ª (NER)

#### ×¡×•×’×™ ×§×•×¨×¤×•×¡×™×:

* **×§×•×¨×¤×•×¡ ×œ× ××¡×•××Ÿ** (raw corpus): ×˜×§×¡×˜ ×¨×’×™×œ ×œ×œ× ×ª×™×•×’
* **×§×•×¨×¤×•×¡ ××¡×•××Ÿ** (annotated corpus): ×›×•×œ×œ ××™×“×¢ × ×•×¡×£ ×›××• ×—×œ×§×™ ×“×™×‘×¨, ×ª×—×‘×™×¨, ×™×©×•×™×•×ª ×•×›×•'

ğŸ“Œ ×”- SpaCy ××•×× ×” ×¢×œ ×§×•×¨×¤×•×¡×™× ××¡×•×× ×™×, ××” ×©×××¤×©×¨ ×œ×” ×œ×”×‘×™×Ÿ ×”×§×©×¨×™× ×•×œ×–×”×•×ª ××‘× ×™× ×œ×©×•× ×™×™× ××•×¨×›×‘×™×

**×“×•×’×× 1**

```python
import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp("The quick brown fox jumped over the lazy dog's back.")

print(doc[4].text, doc[4].pos_, doc[4].tag_, spacy.explain(doc[4].tag_))
```

Output:
```
jumped VERB VBD verb, past tense
```

SpaCy decided that the token jumped is a verb with the VBD tag, meaning it's a verb in past tense

**×“×•×’×× 2**

```python
doc1 = nlp(u'I read books on NLP.')
doc2 = nlp(u'I read a book on NLP.')

print(f'{doc1[1].text:10} {doc1[1].pos_:8} {doc1[1].tag_:6} {spacy.explain(doc1[1].tag_)}')
print()
print(f'{doc2[1].text:10} {doc2[1].pos_:8} {doc2[1].tag_:6} {spacy.explain(doc2[1].tag_)}')
```

Output:
```
read       VERB     VBP    verb, non-3rd person singular present

read       VERB     VBD    verb, past tense
```

ğŸ“Œ SpaCy is smart enough to understand from the textual context that the first read token is in present tense, while the second read is in past tense

## Named Entity Recognition (NER)

×”- NER ×”×•× ×ª×”×œ×™×š ×©×‘×• ××–×”×™× ×™×©×•×™×•×ª ×‘×©× ×‘×˜×§×¡×˜ (×›××• ×©××•×ª ×©×œ ×× ×©×™×, ××§×•××•×ª, ×—×‘×¨×•×ª, ×¡×›×•××™×, ×ª××¨×™×›×™× ×•×›×•')

#### â€¢ **Named Entities**  
×‘× ×™×™×ª ×¨×©×™××” ×©×œ ×™×©×•×™×•×ª ×‘×©× ××ª×•×š ×”×˜×§×¡×˜, ×›×©×”××¤×ª×— ×”×•× ×¡×•×’ ×”×™×©×•×ª ×•×”×¢×¨×›×™× ×”× ×”××•×¤×¢×™× ×©×–×•×”×•  
×œ×“×•×’××”:
- **Persons (PER)** â€“ ×©××•×ª ×©×œ ×× ×©×™× ×›××•: `"Albert Einstein"`, `"Marie Curie"`
- **Locations (LOC)** â€“ ××§×•××•×ª ×’×™××•×’×¨×¤×™×™× ×›××•: `"Paris"`, `"Mount Everest"`

#### â€¢ **Recognition**  
×–×™×”×•×™ ×‘×¤×•×¢×œ ×©×œ ×”××™×œ×™× ××• ×”×‘×™×˜×•×™×™× ×‘×˜×§×¡×˜ ×©××™×™×¦×’×™× ×™×©×•×™×•×ª ×‘×©×  
×œ×“×•×’××”: ×‘××©×¤×˜ `"Barack Obama visited Paris"` â€“ × ×–×”×” ××ª `"Barack Obama"` ×•××ª `"Paris"` ×›×™×©×•×™×•×ª

#### â€¢ **Classification**  
×©×™×•×š ×›×œ ×™×©×•×ª ×©× ××¦××” ×œ×§×˜×’×•×¨×™×” ××ª××™××” (×›××• `PERSON`, `LOCATION`, `ORG`, `DATE` ×•×›×•')


### ×©×œ×‘×™×:

1. **Tokenization** â€“ ×¤×™×¦×•×œ ×”×˜×§×¡×˜ ×œ××™×œ×™×
2. **Feature Extraction** â€“ ×”×¤×§×ª ×××¤×™×™× ×™× ×›××• ×¦×•×¨×ª ××™×œ×”, ××•×ª×™×•×ª ×’×“×•×œ×•×ª, ×¡×™×•××•×ª
3. **Model Training** â€“ ××™××•×Ÿ ××•×“×œ ×¢×œ ×˜×§×¡×˜×™× ××¡×•×× ×™× ××¨××©
4. **Prediction** â€“ ×–×™×”×•×™ ×™×©×•×™×•×ª ×‘×˜×§×¡×˜ ×—×“×©

`Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.`

#### 1. **Tokenization** â€“ ×¤×™×¦×•×œ ×”×˜×§×¡×˜ ×œ××™×œ×™× (×˜×•×§× ×™×)  
×”- SpaCy ××¤×¨×§×ª ××ª ×”××©×¤×˜ ×œ××™×œ×™× ×•×ª×•×•×™ ×¤×™×¡×•×§ â€“ ×›××• `"Barack"`, `"Obama"`, `"August"`, `"4"`, `"1961"`, `","`, `"Hawaii"` ×•×›×•'

#### 2. **Feature Extraction** â€“ ×”×¤×§×ª ×××¤×™×™× ×™× ××”×˜×•×§× ×™×  
×œ×›×œ ××™×œ×” ××—×•×œ×¦×™× ×××¤×™×™× ×™× ×›××•:  
- ×”×× ×”××•×ª ×”×¨××©×•× ×” ×’×“×•×œ×”? (Barack)  
- ×”×× ×–×• ××™×œ×” ××¡×¤×¨×™×ª ××• ×ª××¨×™×š? (4, 1961)  
- ×”×× ×”×™× ×××•×§××ª ××—×¨×™ ××™×œ×ª ××¤×ª×— ×›××• "born on"?  

×××¤×™×™× ×™× ××œ×• ×¢×•×–×¨×™× ×œ××•×“×œ ×œ×”×‘×™×Ÿ ××™×œ×• ××™×œ×™× ×¢×©×•×™×•×ª ×œ×”×™×•×ª ×™×©×•×™×•×ª

#### 3. **Model Training** â€“ (×›×‘×¨ ×‘×•×¦×¢ ××¨××© ×¢×œ ×™×“×™ spaCy)  
×”××•×“×œ ×©××•××Ÿ ×¢×œ ×§×•×¨×¤×•×¡ ×’×“×•×œ ×™×•×“×¢ ×œ×–×”×•×ª ××‘× ×™× ×©×œ ×©××•×ª ×¤×¨×˜×™×™×, ×ª××¨×™×›×™×, ×¢×¨×™× ×•×¢×•×“

#### 4. **Prediction** â€“ ×—×™×–×•×™ ×”×™×©×•×™×•×ª ×‘×˜×§×¡×˜  
×”××•×“×œ ××–×”×” ××ª ×”×™×©×•×™×•×ª ×”×‘××•×ª:

```
Barack Obama â†’ PERSON â†’ ××“× (×××™×ª×™ ××• ×‘×“×™×•× ×™)
August 4, 1961 â†’ DATE â†’ ×ª××¨×™×š
Honolulu â†’ GPE â†’ ××“×™× ×” / ×¢×™×¨ / ××–×•×¨ ×’×™××•×’×¨×¤×™
Hawaii â†’ GPE â†’ ××“×™× ×” / ×¢×™×¨ / ××–×•×¨ ×’×™××•×’×¨×¤×™
```

ğŸ“Œ ×›×š spaCy ××¦×œ×™×—×” ×œ×”×‘×™×Ÿ ××ª×•×š ×”×”×§×©×¨ ×•×œ×¡×•×•×’ × ×›×•×Ÿ ×›×œ ×™×©×•×ª ×‘×˜×§×¡×˜

**×¤×™×™×ª×•×Ÿ**

```python
doc = nlp("Barack Obama was born on August 4, 1961, in Honolulu, Hawaii.")
for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))
```

Output:
```
Barack Obama PERSON People, including fictional
August 4, 1961 DATE Absolute or relative dates or periods
Honolulu GPE Countries, cities, states
Hawaii GPE Countries, cities, states
```

ğŸ“Œ ×”- SpaCy ××—×–×™×¨ ×™×©×•×™×•×ª ×¢× label ×›××• `PERSON`, `DATE`, `GPE` ×•×××¤×©×¨ ×œ×”×¡×‘×™×¨ ×›×œ label ×‘×××¦×¢×•×ª `spacy.explain`

---

### ğŸ§  ×”×ª×××” ××™×©×™×ª ×©×œ NER ×‘Ö¾spaCy

#### ×œ××” × ×¨×¦×” ×œ×”×•×¡×™×£ ×™×©×•×™×•×ª ×™×“× ×™×ª?
×œ×¤×¢××™× spaCy ×œ× ××–×”×” ×™×©×•×™×•×ª ×©×× ×—× ×• ×›×Ÿ ×¨×•×¦×™× ×œ×–×”×•×ª â€“ ×œ×“×•×’××”, "Tesla" ×œ× ××–×•×”×” ×›×‘×¨×™×¨×ª ××—×“×œ ×›×™×©×•×ª ××¡×•×’ ORG  
×‘××§×¨×™× ×›××œ×” × ×•×›×œ ×œ×”×•×¡×™×£ ××ª ×”×™×©×•×ª ×™×“× ×™×ª ×œ××¡××š

#### ×”×©×œ×‘×™× ×œ×”×•×¡×¤×ª ×™×©×•×ª ×™×“× ×™×ª:
1. ×©×œ×™×¤×ª ×”×¢×¨×š ×”××¡×¤×¨×™ (hash) ×©×œ ×”×ª×•×•×™×ª ×”×¨×¦×•×™×” (×œ××©×œ "ORG")
2. ×™×¦×™×¨×ª Span ××”××™×œ×” ××• ×”×‘×™×˜×•×™ ×”×¨×¦×•×™
3. ×”×•×¡×¤×ª ×”Ö¾Span ×œ×¨×©×™××ª ×”×™×©×•×™×•×ª ×‘××¡××š `doc.ents`

```python
import spacy
nlp = spacy.load('en_core_web_sm')

doc = nlp(u'Tesla to build a U.K. factory for $6 million')
for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  

# adding Tesla entity
from spacy.tokens import Span

ORG = doc.vocab.strings["ORG"]
new_ent = Span(doc, start=0, end=1, label=ORG)
doc.ents = list(doc.ents) + [new_ent]

for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  
```

Output:
```
-Before
U.K. GPE Countries, cities, states
$6 million MONEY Monetary values, including unit

-After
Tesla ORG Companies, agencies, institutions, etc.
U.K. GPE Countries, cities, states
$6 million MONEY Monetary values, including unit
```

### ×“×•×’××” ××ª×§×“××ª â€“ Phrase Matcher
×× × ×¨×¦×” ×œ×–×”×•×ª ×‘×™×˜×•×™×™× ×©×œ××™× ×›××•:
- `dashboard website`
- `dashboard-website`

In this example, we demonstrate how to use SpaCyâ€™s `PhraseMatcher` to detect custom named entities that are not recognized by default

#### Step 1: Default Behavior â€“ No Entities Detected

```python
doc = nlp(u"Our company plans to introduce a new dashboard-website. If successful, the dashboard website\
 will be our main customer payed product")

for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  
```

Output:
```
No named entities found.
```

#### Step 2: Define Phrase Matcher Patterns

We want to recognize ```"dashboard website"``` and ```"dashboard-website"``` as **PRODUCT** entities. First, we use PhraseMatcher to define patterns for them

```python
from spacy.matcher import PhraseMatcher

matcher = PhraseMatcher(nlp.vocab)

phrase_list = ['dashboard website', 'dashboard-website']
phrase_patterns = [nlp(text) for text in phrase_list]

matcher.add('newproduct', phrase_patterns)

matches = matcher(doc)
matches
```

Output:
```
[(2689272359382549672, 7, 10), (2689272359382549672, 15, 17)]
```

âœ… We created two phrase patterns: one for "dashboard website" and one for "dashboard-website"

âœ… Added both patterns under the label 'newproduct'

âœ… SpaCy successfully matched both phrases within the doc and returned their span positions

#### Step 3: Add Matched Spans to `doc.ents`

After using the `PhraseMatcher` to find matching phrases in the text, we can now manually add them to SpaCyâ€™s `doc.ents` list so that they are treated as proper named entities.

```python
from spacy.tokens import Span

PROD = doc.vocab.strings['PRODUCT']

new_ents = [Span(doc, match[1], match[2], label=PROD) for match in matches]

doc.ents = list(doc.ents) + new_ents

for ent in doc.ents:
    print(ent.text, ent.label_, spacy.explain(ent.label_))  
```    

Output:
```
dashboard-website PRODUCT Objects, vehicles, foods, etc. (not services)
dashboard website PRODUCT Objects, vehicles, foods, etc. (not services)
```

---

### Sentence Segmentation

### ×¤×™×¦×•×œ ×œ××©×¤×˜×™× (Sentence Segmentation) ğŸ§©

**×¤×™×¦×•×œ ××©×¤×˜×™×** â€“ ×ª×”×œ×™×š ×‘Ö¾NLP ×©××˜×¨×ª×• ×œ×—×œ×§ ×˜×§×¡×˜ ×©×œ× ×œ××©×¤×˜×™× × ×¤×¨×“×™×. ×”××˜×¨×” ×”×™× ×œ×–×”×•×ª ××ª×™ ××¡×ª×™×™× ××©×¤×˜ ××—×“ ×•××ª×—×™×œ ××©×¤×˜ ×—×“×©

#### ğŸŸ£ ×”×’×™×©×” ×”× ×¤×•×¦×”

×‘×“×¨×š ×›×œ×œ, ×¤×™×¦×•×œ ××©×¤×˜×™× ××ª×‘×¦×¢ ×œ×¤×™ ×¡×™×× ×™ ×¤×™×¡×•×§ ×›××•:
- × ×§×•×“×” `.`
- ×¡×™××Ÿ ×©××œ×” `?`
- ×¡×™××Ÿ ×§×¨×™××” `!`

#### ğŸŸ  ××’×‘×œ×•×ª ×”×’×™×©×” ×”×¤×©×•×˜×”

×œ××¨×•×ª ×©×”×©×™×˜×” ×”×–×• ×¤×©×•×˜×”, ×”×™× ×¢×œ×•×œ×” ×œ×”×™×•×ª **×œ× ××“×•×™×§×ª** ×‘×’×œ×œ ×”××•×¨×›×‘×•×ª ×©×œ ×”×©×¤×”:

- × ×§×•×“×•×ª ××•×¤×™×¢×•×ª ×’× ×‘**×§×™×¦×•×¨×™×** (×œ××©×œ: `×“"×¨`, `××¨×”"×‘`)
- **××¡×¤×¨×™× ×¢×©×¨×•× ×™×™×** (×›××• `3.14`)
- **×©×œ×•×© × ×§×•×“×•×ª** (`...`)

×¡×™×× ×™× ××œ×” ×œ× ×‘×”×›×¨×— ××¡×× ×™× ×¡×•×£ ××©×¤×˜.

#### ğŸŸ¢ ×¤×ª×¨×•× ×•×ª ××ª×§×“××™×

××•×“×œ×™× ××ª×§×“××™× ×‘×•×—× ×™× ×’× ×’×•×¨××™× × ×•×¡×¤×™×:

- **××•×ª×™×•×ª ×¨×™×©×™×•×ª** (×”×× ×”××™×œ×” ×©××—×¨×™×” ××ª×—×™×œ×” ×‘××•×ª ×’×“×•×œ×”)
- **×”×§×©×¨ ×ª×—×‘×™×¨×™**
- **××™×œ×™× ×©×¤×•×ª×—×•×ª ××©×¤×˜×™× ×‘×“×¨×š ×›×œ×œ**

#### ğŸ§  ×œ××” ×–×” ×—×©×•×‘?

×¤×™×¦×•×œ × ×›×•×Ÿ ×œ××©×¤×˜×™× ×”×•× ×‘×¡×™×¡ ×—×©×•×‘ ×œ××©×™××•×ª NLP ××ª×§×“××•×ª ×›××•:
- ×ª×¨×’×•× ××›×•× ×”
- ×¡×™×›×•× ×˜×§×¡×˜×™×
- × ×™×ª×•×— ×¡× ×˜×™×× ×˜

×‘×’×œ×œ ×©×¤×¢×•×œ×•×ª ××œ×• ××‘×•×¦×¢×•×ª ×œ×¢×™×ª×™× ×§×¨×•×‘×•×ª **×‘×¨××ª ××©×¤×˜**, ×¤×™×¦×•×œ ×œ× ××“×•×™×§ ×™×¤×’×¢ ×‘×“×™×•×§ ×”×ª×•×¦××•×ª

×›××©×¨ ×× ×• ××¤×¦×œ×™× ×˜×§×¡×˜ ×œ××©×¤×˜×™× ×‘×¢×–×¨×ª SpaCy, ×”×¤×œ×˜ × ×©××¨ ×‘××•×‘×™×™×§×˜ ×‘×©× `doc.sents`  
×–×”×• ×œ× ×××© ×¨×©×™××”, ××œ× **generator** â€“ ××•×‘×™×™×§×˜ ×©××—×–×™×¨ ×›×œ ××©×¤×˜ ×‘× ×¤×¨×“ ×‘×œ×•×œ××”

```python
import spacy
nlp = spacy.load('en_core_web_sm')

doc = nlp('This is the first sentence. This is another sentence. This is the last sentence.')

for sent in doc.sents:
    print(sent)
```

Output:
```
This is the first sentence.
This is another sentence.
This is the last sentence.
```

---

### SpaCy Pipeline

Pipeline ×”×•× ×¨×¦×£ ×”×¤×¢×•×œ×•×ª ×©Ö¾SpaCy ××‘×¦×¢×ª ×¢×œ ×”×˜×§×¡×˜:

1. Tokenization
2. POS Tagging
3. Dependency Parsing
4. Lemmatization
5. Named Entity Recognition
6. Sentence Segmentation

×›×œ ×©×œ×‘ ××©×¤×™×¢ ×¢×œ ×”Ö¾`Doc` ×•××•×¡×™×£ ×œ×• ××™×“×¢. ××¤×©×¨ ×’× ×œ×”×•×¡×™×£ ×©×œ×‘×™× ××•×ª×××™× ×›××• segmentation ××• ×–×™×”×•×™ ×™×©×•×™×•×ª ××•×ª×××•×ª

ğŸ“Œ × ×™×ª×Ÿ ×œ×”×•×¡×™×£ ×©×œ×‘ ×¢× `add_pipe()` ××• ×œ×”×¡×™×¨ ×¢× `disable()`
