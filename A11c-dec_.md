# השוואה בין Gini Impurity ל-Entropy
  
## מטרת המדדים:
- **Gini Impurity** ו-**Entropy** הם מדדים שמודדים **"חוסר טוהר"** או **מידת ערבוב** בקבוצה.
- משתמשים בהם בעצי החלטה (**Classification Trees**) כדי לבחור את הפיצול הכי טוב.
  
## נוסחאות:
  
### 1. Gini Impurity:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Gini%20=%201%20-%20\sum_{i=1}^{k}%20p_i^2"/></p>  
  
  
- <img src="https://latex.codecogs.com/gif.latex?p_i"/> זה ההסתברות של כל מחלקה (class).
- הערך המקסימלי מתקבל כשהמחלקות מעורבבות שווה בשווה.
  
### 2. Entropy:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy%20=%20-%20\sum_{i=1}^{k}%20p_i%20\cdot%20\log_2(p_i)"/></p>  
  
  
- גם פה <img src="https://latex.codecogs.com/gif.latex?p_i"/> זה ההסתברות של כל מחלקה.
- ערך גבוה יותר אומר יותר ערבוב (חוסר טוהר).
  
## דוגמה:
  
נניח שיש לך קבוצה עם 4 דוגמאות:
  
- 2 דוגמאות בקטגוריה A.
- 2 דוגמאות בקטגוריה B.
  
### חישוב Gini Impurity:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Gini%20=%201%20-%20(p_A^2%20+%20p_B^2)%20=%201%20-%20(0.5^2%20+%200.5^2)%20=%201%20-%20(0.25%20+%200.25)%20=%200.5"/></p>  
  
  
### חישוב Entropy:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy%20=%20-%20(p_A%20\cdot%20\log_2(p_A)%20+%20p_B%20\cdot%20\log_2(p_B))%20=%20-%20(0.5%20\cdot%20\log_2(0.5)%20+%200.5%20\cdot%20\log_2(0.5))%20=%201"/></p>  
  
  
## השוואה כללית:
  
| מאפיין              | Gini Impurity            | Entropy                |
|----------------------|---------------------------|------------------------|
| נוסחה               | <img src="https://latex.codecogs.com/gif.latex?1%20-%20\sum%20p_i^2"/>       | <img src="https://latex.codecogs.com/gif.latex?-%20\sum%20p_i%20\log_2%20p_i"/> |
| ערך מקסימלי (2 מחלקות) | 0.5                      | 1                      |
| חישוב               | פשוט ומהיר יותר          | איטי יותר (יש לוגים)   |
| פרשנות              | מדד חוסר טוהר             | מדד חוסר טוהר (מידע)  |
  
  
## מתי להשתמש?
  
| מאפיין            | Gini Impurity           | Entropy                  |
|-------------------|-------------------------|--------------------------|
| חישוביות          | מהיר יותר (ללא לוגריתם) | איטי יותר (יש לוגריתם)   |
| רגישות            | פחות רגיש להבדלים קטנים | יותר רגיש להבדלים קטנים  |
| מתי לבחור         | כשחשובה מהירות          | כשחשובה דיוק/מידע         |
| שימוש נפוץ        | Decision Tree (CART)    | Decision Tree (ID3, C4.5)|
  
**המלצה כללית:**
- אם יש לך הרבה נתונים או אתה צריך חישוב מהיר → **Gini**  
- אם אתה רוצה להיות רגיש יותר לאי-ודאות → **Entropy**
  
- **Gini** is more commonly used in decision trees like **CART** (Classification And Regression Tree) because of its simpler computation
- **Entropy** is often used when information gain is meaningful, such as in the ID3 tree
  
---  
  
## סוגי עצי החלטה
  
## 1. CART (Classification and Regression Trees)
  
**דוגמה**  
- סיווג: חיזוי אם אדם יאושר להלוואה לפי גיל והכנסה  
- רגרסיה: חיזוי מחיר דירה לפי שטח ומספר חדרים
  
**מאפיין עיקרי**  
- יוצר תמיד **עץ בינארי** (כל צומת מתפצל לשני ענפים בלבד)  
- מתאים גם לבעיות **סיווג** וגם לבעיות **רגרסיה**
  
**מדד חלוקה**  
- **Gini impurity** (לבעיות סיווג)  
- **Mean Squared Error (MSE)** (לבעיות רגרסיה)
  
  
### מה זה Mean Squared Error (MSE) בעצי החלטה?
  
**מדד חלוקה** שנמצא בשימוש בעצי החלטה מסוג **רגרסיה**  
המטרה של המדד היא להעריך **כמה טוב הפיצול מנבא את הערכים הרציפים** (כמו מחיר, גיל, משקל)  
ככל שה-MSE קטן יותר, כך הפיצול נחשב **טוב יותר**  
  
##### הנוסחה של MSE
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?MSE%20=%20\frac{1}{n}%20\sum_{i=1}^{n}%20(y_i%20-%20\hat{y})^2"/></p>  
  
  
- <img src="https://latex.codecogs.com/gif.latex?n"/> – מספר הדגימות בצומת  
- <img src="https://latex.codecogs.com/gif.latex?y_i"/> – הערך האמיתי של דגימה <img src="https://latex.codecogs.com/gif.latex?i"/>  
- <img src="https://latex.codecogs.com/gif.latex?\hat{y}"/> – ממוצע הערכים בצומת (תחזית העץ)
  
##### איך זה עובד?
  
1. בכל צומת בעץ, העץ שוקל **פיצולים אפשריים** (למשל: האם לפצל לפי גיל גדול מ-30 או לא)  
2. עבור כל פיצול אפשרי, העץ מחשב את ה-MSE בכל תת-קבוצה שנוצרת  
3. העץ בוחר את הפיצול שמביא ל**ירידה הכי גדולה ב-MSE הכולל** (כלומר, שמקטין את השגיאה הכי הרבה)
  
##### דוגמה פשוטה
  
נניח שיש לנו את הנתונים הבאים (ננסה לנבא **מחיר דירה** לפי **שטח**):
  
| שטח (מ"ר) | מחיר (אלפי ₪) |
|------------|---------------|
| 50         | 1000          |
| 60         | 1200          |
| 70         | 1300          |
| 80         | 1500          |
  
##### צומת ראשי (לפני פיצול):
  
- ממוצע המחירים:  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?\hat{y}%20=%20\frac{1000%20+%201200%20+%201300%20+%201500}{4}%20=%201250"/></p>  
  
  
- חישוב ה-MSE:  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?MSE%20=%20\frac{1}{4}%20\left(%20(1000%20-%201250)^2%20+%20(1200%20-%201250)^2%20+%20(1300%20-%201250)^2%20+%20(1500%20-%201250)^2%20\right)"/></p>  
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?MSE%20=%20\frac{1}{4}%20(62500%20+%202500%20+%202500%20+%2062500)%20=%20\frac{130000}{4}%20=%2032500"/></p>  
  
  
##### אחרי פיצול (למשל לפי שטח גדול מ-65):
  
- קבוצה 1 (שטח ≤ 65):  
  - דירות: 50 מ"ר (1000), 60 מ"ר (1200)  
  - ממוצע: 1100  
  - MSE:  
  <p align="center"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{2}%20\left(%20(1000%20-%201100)^2%20+%20(1200%20-%201100)^2%20\right)%20=%20\frac{1}{2}%20(10000%20+%2010000)%20=%2010000"/></p>  
  
  
- קבוצה 2 (שטח > 65):  
  - דירות: 70 מ"ר (1300), 80 מ"ר (1500)  
  - ממוצע: 1400  
  - MSE:  
  <p align="center"><img src="https://latex.codecogs.com/gif.latex?\frac{1}{2}%20\left(%20(1300%20-%201400)^2%20+%20(1500%20-%201400)^2%20\right)%20=%20\frac{1}{2}%20(10000%20+%2010000)%20=%2010000"/></p>  
  
  
- MSE כולל אחרי פיצול:  
  <p align="center"><img src="https://latex.codecogs.com/gif.latex?\frac{2}{4}%20\cdot%2010000%20+%20\frac{2}{4}%20\cdot%2010000%20=%2010000"/></p>  
  
  
##### סיכום:
  
- לפני פיצול: **MSE = 32500**  
- אחרי פיצול: **MSE = 10000**
  
מכאן שהפיצול **שיפר את הדיוק** (כי ה-MSE ירד)
  
## 2. ID3 (Iterative Dichotomiser 3)
  
**דוגמה**  
- סיווג אם לשחק טניס לפי תנאי מזג האוויר (תכונות כמו "שמשי", "גשום", "לח")
  
**מאפיין עיקרי**  
- עובד טוב עם **נתונים קטגוריים בלבד** – תכונות שמחולקות לקבוצות ברורות כמו צבע או מזג אוויר  
- **נתונים רציפים** (כמו גיל או הכנסה) דורשים **חלוקה לטווחים** לפני השימוש
  
**מדד חלוקה**  
- **Information Gain** (מבוסס על **Entropy**)
  
### מה זה Information Gain?
  
**מדד חלוקה** שמשמש בעצי החלטה (בעיקר **ID3** ו-**C4.5**)  
המטרה של IG היא למדוד **כמה אי-סדר (אנטרופי)** הצלחנו **להפחית** בעזרת פיצול נתונים לפי תכונה מסוימת  
ככל שה-IG **גבוה יותר**, הפיצול נחשב **טוב יותר** כי הוא עושה את הקבוצות **טהורות** יותר
  
##### למה צריך את זה?
  
בכל שלב בבניית עץ החלטה, צריך לבחור **איזו תכונה הכי משתלם לפצל לפיה**  
- תכונה עם **IG גבוה** תביא לחלוקה שבה הקבוצות הרבה יותר **אחידות**  
- תכונה עם **IG נמוך** לא תעזור לנו לסדר את הדאטה, והקבוצות ישארו **מעורבבות**
  
איי גיי עוזר לנו לבחור את **התכונה הכי טובה** לפיצול בכל צומת
  
##### איך מחשבים את זה?
  
1. מחשבים את ה-**Entropy** של הקבוצה לפני הפיצול:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy(S)%20=%20-\sum_{i=1}^{c}%20p_i%20\log_2(p_i)"/></p>  
  
  
- <img src="https://latex.codecogs.com/gif.latex?S"/> – הקבוצה הנוכחית  
- <img src="https://latex.codecogs.com/gif.latex?c"/> – מספר הקבוצות (קטגוריות)  
- <img src="https://latex.codecogs.com/gif.latex?p_i"/> – הסיכוי של כל קבוצה (למשל, אחוז ה-Yes וה-No)
  
2. מחשבים את ה-**Entropy הממוצע אחרי הפיצול** (Weighted Average):
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy_{after}%20=%20\sum_{j=1}^{k}%20\frac{|S_j|}{|S|}%20Entropy(S_j)"/></p>  
  
  
- <img src="https://latex.codecogs.com/gif.latex?k"/> – מספר תתי הקבוצות שנוצרו מהפיצול  
- <img src="https://latex.codecogs.com/gif.latex?S_j"/> – כל תת-קבוצה  
- <img src="https://latex.codecogs.com/gif.latex?|S_j|"/> – גודל תת-הקבוצה  
- <img src="https://latex.codecogs.com/gif.latex?|S|"/> – גודל הקבוצה המקורית
  
3. מחשבים את ה-**Information Gain**:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?IG%20=%20Entropy_{before}%20-%20Entropy_{after}"/></p>  
  
  
##### דוגמה
  
נתונים:  
| Outlook  | Play Tennis |
|----------|-------------|
| Sunny    | No          |
| Sunny    | No          |
| Overcast | Yes         |
| Rain     | Yes         |
| Rain     | Yes         |
| Rain     | No          |
| Overcast | Yes         |
| Sunny    | Yes         |
| Sunny    | Yes         |
| Rain     | Yes         |
| Sunny    | No          |
| Overcast | Yes         |
| Overcast | Yes         |
| Rain     | No          |
  
##### 1. מחשבים Entropy לפני הפיצול (כל הדאטה):
  
- **Yes** = 9, **No** = 5
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy(S)%20=%20-\left(%20\frac{9}{14}%20\log_2%20\frac{9}{14}%20+%20\frac{5}{14}%20\log_2%20\frac{5}{14}%20\right)%20%20\approx%200.940"/></p>  
  
  
##### 2. מחשבים Entropy אחרי פיצול לפי Outlook:
  
- **Sunny** (5 דגימות): 2 Yes, 3 No → Entropy ≈ 0.971  
- **Overcast** (4 דגימות): 4 Yes, 0 No → Entropy = 0  
- **Rain** (5 דגימות): 3 Yes, 2 No → Entropy ≈ 0.971
  
- Weighted Entropy after split:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy_{after}%20=%20\frac{5}{14}%20\cdot%200.971%20+%20\frac{4}{14}%20\cdot%200%20+%20\frac{5}{14}%20\cdot%200.971%20%20\approx%200.693"/></p>  
  
  
##### 3. מחשבים Information Gain:
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?IG%20=%200.940%20-%200.693%20=%200.247"/></p>  
  
  
---
  
## 3. C4.5
  
**דוגמה**  
- חיזוי אם לקוח יקנה מחשב לפי גיל (רציף), תעסוקה (קטגוריה), והכנסה (רציף)
  
**מאפיין עיקרי**  
- **תומך בנתונים רציפים** – מסוגל להתמודד עם מספרים כמו גיל או הכנסה בלי לחלק לטווחים  
- **תומך בערכים חסרים** – יכול לעבוד גם אם חסר מידע בחלק מהשדות  
- כולל **גיזום (Pruning)** – קיצור העץ כדי למנוע **אוברפיטינג**
  
**מדד חלוקה**  
- **Gain Ratio** (שיפור של Information Gain)
  
---
  
### 4. Random Forest
  
**דוגמה**  
- **Churn prediction** – חיזוי **נטישת לקוחות** (למשל אם לקוח יעזוב חברת סלולר) לפי דפוסי שימוש, היסטוריית תשלומים ופניות לשירות לקוחות
  
**מאפיין עיקרי**  
- יער של **עצים אקראיים**  
  - כל עץ נבנה על **מדגם אקראי** של הנתונים (Random Sampling עם החזרה)  
  - בכל פיצול בעץ נבחרת **קבוצת תכונות אקראית** לבדיקה (Random Feature Selection)  
- משפר **דיוק** ומפחית **אוברפיטינג** ע"י שילוב תחזיות של עצים רבים
  
**מדד חלוקה**  
- כל עץ בתוך היער הוא לרוב **CART** – משתמש ב-**Gini impurity** (לסיווג) או **MSE** (לרגרסיה)
  
---
  
  
  
### 1. CART (Classification And Regression Tree)
  
- **מה זה?** אלגוריתם שמייצר עץ החלטה לסיווג (Classification) או חיזוי ערכים מספריים (Regression).
- **מדד חלוקה:** 
  - ב-Classification: משתמש ב-**Gini Impurity**
  - ברגרסיה: משתמש ב-**Mean Squared Error (MSE)**
- **מאפיינים:**
  - תמיד מפצל ל-2 קבוצות (Binary Split)
  - מתאים גם לסיווג וגם לרגרסיה
  
  
#### מה זה Information Gain?
  
מדד שמראה **כמה מידע הרווחנו** (או כמה אי-ודאות הפחתנו) כשאנחנו מחלקים קבוצה לפי פיצ'ר מסוים
  
##### למה צריך את זה?
  
כשבונים **עץ החלטה** (למשל ב-ID3 או C4.5), רוצים לבחור את הפיצ'ר **הכי טוב לפיצול** הדאטה
הפיצ'ר עם ה-**Information Gain** הכי גבוה הוא זה שמפחית הכי הרבה את אי-הוודאות (כלומר עושה את הקבוצה הכי "טהורה")
  
##### איך מחשבים את זה?
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Information/%20Gain%20=%20Entropy_{before}%20-%20Entropy_{after}"/></p>  
  
  
- **Entropy_before**: כמות אי-הוודאות לפני החלוקה
- **Entropy_after**: כמות אי-הוודאות הממוצעת אחרי החלוקה (לפי כל קבוצה שנוצרה)
  
##### דוגמה:
  
נניח שיש לנו 10 דוגמאות:
- 6 **כן** (קנה מוצר)
- 4 **לא** (לא קנה מוצר)
  
**1. מחשבים Entropy לפני החלוקה:**
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy_{before}%20=%20-%20(0.6%20/cdot%20/log_2(0.6)%20+%200.4%20/cdot%20/log_2(0.4))%20///approx%200.9709"/></p>  
  
  
**2. מחלקים לפי פיצ'ר (למשל גיל):**
  
**קבוצה 1 (גיל < 30):**
  
- 4 כן
- 1 לא
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy_1%20=%20-%20(0.8%20/cdot%20/log_2(0.8)%20+%200.2%20/cdot%20/log_2(0.2))%20///approx%200.7219"/></p>  
  
  
**קבוצה 2 (גיל >= 30):**
  
- 2 כן
- 3 לא
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy_2%20=%20-%20(0.4%20/cdot%20/log_2(0.4)%20+%200.6%20/cdot%20/log_2(0.6))%20///approx%200.9709"/></p>  
  
**3. מחשבים את ה-Entropy אחרי החלוקה:**
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Entropy_{after}%20=%20/frac{5}{10}%20/cdot%20Entropy_1%20+%20/frac{5}{10}%20/cdot%20Entropy_2%20//=%200.5%20/cdot%200.7219%20+%200.5%20/cdot%200.9709%20//=%200.8464"/></p>  
  
  
**4. מחשבים את Information Gain:**
  
<p align="center"><img src="https://latex.codecogs.com/gif.latex?Information/%20Gain%20=%200.9709%20-%200.8464%20=%200.1245"/></p>  
  
**מה זה אומר?**
  
- אם **IG גבוה** → הפיצ'ר הזה עוזר **להפחית את אי-הוודאות** → כדאי לבחור בו
- אם **IG נמוך** → הפיצ'ר כמעט לא משפיע → פחות כדאי לבחור בו
  
**טיפ:**
  
- ב-ID3 → תמיד בוחרים את הפיצ'ר עם ה-**IG הכי גבוה**
- ב-C4.5 → משתמשים ב-**Gain Ratio** (שיפור על IG) כדי למנוע העדפה לפיצ'רים עם הרבה קבוצות
  
---
  
  
  
---
  
## 2. ID3 (Iterative Dichotomiser 3)
  
- **מה זה?** אלגוריתם ליצירת עצי החלטה **לסיווג בלבד** (לא רגרסיה).
- **מדד חלוקה:** 
  - משתמש ב-**Entropy** וב-**Information Gain** כדי לבחור את הפיצ'ר הכי טוב לפיצול.
- **מאפיינים:**
  - יכול לפצל ליותר מ-2 קבוצות (לא רק בינארי).
  - לא תומך בנתונים חסרים.
  - מתאים רק לקטגוריות דיסקרטיות (לא תומך במספרים רציפים).
  
---
  
## 3. C4.5
  
- **מה זה?** שדרוג של ID3, גם לסיווג בלבד.
- **מדד חלוקה:** 
  - משתמש ב-**Gain Ratio** (שיפור של Information Gain כדי להתמודד עם בעיות פיצול לא הוגן).
- **מאפיינים:**
  - כן תומך במספרים רציפים (יודע לחלק אותם לפי סף).
  - תומך בנתונים חסרים.
  - מפסיק לבנות עץ כשהוא לא מוצא פיצול טוב (Pruning - גיזום).
  
---
  
## השוואה קצרה:
  
| תכונה              | CART                  | ID3                    | C4.5                 |
|--------------------|-----------------------|------------------------|----------------------|
| שימוש              | סיווג ורגרסיה         | סיווג בלבד             | סיווג בלבד          |
| מדד חלוקה          | Gini / MSE            | Entropy + Info Gain    | Gain Ratio           |
| תומך במספרים רציפים| כן                    | לא                     | כן                   |
| תומך בנתונים חסרים | כן                    | לא                     | כן                   |
| סוג הפיצול         | בינארי בלבד           | לא חייב בינארי         | לא חייב בינארי       |
  
---
  
## גרפים (במילים 😅):
  
- **Gini** מקסימלי כשההסתברויות שוות (0.5, 0.5).
- **Entropy** גם מקסימלי כשההסתברויות שוות, אבל הערך שלו גדול יותר (1 לעומת 0.5 ב-Gini לשני מחלקות).
  
  
  
  