# 📈 Support Vector Regression (SVR) – הסבר מלא בעברית

## מה זה SVR?
SVR (Support Vector Regression) הוא גרסה של אלגוריתם SVM, אבל במקום סיווג (Classification) – הוא מבצע **רגרסיה** (חיזוי של ערכים רציפים, כמו מחירים או טמפרטורות).

בדיוק כמו SVM, SVR מנסה למצוא **פונקציה פשוטה ככל האפשר** שמתאימה לנתונים — אך תוך התעלמות משגיאות קטנות.

---

## 🎯 הרעיון המרכזי

במקום לדרוש שהמודל יתאים לכל נקודה בדיוק, SVR מאפשר **מרווח סובלנות (ε)** סביב הפונקציה. כל עוד השגיאה בתוך ה־ε הזה — לא אכפת לנו. רק נקודות **שמעל או מתחת למרווח הזה** נחשבות.

> ✨ המטרה: למצוא פונקציה **שכמה שיותר נקודות נופלות סביבה בטווח של ±ε**.

---

## 📐 הצינור של SVR – "ε-Tube"

- זהו אזור בגובה ±ε סביב פונקציית החיזוי
- נקודות **בתוך הצינור** → לא משפיעות על הפונקציה
- נקודות **מחוץ לצינור** → משפיעות (Support Vectors)

---

## 🧠 מהן נקודות תמיכה (Support Vectors) ב־SVR?

- הן **הנקודות שמחוץ לצינור** (השגיאה שלהן > ε)
- הן מקבלות משקלים \( \alpha_i \) או \( \alpha_i^* \)
- הן **היחידות שמשפיעות על הפונקציה**

### 🔍 דימוי:
> הגרף הוא גומייה, והנקודות שמחוץ לצינור הן חוטים שמושכים את הגומייה — **ומעצבות את הקו**

---

## ⚙️ נוסחה כללית של SVR:

$$
\text{Minimize: } \frac{1}{2} \|w\|^2 + C \sum_i (\xi_i + \xi_i^*)
$$

בהתאם לתנאים:

$$
\begin{aligned}
& y_i - w^T x_i - b \leq \varepsilon + \xi_i \\
& w^T x_i + b - y_i \leq \varepsilon + \xi_i^*
\end{aligned}
$$

כאשר:
- \( \xi_i, \xi_i^* \) הן שגיאות מחוץ לטווח הסובלנות
- \( C \) שולט על כמה מענישים שגיאות גדולות

---

## 🔢 Linear מול Nonlinear

- ב־**Linear SVR**, קיימת ממש פונקציה מהצורה:  
  $$ f(x) = w^T x + b $$
  ואפשר לחשב את ה־\( w \) ישירות:
  $$ w = \sum_i (\alpha_i - \alpha_i^*) x_i $$

- ב־**Nonlinear SVR**, משתמשים ב־**Kernel** כדי למדוד דמיון, ולא מחושבת פונקציית w מפורשת:
  $$ f(x) = \sum_i (\alpha_i - \alpha_i^*) K(x_i, x) + b $$

---

## 🎛️ פרמטרים חשובים ב־SVR

| פרמטר | תפקיד |
|--------|--------|
| `ε` (epsilon) | קובע את רוחב הצינור – שגיאות קטנות ממנו מתעלמים |
| `C` | מעניש על נקודות מחוץ לצינור – שולט על המורכבות |
| `kernel` | צורת הפונקציה (linear, rbf, poly, sigmoid) |
| `gamma` | משמש בקרנלים לא ליניאריים – קובע את ה"טווח" של ההשפעה |

---

## 🧪 דוגמה בסיסית בקוד
```python
from sklearn.svm import SVR
import numpy as np
import matplotlib.pyplot as plt

# יצירת נתונים
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = np.sin(X).ravel()
y[::5] += 0.5 * np.random.randn(20)  # הוספת רעש קל

# מודל SVR עם kernel RBF
model = SVR(kernel='rbf', C=100, epsilon=0.1)
model.fit(X, y)
y_pred = model.predict(X)

# גרף
plt.scatter(X, y, color='gray', label='Data')
plt.plot(X, y_pred, color='red', label='SVR Prediction')
plt.title('SVR with ε-Tube')
plt.legend()
plt.show()
```

---

## 💬 סיכום מהיר

| תכונה              | SVM (Classification)       | SVR (Regression)                      |
|---------------------|-----------------------------|----------------------------------------|
| המטרה              | הפרדה בין מחלקות           | התאמה לפונקציה תוך טולרנס של ε       |
| margin              | מרחק בין מחלקות            | אין margin – יש צינור סובלנות         |
| נקודות תומכות      | נוגעות בקווי margin         | מחוץ לצינור – משפיעות על הפונקציה     |
| Penalized points    | נקודות חורגות מהמחלקה      | נקודות עם שגיאה > ε                   |
| משוואת חיזוי        | \( f(x) = w^T x + b \)       | \( f(x) = \sum_i (\alpha_i - \alpha_i^*) K(x_i, x) + b \) |

---

רוצה שנוסיף גם גרף שמראה את הצינור והנקודות התומכות? 😘