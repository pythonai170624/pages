# ðŸ“ˆ Support Vector Regression (SVR)

## ×ž×” ×–×” SVR?

SVR (Support Vector Regression) 

×”×•× ×’×¨×¡×” ×©×œ ××œ×’×•×¨×™×ª× ×ž×›×•× ×ª ×•×§×˜×•×¨×™× ×ª×•×ž×›×™×, 
××‘×œ ×‘×ž×§×•× ×¡×™×•×•×’ (×§×œ×¡×™×¤×™×§×¦×™×”) â€“ ×”×•× ×ž×‘×¦×¢ **×¨×’×¨×¡×™×”** (×—×™×–×•×™ ×©×œ ×¢×¨×›×™× ×¨×¦×™×¤×™×, ×›×ž×• ×ž×—×™×¨×™× ××• ×˜×ž×¤×¨×˜×•×¨×•×ª)

×”×•× ×ž× ×¡×” ×œ×ž×¦×•× **×¤×•× ×§×¦×™×” ×¤×©×•×˜×” ×›×›×œ ×”××¤×©×¨** ×©×ž×ª××™×ž×” ×œ× ×ª×•× ×™× â€” ××š ×ª×•×š ×”×ª×¢×œ×ž×•×ª ×ž×©×’×™××•×ª ×§×˜× ×•×ª

---

# ðŸŽ¯ Core Idea

Instead of requiring the model to fit every point exactly, SVR allows a **tolerance margin (Îµ)** around the function. As long as the error is within this Îµ â€” we don't care. Only points **above or below this margin** matter.

> âœ¨ Goal: Find a function where **as many points as possible fall within Â±Îµ**.

---

## ðŸ“ The SVR Tube â€“ "Îµ-Tube"

- This is a region of Â±Îµ around the predicted function.
- Points **inside the tube** â†’ do not affect the function.
- Points **outside the tube** â†’ affect it (Support Vectors).

---

## ðŸ§  What Are Support Vectors in SVR?

- They are **points outside the tube** (error > Îµ).
- They receive weights \( \alpha_i \) or \( \alpha_i^* \). see below
- They are the **only ones that influence the prediction function**.

### ðŸ” Analogy:
> The graph is like a rubber band, and the points outside the tube are strings pulling on it â€” **shaping the function**.

## ðŸŒŸ What is Alpha-Star (\( \alpha_i^* \))?

In SVR, every support vector may influence the prediction in one of two ways:

- If the prediction is **too high** compared to the true value â†’ it contributes with \( \alpha_i \)
- If the prediction is **too low** compared to the true value â†’ it contributes with \( \alpha_i^* \)

This leads to the full prediction function:

$$
f(x) = \sum_i (\alpha_i - \alpha_i^*) K(x_i, x) + b
$$

Only **one** of the two (\( \alpha_i \) or \( \alpha_i^* \)) is non-zero for a given point. Together, they create a tug-of-war:

- \( \alpha_i \) pulls the function **down** (penalizing overestimation)
- \( \alpha_i^* \) pulls the function **up** (penalizing underestimation)

> So the support vectors collectively shape the regression line based on whether the model over- or under-shoots the true value.

##### ðŸ” But what is \( K(x_i, x) \)?

- \( K(x_i, x) \) is the **kernel function**.
- It measures the **similarity** between a training point \( x_i \) and a prediction point \( x \).
- But hereâ€™s the magic:

> ðŸª„ The kernel allows us to act as if weâ€™ve mapped the data into a higher-dimensional space â€” **without actually doing it**.

This is what enables SVR (and SVM in general) to model complex, nonlinear functions without computational cost of actual transformation. Thatâ€™s the **kernel trick**!

---

## âš™ï¸ General SVR Optimization Formula

$$
\text{Minimize: } \frac{1}{2} \|w\|^2 + C \sum_i (\xi_i + \xi_i^*)
$$

Subject to:

$$
\begin{aligned}
& y_i - w^T x_i - b \leq \varepsilon + \xi_i \\
& w^T x_i + b - y_i \leq \varepsilon + \xi_i^*
\end{aligned}
$$

Where:
- \( \xi_i \): the amount the prediction exceeds the upper boundary (above the Îµ-tube)
- \( \xi_i^* \): the amount the prediction falls below the lower boundary (below the Îµ-tube)
- Only one of \( \xi_i \) or \( \xi_i^* \) is non-zero per data point
- \( C \) controls the penalty for large errors (how much we care about violations outside the tube)
- xð‘– is the input vector (feature)
- ð‘¦ð‘– is the true output (label) for that input

---

## ðŸ”¢ Linear vs Nonlinear SVR

- In **Linear SVR**, the function is explicitly:
  $$ f(x) = w^T x + b $$
  and we can compute:
  $$ w = \sum_i (\alpha_i - \alpha_i^*) x_i $$

- In **Nonlinear SVR**, we use a **kernel** to measure similarity, and we donâ€™t compute an explicit w:
  $$ f(x) = \sum_i (\alpha_i - \alpha_i^*) K(x_i, x) + b $$

---

## ðŸŽ›ï¸ Important SVR Parameters

| Parameter | Role |
|----------|------|
| `Îµ` (epsilon) | Defines the width of the tube â€“ errors smaller than Îµ are ignored |
| `C` | Penalizes points outside the tube â€“ controls model complexity |
| `kernel` | Determines function shape (linear, rbf, poly, sigmoid) |
| `gamma` | Used in nonlinear kernels â€“ sets how far each point's influence reaches |

---

## ðŸ§ª Basic Example in Code
```python
from sklearn.svm import SVR
import numpy as np
import matplotlib.pyplot as plt

# Generate data
X = np.linspace(0, 10, 100).reshape(-1, 1)
y = np.sin(X).ravel()
y[::5] += 0.5 * np.random.randn(20)  # add some noise

# SVR model with RBF kernel
model = SVR(kernel='rbf', C=100, epsilon=0.1)
model.fit(X, y)
y_pred = model.predict(X)

# Plot
plt.scatter(X, y, color='gray', label='Data')
plt.plot(X, y_pred, color='red', label='SVR Prediction')
plt.title('SVR with Îµ-Tube')
plt.legend()
plt.show()
```

---

## ðŸ’¬ Quick Summary

| Feature              | SVM (Classification)       | SVR (Regression)                      |
|----------------------|-----------------------------|----------------------------------------|
| Goal                 | Separate between classes    | Fit function with Îµ tolerance         |
| Margin               | Distance between classes    | No margin â€“ there's a tolerance tube  |
| Support Vectors      | Touch the margin boundaries | Outside the tube â€“ influence function |
| Penalized Points     | Misclassified samples        | Points with error > Îµ                 |
| Prediction Formula   | \( f(x) = w^T x + b \)       | \( f(x) = \sum_i (\alpha_i - \alpha_i^*) K(x_i, x) + b \) |

---

Want me to add a plot showing the Îµ-tube and the support vectors? ðŸ˜˜
