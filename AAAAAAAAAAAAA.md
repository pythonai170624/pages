## פונקציית הפעלה (Activation Function)

כדי שהנוירון ברשת יוכל לזהות דפוסים מורכבים בדאטה, הוא צריך פונקציית הפעלה – שהיא בעצם פונקציה מתמטית שמוסיפה **לא־לינאריות** לפלט של הנוירון
אם לא נשתמש בפונקציית הפעלה, כל הרשת תתנהג כמו רגרסיה ליניארית פשוטה, ולא תוכל לפתור בעיות מורכבות

### 🎯 למה זה חשוב?

כי העולם לא לינארי – ולכן גם המודלים שלנו צריכים להכיל עקומות ודפוסים לא ישרים

## סוגים של פונקציות הפעלה

### ✅ Threshold Function

מוציאה ערכים של 0 או 1 לפי סף מסוים (למשל 0)

* אם סכום המשקלים והקלטים גדול או שווה ל־0 → הפלט יהיה 1
* אחרת → הפלט יהיה 0

### ✅ Sigmoid Function

הפונקציה הלוגיסטית – ממירה כל מספר לפלט רציף בין 0 ל־1
מצוינת למודלים של סיווג בינארי, כי אפשר להבין את הפלט כסבירות (הסתברות)

### ✅ ReLU (Rectified Linear Unit)

הכי נפוצה ברשתות עמוקות – מחזירה את הקלט אם הוא חיובי, ואם לא אז 0
פשוטה ומהירה – ולכן נפוצה בשכבות נסתרות

### ✅ Tanh (Hyperbolic Tangent)

ממירה את הקלט לפלט בין ‎-1 ל־1 – כלומר מרכז את הקלט סביב 0
מתאימה לדאטה שמכיל גם ערכים שליליים

### ✅ Softmax

ממירה את כל הפלטים של שכבת הפלט **להסתברויות** כך שהסכום שלהן יהיה 1
כל נוירון בשכבת הפלט מקבל ציון (logit) וה־Softmax ממירה את כל הציונים האלו לפלטים שנראים כמו הסתברות
הפלט של Softmax הוא **וקטור של הסתברויות** – לא מספר אחד

### 🎯 דוגמה:

אם המודל מנסה לסווג תמונה לחיה מתוך {חתול, כלב, סוס} והוא מחזיר:

* חתול → 0.7
* כלב → 0.2
* סוס → 0.1

אז הפלט הוא: `[0.7, 0.2, 0.1]` והסכום הוא 1

לא שהפלט הוא תמיד 1 – אלא שסך כל ההסתברויות בקטגוריות שווה ל־1ממירה את כל הפלטים של שכבת הפלט **להסתברויות** כך שהסכום שלהן יהיה 1
מצוינת לסיווג רב־קטגורי (Multi-Class Classification)

### ✅ Linear

משאירה את הפלט כמו שהוא – משמשת לרוב ברגרסיה (כאשר הפלט הוא מספר רציף ולא קטגוריה)

**add here picture from page 15**

## תהליך למידה אחורית (Backpropagation)

### 🧠 מה זה בעצם?

אחרי שהרשת מנסה לנבא משהו – היא בודקת כמה היא טעתה, ואז חוזרת אחורה ומעדכנת את המשקלים כדי לטעות פחות בפעם הבאה
זה כמו מורה שמתקן את עצמו לפי התוצאה של הבחינה

### ✏️ איך זה עובד בפועל?

1. מחשבים את הפלט של הנוירון כמו שלמדנו (סכום קלטים × משקלים → פונקציית הפעלה)
2. מחשבים את **שגיאת התחזית** (כמה רחוק הפלט מהתוצאה הנכונה)
3. מחשבים **נגזרת** של פונקציית העלות לפי כל משקל → כדי לדעת באיזה כיוון צריך לשנות אותו
4. מעדכנים את המשקלים בהתאם למהירות הלמידה (learning rate)
5. חוזרים על זה שוב ושוב עם כל הדאטה

### 💡 דוגמה:

אם אנחנו חוזים ציון מבחן של תלמיד לפי גיל וימי למידה – המודל ינסה לצמצם את השגיאה בין הציון שחזה לציון האמיתי
**add here picture from page 17**

## סוגי פונקציות עלות (Cost Functions)

* 🔢 בעיה רציפה (רגרסיה) → משתמשים ב־**MSE (Mean Squared Error)**
* 🧮 בעיה קטגורית (סיווג) → משתמשים ב־**Categorical Cross-Entropy**

💡 כל נוירון ברשת עושה לעצמו backpropagation – אבל כל הרשת מתעדכנת בצורה **מתואמת** כדי לצמצם את הטעות הכוללת

## שיטות עדכון משקלות

* 🔁 Batch Gradient Descent → מחשב את הטעות על כל הדאטה ואז מעדכן
* 🔀 Stochastic Gradient Descent (SGD) → מעדכן משקלות כל פעם אחרי דוגמה אחת בלבד
* ⚖️ Mini-Batch Gradient Descent → פשרה בין השניים – מחשב על קבוצות קטנות בכל פעם
