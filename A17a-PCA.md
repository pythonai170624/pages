## PCA - Principal Component Analysis

שיטת **PCA** (ניתוח רכיבים עיקריים) היא שיטה נפוצה בלמידת מכונה וסטטיסטיקה שנועדה לבצע **צמצום ממדים** – כלומר, להפוך דאטה עם הרבה תכונות (Features) לדאטה עם פחות תכונות, מבלי לאבד יותר מדי מידע חשוב.

פי.סי.איי יוצרת **ציר חדש** שמורכב משילוב של התכונות המקוריות – הציר הזה נקרא **רכיב עיקרי** (Principal Component) – והוא משמר את מירב השונות בדאטה.

### למה להשתמש ב־PCA?

- דאטה עם הרבה תכונות גורם לבעיות: **עומס חישובי**, **קושי בויזואליזציה**, ו־**הסתברות לאוברפיטינג**
- PCA מצמצם את מספר התכונות, אבל שומר על כמה שיותר **שונות** מהדאטה המקורי

## PCA and Unsupervised Learning

ב־Unsupervised Learning אין לנו תוויות (labels), ולכן אין דרך לדעת איזו תכונה הכי חשובה
PCA מאפשרת לצמצם תכונות **באופן חכם ולא מפוקח** – בלי לדעת מה חשוב למה

המטרה היא להקרין את הדאטה למרחב חדש עם פחות ממדים, תוך כדי שמירה על כמה שיותר מהמידע (variance)

### שים לב:
פי.סי.איי לא בוחר תכונות קיימות – הוא מייצר **חדשות** שהן שילובים ליניאריים של הישנות

## Principal Components

הרכיבים העיקריים הם צירים חדשים שנוצרים משילוב של התכונות המקוריות
כל רכיב הוא **ציר אורתוגונלי** לרכיבים האחרים, ומכיל כמה שיותר שונות (variance)

<img src="pca1.png" style="width: 90%" />

### Explained Variance

מספר שמציין **כמה שונות** (variance) נשמרת ע״י כל רכיב עיקרי
רכיב עם Explained Variance גבוה → יותר מידע נשמר

משמש להחלטה כמה רכיבים לשמור (למשל: מספיק לי 2 רכיבים אם הם שומרים 90% מהשונות)

### Eigenvalues

ה־Eigenvalue (ערך עצמי) של כל רכיב מציין כמה שונות הוא תורם לדאטה
ככל שהערך גבוה → הרכיב הזה חשוב יותר

### Eigenvectors

וקטורים שמתארים את **הכיוון** של כל רכיב עיקרי
הם בעצם השילוב של התכונות הישנות ליצירת הרכיב החדש

### Covariance Matrix

מטריצה שמתארת את הקשר (קו-וריאציה) בין כל שני פיצ'רים
PCA משתמשת במטריצה הזו כדי להבין אילו תכונות משתנות יחד ולמצוא את הצירים החדשים

## Explained Variance

כשאנחנו עושים PCA, אנחנו רוצים לדעת כמה מידע (שונות) נשאר אחרי שהפכנו את הדאטה למשהו פשוט יותר

שונות = כמה הנתונים שלנו מתפזרים או משתנים  
Explained Variance זה כמה מתוך כל השונות בדאטה נשמרה בכל רכיב חדש שיצרנו

### דוגמה:
אם נוריד את כל הציונים של כיתה מ־100, ונקבל:
- רכיב ראשון שמסביר 80% מההבדלים בין תלמידים
- רכיב שני שמסביר עוד 15%
אז סך הכול 95% מהמידע נשמר – וזה מעולה

## Eigenvalues (ערכים עצמיים)

זה מספר שאומר כמה חשובה כל צורת הסתכלות חדשה על הדאטה

PCA מנסה לסובב את הדאטה כדי למצוא את הכיוון שבו יש הכי הרבה שונות  
ה־Eigenvalue של כיוון מסוים אומר כמה מהשונות הכללית של הדאטה נמצאת על הציר הזה

### דוגמה:
אם לציר A יש ערך 4.8 ולציר B יש ערך 1.2, אז ציר A מסביר הרבה יותר מידע

## Eigenvectors (וקטורים עצמיים)

וקטור עצמי הוא הכיוון שאליו אנחנו מסתכלים על הדאטה החדש  
כל רכיב עיקרי (Principal Component) הוא בעצם שילוב של כמה תכונות קיימות, בכיוונים מסוימים

### דוגמה:
אם הדאטה שלנו כולל גובה ומשקל  
PCA יוצר רכיב חדש כמו:  
0.6 × גובה + 0.8 × משקל  
הכיוונים האלה הם ה־Eigenvectors – והם מסבירים איך נראה הציר החדש

## Covariance Matrix (מטריצת קווריאציה)

זאת טבלה שמראה איך כל שתי תכונות משתנות יחד

- אם שתיהן עולות יחד – הקשר חיובי  
- אם אחת עולה והשנייה יורדת – שלילי  
- אם אין קשר – הקשר אפסי

PCA משתמש במטריצה הזו כדי להבין לאן כדאי “להסתכל” כדי לשמור הכי הרבה מידע

### דוגמה:
אם בקובץ יש גובה ומשקל, והם תמיד עולים יחד (למשל בילדים) – אז הקווריאציה ביניהם תהיה גבוהה  
PCA יקח את זה בחשבון וייצר רכיב חדש שמבוסס על שניהם


## כיצד PCA פועל בפועל?

1. מנרמלים את הדאטה (סקיילינג)
2. מחשבים מטריצת קווריאציה
3. מחשבים את ה־Eigenvalues וה־Eigenvectors
4. בוחרים את הרכיבים עם הערכים הגבוהים ביותר (שמכילים את מירב השונות)
5. מקרינים את הדאטה למרחב החדש
