## PCA - Principal Component Analysis

שיטת **PCA** (ניתוח רכיבים עיקריים) היא שיטה נפוצה בלמידת מכונה וסטטיסטיקה שנועדה לבצע **צמצום ממדים** – כלומר, להפוך דאטה עם הרבה תכונות (Features) לדאטה עם פחות תכונות, מבלי לאבד יותר מדי מידע חשוב.

פי.סי.איי יוצרת **ציר חדש** שמורכב משילוב של התכונות המקוריות – הציר הזה נקרא **רכיב עיקרי** (Principal Component) – והוא משמר את מירב השונות בדאטה.

### למה להשתמש ב־PCA?

- דאטה עם הרבה תכונות גורם לבעיות: **עומס חישובי**, **קושי בויזואליזציה**, ו־**הסתברות לאוברפיטינג**
- PCA מצמצם את מספר התכונות, אבל שומר על כמה שיותר **שונות** מהדאטה המקורי

## PCA and Unsupervised Learning

ב־Unsupervised Learning אין לנו תוויות (labels), ולכן אין דרך לדעת איזו תכונה הכי חשובה
PCA מאפשרת לצמצם תכונות **באופן חכם ולא מפוקח** – בלי לדעת מה חשוב למה

המטרה היא להקרין את הדאטה למרחב חדש עם פחות ממדים, תוך כדי שמירה על כמה שיותר מהמידע (variance)

### שים לב:
PCA לא בוחר תכונות קיימות – הוא מייצר **חדשות** שהן שילובים ליניאריים של הישנות

## Principal Components

הרכיבים העיקריים הם צירים חדשים שנוצרים משילוב של התכונות המקוריות
כל רכיב הוא **ציר אורתוגונלי** לרכיבים האחרים, ומכיל כמה שיותר שונות (variance)

## Explained Variance

מספר שמציין **כמה שונות** (variance) נשמרת ע״י כל רכיב עיקרי
רכיב עם Explained Variance גבוה → יותר מידע נשמר

משמש להחלטה כמה רכיבים לשמור (למשל: מספיק לי 2 רכיבים אם הם שומרים 90% מהשונות)

## Eigenvalues

ה־Eigenvalue (ערך עצמי) של כל רכיב מציין כמה שונות הוא תורם לדאטה
ככל שהערך גבוה → הרכיב הזה חשוב יותר

## Eigenvectors

וקטורים שמתארים את **הכיוון** של כל רכיב עיקרי
הם בעצם השילוב של התכונות הישנות ליצירת הרכיב החדש

### דוגמה:
אם יש לי שני פיצ'רים X1 ו־X2, והרכיב הראשון הוא שילוב של 0.7·X1 + 0.7·X2 – זה ה־eigenvector

## Covariance Matrix

מטריצה שמתארת את הקשר (קו-וריאציה) בין כל שני פיצ'רים
PCA משתמשת במטריצה הזו כדי להבין אילו תכונות משתנות יחד ולמצוא את הצירים החדשים

## כיצד PCA פועל בפועל?

1. מנרמלים את הדאטה (סקיילינג)
2. מחשבים מטריצת קווריאציה
3. מחשבים את ה־Eigenvalues וה־Eigenvectors
4. בוחרים את הרכיבים עם הערכים הגבוהים ביותר (שמכילים את מירב השונות)
5. מקרינים את הדאטה למרחב החדש

📷 שקף 21 – מציג גרפית איך שני פיצ'רים הופכים לרכיב עיקרי אחד לפי ציר עם הכי הרבה שונות
📷 שקף 22 – מראה את בחירת הרכיב המתאים ביותר (Principal Component 1)
📷 שקף 23 – מראה את השילוב הליניארי של X1 ו־X2 שיוצרים את הרכיב החדש

רוצה שאכין לך גם את הקוד הפייתוני שמופיע משקף 24 ואילך כולל גרפים? 😘

