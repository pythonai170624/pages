# KNN - ×ª×¨×—×™×©×™ ×ª×™×§×• ×•×©×™×˜×•×ª ××ª×§×“××•×ª

## ×˜×™×¤×•×œ ×‘××§×¨×™ ×ª×™×§×•

××—×ª ×”×‘×¢×™×•×ª ×©×¢×œ×•×œ×•×ª ×œ×”×ª×¢×•×¨×¨ ×‘××œ×’×•×¨×™×ª× KNN ×”×™× ×›××©×¨ ×§×™×™× ×ª×™×§×• (tie) ×‘×™×Ÿ ××¡×¤×¨ ×§×˜×’×•×¨×™×•×ª. ××¦×‘ ×–×” ××ª×¨×—×© ×›××©×¨ K ×©×›× ×™× ××ª×—×œ×§×™× ×‘××•×¤×Ÿ ×©×•×•×” ×‘×™×Ÿ ×©×ª×™ ×§×˜×’×•×¨×™×•×ª ××• ×™×•×ª×¨

**×“×•×’××”**: × × ×™×— ×©×‘×—×¨× ×• K=4 ×•××¦×× ×• ×©××ª×•×š ××¨×‘×¢×ª ×”×©×›× ×™× ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨, 2 ×”× ×ª×¤×•×—×™× ×•-2 ×”× ×ª×¤×•×–×™×. ××™×š × ×—×œ×™×˜ ×œ××™×–×• ×§×˜×’×•×¨×™×” ×œ×¡×•×•×’ ××ª ×”×¤×¨×™ ×”×—×“×©?

### ××¡×˜×¨×˜×’×™×•×ª ×œ×˜×™×¤×•×œ ×‘×ª×™×§×•:

1. **×‘×—×™×¨×ª K ××™-×–×•×’×™**: ×”×©×™×˜×” ×”×¤×©×•×˜×” ×‘×™×•×ª×¨ ×”×™× ×œ×”×©×ª××© ×‘-K ××™-×–×•×’×™ (3, 5, 7, ×•×›×•') ×›×“×™ ×œ×× ×•×¢ ×ª×™×§×• ××œ×›×ª×—×™×œ×”. ×–×” ×¢×•×‘×“ ×”×™×˜×‘ ×¢×‘×•×¨ ×‘×¢×™×•×ª ×¡×™×•×•×’ ×‘×™× ××¨×™ (×©×ª×™ ×§×˜×’×•×¨×™×•×ª)

2. **×©×§×œ×•×œ ×œ×¤×™ ××¨×—×§**: ×‘××§×•× ×œ×ª×ª ×œ×›×œ ×©×›×Ÿ ××©×§×œ ×–×”×”, × ×©×§×œ×œ ××ª ×”×§×‘×•×¦×” ×©×œ ×”× ×§×•×“×” ×œ×¤×™ 1 ×—×œ×§×™ d, ×›××©×¨ d ×”×•× ×”××¨×—×§ ××”× ×§×•×“×” ×”×—×“×©×”. ×›×š, ×©×›× ×™× ×§×¨×•×‘×™× ×™×•×ª×¨ ××§×‘×œ×™× ×”×©×¤×¢×” ×’×“×•×œ×” ×™×•×ª×¨:

$$\text{××©×§×œ ×”×©×›×Ÿ} = \frac{1}{d(x, x_i)}$$

×“×•×’××” ××¡×¤×¨×™×ª â€“×œ×¤×™ 

$$
\frac{1}{d}
$$

× × ×™×— ×©×”×©×ª××©× ×• ×‘-K=4 ×•×”×©×›× ×™× ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨ ×œ×¤×¨×™ ×—×“×© ×”×:

| Neighbor | Class     | Distance from new fruit \( d \) | Weight $\frac{1}{d}$ |
|----------|-----------|-------------------------------|--------------------------|
| 1        | Apple ğŸ   | 1.0                           | 1.00                     |
| 2        | Apple ğŸ   | 2.0                           | 0.50                     |
| 3        | Orange ğŸŠ  | 3.0                           | 0.33                     |
| 4        | Orange ğŸŠ  | 4.0                           | 0.25                     |

- **×ª×¤×•×— ğŸ**:  
  \( 1.00 + 0.50 = 1.50 \)

- **×ª×¤×•×– ğŸŠ**:  
  \( 0.33 + 0.25 = 0.58 \)


**××¡×§× ×”:**

×œ××¨×•×ª ×©×™×© ×ª×™×§×• ×‘××¡×¤×¨ ×”×§×˜×’×•×¨×™×•×ª (2 ×ª×¤×•×—×™×, 2 ×ª×¤×•×–×™×),  
×›××©×¨ ××‘×¦×¢×™× ×©×§×œ×•×œ ×œ×¤×™ ×”××¨×—×§ â€“ ×”×©×›× ×™× ×”×§×¨×•×‘×™× ××©×¤×™×¢×™× ×™×•×ª×¨,  
×•×œ×›×Ÿ ×”×§×˜×’×•×¨×™×” **×ª×¤×•×—** ×× ×¦×—×ª

3. **×”×•×¨×“×ª K ×‘-1**: ×× ××ª×¨×—×© ×ª×™×§×•, × ×™×ª×Ÿ ×œ×”×§×˜×™×Ÿ ××ª K ×‘××•×¤×Ÿ ×–×× ×™ ×‘-1 ×•×œ×‘×“×•×§ ×× ×”×ª×™×§×• × ×¤×ª×¨

4. **×”×¢×“×¤×ª ×§×˜×’×•×¨×™×”**: ×× ×™×© ×¡×™×‘×” ×œ×•×’×™×ª ×œ×”×¢×“×™×£ ×§×˜×’×•×¨×™×” ××¡×•×™××ª (×œ××©×œ ×§×˜×’×•×¨×™×” ×©×›×™×—×” ×™×•×ª×¨), × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×” ×›××›×¨×™×¢×” ×‘××§×¨×™ ×ª×™×§×•

5. **×‘×—×™×¨×” ××§×¨××™×ª**: ×‘×—×™×¨×” ××§×¨××™×ª ×©×œ ××—×ª ××”×§×˜×’×•×¨×™×•×ª ×”×©×•×•×ª

×“×•×’××ª ×§×•×“ ×œ×˜×™×¤×•×œ ×‘×ª×™×§×• ×‘×××¦×¢×•×ª ×©×§×œ×•×œ ××¨×—×§×™×:

```python
def weighted_knn_predict(X_train, y_train, x_new, k):
    # ×—×™×©×•×‘ ××¨×—×§×™× ×‘×™×Ÿ ×”× ×§×•×“×” ×”×—×“×©×” ×œ×›×œ × ×§×•×“×•×ª ×”××™××•×Ÿ
    distances = []
    for i, x_train in enumerate(X_train):
        dist = np.sqrt(np.sum((x_train - x_new) ** 2))
        distances.append((dist, i))
    
    # ××™×•×Ÿ ×”××¨×—×§×™× ×•×‘×—×™×¨×ª K ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨
    distances.sort()
    k_nearest = distances[:k]
    
    # ×©×§×œ×•×œ ×”×¦×‘×¢×•×ª ×œ×¤×™ ×”××¨×—×§
    class_votes = {}
    for dist, idx in k_nearest:
        weight = 1.0 / max(dist, 0.000001)  # ×× ×™×¢×ª ×—×œ×•×§×” ×‘××¤×¡
        vote = y_train[idx]
        
        if vote in class_votes:
            class_votes[vote] += weight
        else:
            class_votes[vote] = weight
    
    # ×‘×—×™×¨×ª ×”×§×˜×’×•×¨×™×” ×¢× ×”×¦×™×•×Ÿ ×”××©×•×§×œ×œ ×”×’×‘×•×” ×‘×™×•×ª×¨
    return max(class_votes.items(), key=lambda x: x[1])[0]
```

## ××“×“×™ ×”×¢×¨×›×”: Precision, Recall, F1, Support

×œ×”×¢×¨×›×ª ×‘×™×¦×•×¢×™ ××•×“×œ KNN, ××©×ª××©×™× ×‘×›××” ××“×“×™× ×¡×˜× ×“×¨×˜×™×™×:

### 1. Precision (×“×™×•×§)
××•×“×“ ××ª ××—×•×– ×”× ×™×‘×•×™×™× ×”×—×™×•×‘×™×™× ×©×”×™×• × ×›×•× ×™× ×‘×××ª:

$$\text{Precision} = \frac{TP}{TP + FP}$$

×›××©×¨:
- TP (True Positive): ×—×™×–×•×™ ×—×™×•×‘×™ × ×›×•×Ÿ
- FP (False Positive): ×—×™×–×•×™ ×—×™×•×‘×™ ×©×’×•×™

×“×™×•×§ ×’×‘×•×” ××©××¢×•×ª×• ×©×›××©×¨ ×”××•×“×œ ×× ×‘× '×›×Ÿ', ×”×•× ×‘×“×¨×š ×›×œ×œ ×¦×•×“×§

### 2. Recall (×›×™×¡×•×™ ××• ×¨×’×™×©×•×ª)
××•×“×“ ××™×–×” ××—×•×– ××”××§×¨×™× ×”×—×™×•×‘×™×™× ×‘×××ª ×”××•×“×œ ×”×¦×œ×™×— ×œ×–×”×•×ª:

$$\text{Recall} = \frac{TP}{TP + FN}$$

×›××©×¨:
- FN (False Negative): ×—×™×–×•×™ ×©×œ×™×œ×™ ×©×’×•×™

×›×™×¡×•×™ ×’×‘×•×” ××©××¢×•×ª×• ×©×”××•×“×œ ××–×”×” ××ª ×¨×•×‘ ×”××§×¨×™× ×”×—×™×•×‘×™×™× ×”×××™×ª×™×™×

### 3. F1 Score
×××•×¦×¢ ×”×¨××•× ×™ ×©×œ Precision ×•-Recall, ×××–×Ÿ ×‘×™×Ÿ ×©× ×™ ×”××“×“×™×:

$$\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$$

F1 ×’×‘×•×” ××©×§×£ ××™×–×•×Ÿ ×˜×•×‘ ×‘×™×Ÿ ×“×™×•×§ ×œ×›×™×¡×•×™.

### 4. Support
××¡×¤×¨ ×”××•×¤×¢×™× ×‘×¤×•×¢×œ ×©×œ ×›×œ ×§×˜×’×•×¨×™×” ×‘×§×‘×•×¦×ª ×”×‘×“×™×§×”

### ×“×•×’××” ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ (Confusion Matrix):

```               
                  | Predicted: Apple | Predicted: Orange | Predicted: Banana
------------------|------------------|-------------------|-------------------
Actual: Apple     |        14        |         2         |         1
Actual: Orange    |         3        |        16         |         0
Actual: Banana    |         0        |         1         |        13
```

×•×—×™×©×•×‘ ×”××“×“×™×:

```
              precision    recall  f1-score   support
       APPLE      0.82      0.82      0.82        17
       ORANGE     0.84      0.84      0.84        19
       BANANA     0.93      0.93      0.93        14

    accuracy                          0.86        50
   macro avg      0.86      0.86      0.86        50
weighted avg      0.86      0.86      0.86        50
```

×”×¡×‘×¨ ×¢×œ `macro avg` ×•Ö¾`weighted avg` ×‘×“×•"×— ×¡×™×•×•×’ (Classification Report)

×›××©×¨ ×× ×• ××©×ª××©×™× ×‘×¤×•× ×§×¦×™×” `classification_report` ×Ö¾Scikit-learn, ×× ×• ××§×‘×œ×™× ×˜×‘×œ×” ×¢× ××“×“×™× ×œ×›×œ ×§×˜×’×•×¨×™×”, ×›××•:
- **precision** â€“ ×“×™×•×§ ×”×ª×—×–×™×•×ª
- **recall** â€“ ×™×›×•×œ×ª ×œ×–×”×•×ª ××ª ×”××§×¨×™× ×”× ×›×•× ×™×
- **f1-score** â€“ the harmonic mean between precision and recall
- **support** â€“ ××¡×¤×¨ ×”×“×•×’×××•×ª ×©×œ ×›×œ ×§×˜×’×•×¨×™×”

××” ×–×” `macro avg`?

- **×××•×¦×¢ ×¤×©×•×˜** ×©×œ ×”××“×“×™× ×¢×‘×•×¨ ×›×œ ×”×§×˜×’×•×¨×™×•×ª.
- ×›×œ ×§×˜×’×•×¨×™×” ××§×‘×œ×ª **×—×©×™×‘×•×ª ×©×•×•×”**, ×‘×œ×™ ×§×©×¨ ×œ×’×•×“×œ ×©×œ×”.

ğŸ“Œ ×“×•×’××”:
×× × ×—×©×‘ ××ª `macro avg` ×œÖ¾precision:

$$
\text{macro avg precision} = \frac{0.82 + 0.84 + 0.93}{3} = 0.863
$$

××ª××™× ×›××©×¨ ×—×©×•×‘ ×œ× ×• ×œ×”×ª×™×™×—×¡ ×œ×›×œ ×§×˜×’×•×¨×™×” **×‘××•×¤×Ÿ ×©×•×•×”**.

××” ×–×” `weighted avg`?

- ××—×©×‘×™× ×××•×¦×¢ ×©×œ ×”××“×“×™×, ××‘×œ ×›×œ ×§×˜×’×•×¨×™×” ××§×‘×œ×ª **××©×§×œ ×œ×¤×™ ×›××•×ª ×”×“×•×’×××•×ª ×©×œ×” (support)**.
- ×§×˜×’×•×¨×™×” ×©×™×© ×œ×” ×™×•×ª×¨ ×“×•×’×××•×ª ×ª×©×¤×™×¢ ×™×•×ª×¨ ×¢×œ ×”×××•×¦×¢.

ğŸ“Œ ×“×•×’××”:
×× × ×—×©×‘ ××ª `weighted avg` ×œÖ¾precision:

$$
\text{weighted avg precision} = \frac{(17 \times 0.82) + (19 \times 0.84) + (14 \times 0.93)}{50} = 0.858
$$

××ª××™× ×›××©×¨ ×¨×•×¦×™× ×œ×§×‘×œ ××“×“ ×›×•×œ×œ ×©××™×™×¦×’ **××ª ×”××¦×™××•×ª ×©×œ ×”×”×ª×¤×œ×’×•×ª** ×‘×“××˜×”.

×˜×‘×œ×ª ×¡×™×›×•×:

| ×¡×•×’ ×××•×¦×¢       | ××™×š ××—×©×‘×™×?                   | ××ª×™ ××ª××™×?                         |
|------------------|-------------------------------|-------------------------------------|
| `macro avg`      | ×××•×¦×¢ ×¤×©×•×˜ ×©×œ ×›×œ ×”×§×˜×’×•×¨×™×•×ª     | ×›×©××ª×” ×¨×•×¦×” ×œ×ª×ª ×—×©×™×‘×•×ª ×©×•×•×” ×œ×›×•×œ×Ÿ   |
| `weighted avg`   | ×××•×¦×¢ ××©×•×§×œ×œ ×œ×¤×™ ××¡×¤×¨ ×“×•×’×××•×ª | ×›×©××ª×” ×¨×•×¦×” ×œ×©×§×£ ××ª ×”××¦×™××•×ª ×‘×“××˜×”   |


×§×•×“ ×¤×™×™×ª×•×Ÿ ×œ×”×¦×’×ª ××“×“×™ ×‘×™×¦×•×¢ **×¢×œ ×—×œ×§ ×”×˜×¡×˜:**

```python
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

colors = np.array([200, 50, 220, 240, 250, 230, 30, 40, 20])
sizes = np.array([7, 7, 6, 9, 8, 9, 12, 13, 11])
weights = np.array([150, 160, 140, 170, 165, 180, 120, 130, 115])
fruit_types = np.array(['apple', 'apple', 'apple', 'orange', 'orange', 'orange', 'banana', 'banana', 'banana'])

# Create feature matrix
X = np.column_stack((colors, sizes, weights))
y = fruit_types

# ××™××•×Ÿ ×”××•×“×œ
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = KNeighborsClassifier(n_neighbors=3)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Calculate and display classification report (precision, recall, f1-score, support)
print("Classification Report:")
report = classification_report(y_test, y_pred)
print(report)

# Calculate and display confusion matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Create prettier confusion matrix display with labels
fruit_labels = ['apple', 'orange', 'banana']
cm_df = pd.DataFrame(cm,
                    index=[f'Actual: {label}' for label in fruit_labels],
                    columns=[f'Predicted: {label}' for label in fruit_labels])
print("\nConfusion Matrix (labeled):")
print(cm_df)

# Visualize confusion matrix with heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=fruit_labels,
            yticklabels=fruit_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix for Fruit Classification')
plt.tight_layout()
plt.show()
```

## ×©×™×˜×•×ª ×œ×‘×—×™×¨×ª K ××•×¤×˜×™××œ×™

### 1. Elbow Method

×©×™×˜×ª ×”-Elbow ××‘×•×¡×¡×ª ×¢×œ ×‘×“×™×§×ª ×©×™×¢×•×¨ ×”×©×’×™××” ×©×œ ×”××•×“×œ ×¢× ×¢×¨×›×™ K ×©×•× ×™×, ×•×—×™×¤×•×© × ×§×•×“×ª "××¨×¤×§" ×©×‘×” ×ª×•×¡×¤×ª ×¢×¨×›×™ K × ×•×¡×¤×™× ××™× ×” ××©×¤×¨×ª ××©××¢×•×ª×™×ª ××ª ×”×‘×™×¦×•×¢×™×:

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# ×”×›× ×ª ×”× ×ª×•× ×™×
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ×‘×“×™×§×ª ×“×™×•×§ ×¢×‘×•×¨ ×¢×¨×›×™ K ×©×•× ×™×
k_range = range(1, 31)
scores = []

for k in k_range:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    scores.append(accuracy_score(y_test, y_pred))  # accuracy = (TP + TN) / (TP + TN + FP + FN)

# ×”×¦×’×ª ×”×ª×•×¦××•×ª ×‘×’×¨×£
plt.figure(figsize=(10, 6))
plt.plot(k_range, scores, marker='o')
plt.title('KNN: Accuracy for different K values')
plt.xlabel('K Value')
plt.ylabel('Accuracy')
plt.xticks(k_range[::2])  # ×”×¦×’×ª ×¢×¨×›×™ K ×–×•×’×™×™× ×‘×œ×‘×“ ×œ× ×•×—×•×ª
plt.grid(True)
plt.show()

# ××¦×™××ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™
optimal_k = k_range[np.argmax(scores)]
print(f"×¢×¨×š K ×”××•×¤×˜×™××œ×™ ×”×•×: {optimal_k} ×¢× ×“×™×•×§ ×©×œ: {max(scores):.4f}")
```

<img src="knn4.png" style="width:60%;"/>

×‘×©×™×˜×ª ×”-Elbow, ×× ×• ××—×¤×©×™× ××ª ×”× ×§×•×“×” ×©×‘×” ×”×©×™×¤×•×¨ ×‘×“×™×•×§ ××ª×—×™×œ ×œ×”×ª××ª×Ÿ ××©××¢×•×ª×™×ª, ×›××• "××¨×¤×§" ×‘×’×¨×£.

### 2. Cross-Validation (××™××•×ª ×¦×•×œ×‘)

××™××•×ª ×¦×•×œ×‘ ××—×œ×§ ××ª ×”× ×ª×•× ×™× ×œ××¡×¤×¨ "×§×™×¤×•×œ×™×" (folds), ×•××‘×¦×¢ ××™××•×Ÿ ×•× ×™×‘×•×™ ×¢×œ ×›×œ ×§×™×¤×•×œ, ×××¦×¢ ××ª ×”×ª×•×¦××•×ª ×›×“×™ ×œ×§×‘×œ ×”×¢×¨×›×” ××“×•×™×§×ª ×™×•×ª×¨ ×©×œ ×‘×™×¦×•×¢×™ ×”××•×“×œ:

```python
from sklearn.model_selection import cross_val_score

# ×‘×“×™×§×ª ×‘×™×¦×•×¢×™× ×¢× ××™××•×ª ×¦×•×œ×‘ ×¢×‘×•×¨ ×¢×¨×›×™ K ×©×•× ×™×
k_range = range(1, 31)
cv_scores = []

for k in k_range:
    model = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(model, X, y, cv=10, scoring='accuracy')  # 10-fold CV
    # accuracy = (TP + TN) / (TP + TN + FP + FN)
    cv_scores.append(scores.mean())

# ×”×¦×’×ª ×”×ª×•×¦××•×ª ×‘×’×¨×£
plt.figure(figsize=(10, 6))
plt.plot(k_range, cv_scores, marker='o')
plt.title('KNN: CV Accuracy for different K values')
plt.xlabel('K Value')
plt.ylabel('CV Accuracy')
plt.xticks(k_range[::2])
plt.grid(True)
plt.show()

# ××¦×™××ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™
optimal_k_cv = k_range[np.argmax(cv_scores)]
print(f"×¢×¨×š K ×”××•×¤×˜×™××œ×™ (CV) ×”×•×: {optimal_k_cv} ×¢× ×“×™×•×§ ×××•×¦×¢ ×©×œ: {max(cv_scores):.4f}")
```

<img src="knn5.png" style="width:60%;"/>

### GridSearchCV with KNN â€“ Explanation & Example

`GridSearchCV` from Scikit-learn helps you **find the best parameters** for your model  
by searching through all possible combinations (a "grid") of parameters.

#### ğŸ“¦ In the context of KNN:

You might want to try different values for:

- `n_neighbors`: Number of neighbors (K)
- `weights`: 
  - `'uniform'` â€” all neighbors have equal weight  
  - `'distance'` â€” closer neighbors get more weight
- `metric`: 
  - `'euclidean'` â€” standard distance  
  - `'manhattan'` â€” city block distance. It measures the distance between two points by only moving horizontally and vertically, like you would in a city with square blocks
    abs(x1-x2) + abs(y1-y2)

#### âš™ï¸ How it works:

1. You define a **grid of parameters** to test
2. `GridSearchCV` trains the model using **cross-validation** for each combination
3. It evaluates each setup using a scoring metric (e.g., accuracy)
4. It returns the **best parameter combination** based on results

#### ğŸ§  Python Example:

```python
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Example data (X, y)
# Assume X and y are already defined

# Base model
knn = KNeighborsClassifier()

# Grid of parameters to try
param_grid = {
    'n_neighbors': list(range(1, 31)),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

# Grid Search with 5-fold cross-validation
grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid.fit(X, y)

# Show best results
print("Best parameters:", grid.best_params_)
print("Best accuracy:", grid.best_score_)
```



### ×›×™×¦×“ ×œ×‘×—×•×¨ ××ª K ×”××ª××™×?

1. **×’×•×“×œ ×”××“×’×**: ×›×›×œ ×©××“×’× ×”××™××•×Ÿ ×’×“×•×œ ×™×•×ª×¨, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘-K ×’×“×•×œ ×™×•×ª×¨.
   
2. **×¨××ª ×”×¨×¢×© ×‘× ×ª×•× ×™×**: 
   - ×œ× ×ª×•× ×™× ×¢× ××¢×˜ ×¨×¢×©: K ×§×˜×Ÿ ×™×•×ª×¨ (1-5)
   - ×œ× ×ª×•× ×™× ×¢× ×”×¨×‘×” ×¨×¢×©: K ×’×“×•×œ ×™×•×ª×¨ (×œ×”×¤×—×™×ª ××ª ×”×©×¤×¢×ª ×”×¨×¢×©)

3. **××•×¨×›×‘×•×ª ×”×’×‘×•×œ×•×ª ×‘×™×Ÿ ×”××—×œ×§×•×ª**:
   - ×’×‘×•×œ×•×ª ×¤×©×•×˜×™×: K ×’×“×•×œ ×™×•×ª×¨
   - ×’×‘×•×œ×•×ª ××•×¨×›×‘×™×: K ×§×˜×Ÿ ×™×•×ª×¨

4. **×›×œ×œ ××¦×‘×¢**: ×œ×¢×ª×™× ×§×¨×•×‘×•×ª ××•××œ×¥ ×œ×”×ª×—×™×œ ×¢× $K = \sqrt{n}$ ×›××©×¨ n ×”×•× ××¡×¤×¨ ×”×“×•×’×××•×ª ×‘××“×’× ×”××™××•×Ÿ.

5. **×¢×¨×›×™× ××™-×–×•×’×™×™×**: ×¢×‘×•×¨ ×‘×¢×™×•×ª ×¡×™×•×•×’ ×‘×™× ××¨×™, ×›×“××™ ×œ×‘×—×•×¨ ×¢×¨×›×™ K ××™-×–×•×’×™×™× ×›×“×™ ×œ×× ×•×¢ ×ª×™×§×•.

## ×™×™×©×•× ××¢×©×™ - ×©×™××•×© ×‘×›×œ ×”×©×™×˜×•×ª

× ×¡×›× ××ª ×›×œ ××” ×©×œ××“× ×• ×‘×“×•×’××” ××¢×©×™×ª ××§×™×¤×”:

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# ×˜×¢×™× ×ª × ×ª×•× ×™× (×œ×“×•×’××” × ×©×ª××© ×‘-Iris dataset)
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

# ×ª×§× ×•×Ÿ ×”× ×ª×•× ×™×
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ×—×œ×•×§×” ×œ××™××•×Ÿ ×•×‘×“×™×§×”
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)

# ×—×œ×§ 1: ×‘×—×™×¨×ª K ××•×¤×˜×™××œ×™ 
k_range = range(1, 30, 2)  # ×¢×¨×›×™ K ××™-×–×•×’×™×™× ×¢×“ 30

# ×©×™×˜×” 1: Elbow Method
test_scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train)
    test_scores.append(accuracy_score(y_test, knn.predict(X_test)))

# ×©×™×˜×” 2: Cross-Validation
cv_scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_scaled, y, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())

# ×©×™×˜×” 3: GridSearchCV
param_grid = {'n_neighbors': list(k_range)}
grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10)
grid_search.fit(X_scaled, y)

# ×”×¦×’×ª ×ª×•×¦××•×ª ×‘×—×™×¨×ª K
plt.figure(figsize=(12, 6))
plt.plot(k_range, test_scores, marker='o', label='Test Accuracy')
plt.plot(k_range, cv_scores, marker='s', label='CV Accuracy')
plt.axvline(x=k_range[np.argmax(test_scores)], color='r', linestyle='--', 
            label=f'Best K (Test): {k_range[np.argmax(test_scores)]}')
plt.axvline(x=k_range[np.argmax(cv_scores)], color='g', linestyle='--', 
            label=f'Best K (CV): {k_range[np.argmax(cv_scores)]}')
plt.axvline(x=grid_search.best_params_['n_neighbors'], color='m', linestyle='--', 
            label=f"Best K (GridSearch): {grid_search.best_params_['n_neighbors']}")
plt.title('Finding Optimal K Value')
plt.xlabel('K Value')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# ×—×œ×§ 2: ×˜×™×¤×•×œ ×‘×ª×™×§×• - ×©×™××•×© ×‘××©×§×•×œ×•×ª ××¨×—×§
best_k = grid_search.best_params_['n_neighbors']
knn_uniform = KNeighborsClassifier(n_neighbors=best_k, weights='uniform')
knn_distance = KNeighborsClassifier(n_neighbors=best_k, weights='distance')

# × ×“×’×™× ××ª ×”×”×‘×“×œ ×‘×“×™×•×§ ×‘×™×Ÿ ×©× ×™ ×”××•×“×œ×™×
knn_uniform.fit(X_train, y_train)
knn_distance.fit(X_train, y_train)

y_pred_uniform = knn_uniform.predict(X_test)
y_pred_distance = knn_distance.predict(X_test)

print("Uniform Weights Accuracy:", accuracy_score(y_test, y_pred_uniform))
print("Distance Weights Accuracy:", accuracy_score(y_test, y_pred_distance))

# ×—×œ×§ 3: ××“×“×™ ×”×¢×¨×›×” ××¤×•×¨×˜×™×
print("\nClassification Report (Distance Weights):")
print(classification_report(y_test, y_pred_distance, target_names=target_names))

# ×”×¦×’×ª ××˜×¨×™×¦×ª ×”×‘×œ×‘×•×œ
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_distance)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=target_names, 
            yticklabels=target_names)
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# ×—×œ×§ 4: ×•×™×–×•××œ×™×–×¦×™×” ×©×œ ×”×—×œ×˜×•×ª ×”×¡×™×•×•×’
# ×¨×§ ×¢× ×©× ×™ ×××“×™× ×œ×¦×•×¨×š ×”×•×™×–×•××œ×™×–×¦×™×” (2 ×××¤×™×™× ×™× ×¨××©×•× ×™×)
X_2d = X_scaled[:, :2]
y_2d = y

# ×—×œ×•×§×” ×œ××™××•×Ÿ ×•×‘×“×™×§×”
X_2d_train, X_2d_test, y_2d_train, y_2d_test = train_test_split(X_2d, y_2d, test_size=0.3, random_state=42)

# ××™××•×Ÿ ×”××•×“×œ
knn_2d = KNeighborsClassifier(n_neighbors=best_k, weights='distance')
knn_2d.fit(X_2d_train, y_2d_train)

# ×™×¦×™×¨×ª ×¨×©×ª ×œ×•×™×–×•××œ×™×–×¦×™×”
h = 0.02  # ×’×•×“×œ ×¦×¢×“ ×‘×’×¨×™×“
x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

# ×—×™×–×•×™ ×¢×‘×•×¨ ×›×œ × ×§×•×“×” ×‘×¨×©×ª
Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# ×”×¦×’×ª ××–×•×¨×™ ×”×”×—×œ×˜×”
plt.figure(figsize=(10, 8))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)

# ×”×¦×’×ª × ×§×•×“×•×ª ×”××™××•×Ÿ
scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, edgecolors='k', 
                    s=80, cmap=plt.cm.coolwarm)

plt.title(f'KNN Decision Boundaries with K={best_k}')
plt.xlabel(feature_names[0])
plt.ylabel(feature_names[1])
plt.legend(*scatter.legend_elements(), title="Classes", loc="upper right")
plt.show()

# ×—×œ×§ 5: ×”×“×’××ª ×˜×™×¤×•×œ ×‘×ª×™×§×• - ××¦×™××ª × ×§×•×“×•×ª ×¢× ×ª×™×§×• ×¤×•×˜× ×¦×™××œ×™
def find_potential_ties(X, y, k):
    """××¦×™××ª × ×§×•×“×•×ª ×¢× ×ª×™×§×• ×¤×•×˜× ×¦×™××œ×™ - ×›×œ×•××¨ × ×§×•×“×•×ª ×©×× ×”×™×™× ×• ××¡×•×•×’×™× ××•×ª×Ÿ ×¢× K ×©×›× ×™×
    ×”×™×” × ×•×¦×¨ ×ª×™×§×• ×‘×™×Ÿ ×©×ª×™ ×§×˜×’×•×¨×™×•×ª ××• ×™×•×ª×¨"""
    potential_ties = []
    
    for i, x in enumerate(X):
        # ×—×™×©×•×‘ ××¨×—×§×™× ××›×œ ×”× ×§×•×“×•×ª ×”××—×¨×•×ª
        distances = []
        for j, other_x in enumerate(X):
            if i != j:  # ×œ× ×›×•×œ×œ ××ª ×”× ×§×•×“×” ×¢×¦××”
                dist = np.sqrt(np.sum((x - other_x) ** 2))
                distances.append((dist, j))
        
        # ××™×•×Ÿ ×”××¨×—×§×™× ×•×‘×—×™×¨×ª K ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨
        distances.sort()
        k_nearest = distances[:k]
        
        # ×¡×¤×™×¨×ª ×”×§×˜×’×•×¨×™×•×ª
        class_counts = {}
        for _, idx in k_nearest:
            label = y[idx]
            if label in class_counts:
                class_counts[label] += 1
            else:
                class_counts[label] = 1
        
        # ×‘×“×™×§×” ×× ×™×© ×ª×™×§×• (×©×ª×™ ×§×˜×’×•×¨×™×•×ª ××• ×™×•×ª×¨ ×¢× ××•×ª×• ××¡×¤×¨ ×©×›× ×™×)
        values = list(class_counts.values())
        if len(values) > 1 and values.count(max(values)) > 1:
            potential_ties.append((i, class_counts))
    
    return potential_ties

# ××¦×™××ª × ×§×•×“×•×ª ×¢× ×ª×™×§×• ×¤×•×˜× ×¦×™××œ×™ ×¢×œ ×§×‘×•×¦×ª ×”××™××•×Ÿ
k_to_check = 4  # K ×–×•×’×™, ×™×•×ª×¨ ×¡×™×›×•×™ ×œ×ª×™×§×•
potential_ties = find_potential_ties(X_2d, y_2d, k_to_check)

if potential_ties:
    print(f"\n××¦×× ×• {len(potential_ties)} × ×§×•×“×•×ª ×¢× ×ª×™×§×• ×¤×•×˜× ×¦×™××œ×™ ×›××©×¨ K={k_to_check}")
    
    # ××“×’×™× ××ª 3 ×”× ×§×•×“×•×ª ×”×¨××©×•× ×•×ª ×¢× ×ª×™×§×•
    for i, (idx, counts) in enumerate(potential_ties[:3]):
        print(f"× ×§×•×“×” {idx}: {dict(counts)}")
        
        # ×•×™×–×•××œ×™×–×¦×™×” ×©×œ × ×§×•×“×” ×¢× ×ª×™×§×•
        plt.figure(figsize=(8, 6))
        
        # ×”×¦×’×ª ×›×œ ×”× ×§×•×“×•×ª
        plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.coolwarm, alpha=0.3)
        
        # ×”×“×’×©×ª ×”× ×§×•×“×” ×¢× ×”×ª×™×§×•
        plt.scatter(X_2d[idx, 0], X_2d[idx, 1], color='black', s=100, marker='*')
        
        # ××¦×™××ª ×”×©×›× ×™× ×”×§×¨×•×‘×™×
        distances = []
        for j, other_x in enumerate(X_2d):
            if idx != j:
                dist = np.sqrt(np.sum((X_2d[idx] - other_x) ** 2))
                distances.append((dist, j))
        
        distances.sort()
        k_nearest = distances[:k_to_check]
        
        # ×”×“×’×©×ª ×”×©×›× ×™× ×”×§×¨×•×‘×™× ×‘×™×•×ª×¨
        for dist, neighbor_idx in k_nearest:
            plt.scatter(X_2d[neighbor_idx, 0], X_2d[neighbor_idx, 1], 
                      color='red', s=80, alpha=0.6, edgecolors='k')
            
            # ×¦×™×•×¨ ×§×• ××”× ×§×•×“×” ×œ×©×›×Ÿ
            plt.plot([X_2d[idx, 0], X_2d[neighbor_idx, 0]], 
                   [X_2d[idx, 1], X_2d[neighbor_idx, 1]], 'k--', alpha=0.3)
        
        # ×”×•×¡×¤×ª ××¢×’×œ ×”××“×’×™×© ××ª ××–×•×¨ ×”×©×›× ×™× ×”×§×¨×•×‘×™×
        max_dist = k_nearest[-1][0]
        circle = plt.Circle((X_2d[idx, 0], X_2d[idx, 1]), max_dist, 
                          fill=False, color='blue', linestyle='-')
        plt.gca().add_patch(circle)
        
        plt.title(f'Tie Example {i+1}: Point {idx} with K={k_to_check}')
        plt.xlabel(feature_names[0])
        plt.ylabel(feature_names[1])
        plt.show()
else:
    print(f"×œ× × ××¦××• × ×§×•×“×•×ª ×¢× ×ª×™×§×• ×¤×•×˜× ×¦×™××œ×™ ×›××©×¨ K={k_to_check}")
```

## ×¡×™×›×•×

×‘×—×™×¨×ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™ ×”×™× ××¤×ª×— ×œ×”×¦×œ×—×ª ××œ×’×•×¨×™×ª× KNN:

1. **×©×™×˜×•×ª ××•××œ×¦×•×ª ×œ×‘×—×™×¨×ª K**:
   - ×¢×‘×•×¨ ××“×’××™× ×§×˜× ×™×: ××™××•×ª ×¦×•×œ×‘ (cross-validation)
   - ×¢×‘×•×¨ ××“×’××™× ×’×“×•×œ×™×: ×©×™×˜×ª Elbow ××• GridSearchCV
   - ×©×™××•×© ×‘-K ××™-×–×•×’×™ ×œ×× ×™×¢×ª ×ª×™×§×• ×‘×‘×¢×™×•×ª ×‘×™× ××¨×™×•×ª

2. **×˜×™×¤×•×œ ×‘××§×¨×™ ×ª×™×§×•**:
   - ×©×™××•×© ×‘××©×§×•×œ×•×ª ××¨×—×§ (`weights='distance'`)
   - ×‘×—×™×¨×ª K ××™-×–×•×’×™
   - ×”×¤×—×ª×ª K ××• ×”×’×“×œ×ª×• ×‘××§×¨×” ×”×¦×•×¨×š

3. **×˜×™×¤×™× ×œ×©×™×¤×•×¨ ×‘×™×¦×•×¢×™ KNN**:
   - ×ª×§× ×•×Ÿ × ×ª×•× ×™× ×”×•× ×§×¨×™×˜×™!
   - ×¡×™×œ×•×§ ×××¤×™×™× ×™× ×œ× ×¨×œ×•×•× ×˜×™×™× (feature selection)
   - ×©×§×™×œ×ª ×©×™××•×© ×‘×©×™×˜×•×ª ×”×¤×—×ª×ª ×××“×™× (×›××• PCA)
   - ×”×ª×××ª ××“×“ ××¨×—×§ ×œ×¡×•×’ ×”× ×ª×•× ×™× (×× ×”×˜×Ÿ, ×™×•×§×œ×™×“×™, ××™× ×§×•×‘×¡×§×™)

## ×ª×¨×’×™×œ

**×ª×¨×’×™×œ**: ×—×‘×¨×ª ××©×¨××™ ××©×ª××©×ª ×‘××œ×’×•×¨×™×ª× KNN ×œ× ×™×‘×•×™ ×¡×™×›×•×Ÿ ×”×œ×•×•××•×ª. ×”× ×ª×•× ×™× ×›×•×œ×œ×™× 100 ×œ×§×•×—×•×ª. ×‘× ×™×¡×™×•×Ÿ ×œ×§×‘×•×¢ ××ª ×¢×¨×š K ×”××•×¤×˜×™××œ×™, ×”×ª×§×‘×œ×• ×”×ª×•×¦××•×ª ×”×‘××•×ª:

| K | ×“×™×•×§ (Accuracy) | Precision | Recall | F1 Score |
|---|----------------|-----------|--------|----------|
| 1 | 0.82           | 0.75      | 0
