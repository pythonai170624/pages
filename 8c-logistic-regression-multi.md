# ğŸŒŸ ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×¢× ×›××” ××©×ª× ×™× (Multivariable Logistic Regression)

## ğŸ“˜ ××” ×–×” ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª?
×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×”×™× ×©×™×˜×” ×¡×˜×˜×™×¡×˜×™×ª ×œ×—×™×–×•×™ ××©×ª× ×” ×ª×œ×•×™ ×‘×™× ××¨×™ ××• ×¨×‘Ö¾×§×˜×’×•×¨×™ (×›××•: ×›×Ÿ/×œ×, ×¡×•×’ ×¤×¨×—), ×¢×œ ×¡××š ×¢×¨×›×™× ×©×œ ××©×ª× ×™× ××¡×‘×™×¨×™× (×ª×›×•× ×•×ª).

×‘××§×¨×” ×©×œ ××¡×¤×¨ ××©×ª× ×™× ××¡×‘×™×¨×™× (features), ××“×•×‘×¨ ×‘×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×¢× **×›××” × ×¢×œ××™×**.

---

## âœï¸ ××•×©×’×™× ×‘×¡×™×¡×™×™×:

- **××©×ª× ×” ×ª×œ×•×™ (Y):** ××” ×©×× ×—× ×• ×× ×¡×™× ×œ×—×–×•×ª (×œ××©×œ ×¡×•×’ ×¤×¨×—)
- **××©×ª× ×™× ×‘×œ×ª×™ ×ª×œ×•×™×™× (Xâ‚, Xâ‚‚, Xâ‚ƒ...):** ×”×ª×›×•× ×•×ª ×©××¡×‘×™×¨×•×ª ××ª Y (×œ××©×œ ××•×¨×š ×¢×œ×™ ×›×•×ª×¨×ª)
- **Î² (×‘×˜×):** ××§×“××™× ×©× ×œ××“×™× ×¢×œ ×™×“×™ ×”××•×“×œ. ×›×œ ××©×ª× ×” ××§×‘×œ Î² ××©×œ×•.

---

## ğŸ“ × ×•×¡×—×” ××ª××˜×™×ª (Softmax Multiclass):

×× ×™×© ×œ× ×• \( K \) ×§×˜×’×•×¨×™×•×ª ×•Ö¾\( n \) ××©×ª× ×™×:

$$
P(y = k \mid x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \beta_{k2}x_2 + \dots + \beta_{kn}x_n}}{\sum_{j=1}^K e^{\beta_{j0} + \beta_{j1}x_1 + \beta_{j2}x_2 + \dots + \beta_{jn}x_n}}
$$

- ×–×•×”×™ ×¤×•× ×§×¦×™×™×ª **Softmax** â€“ ×××™×¨×” ××ª ×”×¦×™×•× ×™× ×œ×”×¡×ª×‘×¨×•×™×•×ª.
- ×›×œ ×§×˜×’×•×¨×™×” ××§×‘×œ×ª ×”×¡×ª×‘×¨×•×ª, ×•×”×—×–×•×™×” ×”×™× ×–×• ×¢× ×”×”×¡×ª×‘×¨×•×ª ×”×’×‘×•×”×” ×‘×™×•×ª×¨.

---

## ğŸŒ¸ ×“×•×’××” â€“ ×¢×œ ×‘×¡×™×¡ × ×ª×•× ×™ Iris:

<img src="iris.jpg" style="width: 60%" />


× × ×¡×” ×œ×—×–×•×ª ××ª ×¡×•×’ ×”×¤×¨×— (setosa, versicolor, virginica) ×¢×œ ×¡××š 4 ××©×ª× ×™×:
- sepal length (Xâ‚)
- sepal width (Xâ‚‚)
- petal length (Xâ‚ƒ)
- petal width (Xâ‚„)

×›×œ ×¡×•×’ ×¤×¨×— ×™×§×‘×œ ××§×“××™× ××©×œ×•:

×œ××©×œ:

$$
P(y = \text{setosa}) = \frac{e^{\beta_{0} + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4}}{\text{sum of all classes}}
$$

### ×œ×•×’×™×¡×˜×™×ª ××¨×•×‘×ª ××©×ª× ×™× ×¢× softmax

### ğŸ¯ ××˜×¨×”:
×œ×—×–×•×ª ××™×–×” ×¤×¨×— ×–×” (Setosa / Versicolor / Virginica) ×œ×¤×™ ×”×ª×›×•× ×•×ª ×”×‘××•×ª:

- `xâ‚` = sepal length = 5.0  
- `xâ‚‚` = sepal width = 3.0  
- `xâ‚ƒ` = petal length = 1.5  
- `xâ‚„` = petal width = 0.2  

### ğŸ§  × × ×™×— ××ª ×”××§×“××™× ×”×‘××™×:

| Class       | Î²â‚€ (bias) | Î²â‚  | Î²â‚‚  | Î²â‚ƒ   | Î²â‚„   |
|-------------|-----------|-----|-----|------|------|
| Setosa      | 2.0       | 1.0 | 0.5 | -0.5 | -1.0 |
| Versicolor  | 1.0       | 0.5 | 0.2 |  0.1 |  0.3 |
| Virginica   | -0.5      | 0.3 | 0.1 |  0.8 |  0.9 |

### âœï¸ × ×—×©×‘ ××ª ×”×¦×™×•× ×™× (logits):

$$
z_{setosa} = 2.0 + 1.0 \cdot 5.0 + 0.5 \cdot 3.0 - 0.5 \cdot 1.5 - 1.0 \cdot 0.2 = 7.55
$$

$$
z_{versicolor} = 1.0 + 0.5 \cdot 5.0 + 0.2 \cdot 3.0 + 0.1 \cdot 1.5 + 0.3 \cdot 0.2 = 4.31
$$

$$
z_{virginica} = -0.5 + 0.3 \cdot 5.0 + 0.1 \cdot 3.0 + 0.8 \cdot 1.5 + 0.9 \cdot 0.2 = 2.68
$$

### ğŸ§® ××—×©×‘×™× softmax:

$$
P(k) = \frac{e^{z_k}}{\sum_j e^{z_j}}
$$

$$
e^{7.55} â‰ˆ 1902.56
$$

$$ 
e^{4.31} â‰ˆ 74.46 
$$

$$ 
e^{2.68} â‰ˆ 14.61 
$$
- ×¡×š ×”×›×•×œ: \( 1991.63 \)


**×”×¡×ª×‘×¨×•×™×•×ª ×œ×›×œ ×¤×¨×—:**

Setosa

$$
\frac{1902.56}{1991.63} = **0.955**  
$$

Versicolor

$$
\frac{74.46}{1991.63} = 0.037
$$

Virginica

$$
\frac{14.61}{1991.63} = 0.007
$$

### âœ… ××¡×§× ×”:

> ×”××•×“×œ ×—×•×–×” ×©×”×¤×¨×— ×”×•× **Setosa** ×¢× ×”×¡×ª×‘×¨×•×ª ×©×œ **95.5%**  
> ×•×œ×›×Ÿ ×–×” ××” ×©×ª×—×–×™×¨ ×”×¤×•× ×§×¦×™×” `predict` ×©×œ ×”××•×“×œ ×¢×‘×•×¨ ×”×¤×¨××˜×¨×™× ×”× ×•×›×—×™×™×


---

## ğŸ§ª ×§×•×“ ×¤×™×™×ª×•×Ÿ ×œ×“×•×’××”:

```python
import pandas as pd
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

# Load dataset
df = pd.read_csv("iris.csv")
X = df.drop(columns="species")  # Features
y = df["species"]               # Target (3 flower types)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train logistic regression with multinomial softmax
model = LogisticRegressionCV(
    solver='lbfgs',           # Efficient for multiclass problems
    multi_class='multinomial',# Uses softmax across all classes
    cv=5,
    max_iter=500
)
model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
