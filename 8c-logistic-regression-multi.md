# ðŸŒŸ ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×¢× ×›×ž×” ×ž×©×ª× ×™× (Multivariable Logistic Regression)

## ðŸ“˜ ×ž×” ×–×” ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª?
×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×”×™× ×©×™×˜×” ×¡×˜×˜×™×¡×˜×™×ª ×œ×—×™×–×•×™ ×ž×©×ª× ×” ×ª×œ×•×™ ×‘×™× ××¨×™ ××• ×¨×‘Ö¾×§×˜×’×•×¨×™ (×›×ž×•: ×›×Ÿ/×œ×, ×¡×•×’ ×¤×¨×—), ×¢×œ ×¡×ž×š ×¢×¨×›×™× ×©×œ ×ž×©×ª× ×™× ×ž×¡×‘×™×¨×™× (×ª×›×•× ×•×ª).

×‘×ž×§×¨×” ×©×œ ×ž×¡×¤×¨ ×ž×©×ª× ×™× ×ž×¡×‘×™×¨×™× (features), ×ž×“×•×‘×¨ ×‘×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×¢× **×›×ž×” × ×¢×œ×ž×™×**.

---

## âœï¸ ×ž×•×©×’×™× ×‘×¡×™×¡×™×™×:

- **×ž×©×ª× ×” ×ª×œ×•×™ (Y):** ×ž×” ×©×× ×—× ×• ×ž× ×¡×™× ×œ×—×–×•×ª (×œ×ž×©×œ ×¡×•×’ ×¤×¨×—)
- **×ž×©×ª× ×™× ×‘×œ×ª×™ ×ª×œ×•×™×™× (Xâ‚, Xâ‚‚, Xâ‚ƒ...):** ×”×ª×›×•× ×•×ª ×©×ž×¡×‘×™×¨×•×ª ××ª Y (×œ×ž×©×œ ××•×¨×š ×¢×œ×™ ×›×•×ª×¨×ª)
- **Î² (×‘×˜×):** ×ž×§×“×ž×™× ×©× ×œ×ž×“×™× ×¢×œ ×™×“×™ ×”×ž×•×“×œ. ×›×œ ×ž×©×ª× ×” ×ž×§×‘×œ Î² ×ž×©×œ×•.

---

## ðŸ“ × ×•×¡×—×” ×ž×ª×ž×˜×™×ª (Softmax Multiclass):

×× ×™×© ×œ× ×• \( K \) ×§×˜×’×•×¨×™×•×ª ×•Ö¾\( n \) ×ž×©×ª× ×™×:

$$
P(y = k \mid x) = \frac{e^{\beta_{k0} + \beta_{k1}x_1 + \beta_{k2}x_2 + \dots + \beta_{kn}x_n}}{\sum_{j=1}^K e^{\beta_{j0} + \beta_{j1}x_1 + \beta_{j2}x_2 + \dots + \beta_{jn}x_n}}
$$

- ×–×•×”×™ ×¤×•× ×§×¦×™×™×ª **Softmax** â€“ ×ž×ž×™×¨×” ××ª ×”×¦×™×•× ×™× ×œ×”×¡×ª×‘×¨×•×™×•×ª.
- ×›×œ ×§×˜×’×•×¨×™×” ×ž×§×‘×œ×ª ×”×¡×ª×‘×¨×•×ª, ×•×”×—×–×•×™×” ×”×™× ×–×• ×¢× ×”×”×¡×ª×‘×¨×•×ª ×”×’×‘×•×”×” ×‘×™×•×ª×¨.

---

## ðŸŒ¸ ×“×•×’×ž×” â€“ ×¢×œ ×‘×¡×™×¡ × ×ª×•× ×™ Iris:

× × ×¡×” ×œ×—×–×•×ª ××ª ×¡×•×’ ×”×¤×¨×— (setosa, versicolor, virginica) ×¢×œ ×¡×ž×š 4 ×ž×©×ª× ×™×:
- sepal length (Xâ‚)
- sepal width (Xâ‚‚)
- petal length (Xâ‚ƒ)
- petal width (Xâ‚„)

×›×œ ×¡×•×’ ×¤×¨×— ×™×§×‘×œ ×ž×§×“×ž×™× ×ž×©×œ×•:

×œ×ž×©×œ:

$$
P(y = \text{setosa}) = \frac{e^{\beta_{0} + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4}}{\text{sum of all classes}}
$$

# ðŸŒ¸ ×“×•×’×ž×”: ×¨×’×¨×¡×™×” ×œ×•×’×™×¡×˜×™×ª ×ž×¨×•×‘×ª ×ž×©×ª× ×™× ×¢× softmax

## ðŸŽ¯ ×ž×˜×¨×”:
×œ×—×–×•×ª ××™×–×” ×¤×¨×— ×–×” (Setosa / Versicolor / Virginica) ×œ×¤×™ ×”×ª×›×•× ×•×ª ×”×‘××•×ª:

- `xâ‚` = sepal length = 5.0  
- `xâ‚‚` = sepal width = 3.0  
- `xâ‚ƒ` = petal length = 1.5  
- `xâ‚„` = petal width = 0.2  

---

## ðŸ§  × × ×™×— ××ª ×”×ž×§×“×ž×™× ×”×‘××™×:

| Class       | Î²â‚€ (bias) | Î²â‚  | Î²â‚‚  | Î²â‚ƒ   | Î²â‚„   |
|-------------|-----------|-----|-----|------|------|
| Setosa      | 2.0       | 1.0 | 0.5 | -0.5 | -1.0 |
| Versicolor  | 1.0       | 0.5 | 0.2 |  0.1 |  0.3 |
| Virginica   | -0.5      | 0.3 | 0.1 |  0.8 |  0.9 |

---

## âœï¸ × ×—×©×‘ ××ª ×”×¦×™×•× ×™× (logits):

$$
z_{setosa} = 2.0 + 1.0 \cdot 5.0 + 0.5 \cdot 3.0 - 0.5 \cdot 1.5 - 1.0 \cdot 0.2 = 7.55
$$

$$
z_{versicolor} = 1.0 + 0.5 \cdot 5.0 + 0.2 \cdot 3.0 + 0.1 \cdot 1.5 + 0.3 \cdot 0.2 = 4.31
$$

$$
z_{virginica} = -0.5 + 0.3 \cdot 5.0 + 0.1 \cdot 3.0 + 0.8 \cdot 1.5 + 0.9 \cdot 0.2 = 2.68
$$

---

## ðŸ§® ×ž×—×©×‘×™× softmax:

$$
P(k) = \frac{e^{z_k}}{\sum_j e^{z_j}}
$$

$$
e^{7.55} â‰ˆ 1902.56
$$

$$ 
e^{4.31} â‰ˆ 74.46 
$$

$$ 
e^{2.68} â‰ˆ 14.61 
$$
- ×¡×š ×”×›×•×œ: \( 1991.63 \)


**×”×¡×ª×‘×¨×•×™×•×ª ×œ×›×œ ×¤×¨×—:**

Setosa
$$
\frac{1902.56}{1991.63} = **0.955**  
$$

Versicolor
$$
\frac{74.46}{1991.63} = 0.037
$$

Virginica
$$
\frac{14.61}{1991.63} = 0.007
$$

---

## âœ… ×ž×¡×§× ×”:

> ×”×ž×•×“×œ ×—×•×–×” ×©×”×¤×¨×— ×”×•× **Setosa** ×¢× ×”×¡×ª×‘×¨×•×ª ×©×œ **95.5%**  
> ×•×œ×›×Ÿ ×–×” ×ž×” ×©×ª×—×–×™×¨ ×”×¤×•× ×§×¦×™×” `predict()` ×©×œ ×”×ž×•×“×œ.


---

## ðŸ§ª ×§×•×“ ×¤×™×™×ª×•×Ÿ ×œ×“×•×’×ž×”:

```python
import pandas as pd
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

# Load dataset
df = pd.read_csv("iris.csv")
X = df.drop(columns="species")  # Features
y = df["species"]               # Target (3 flower types)

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train logistic regression with multinomial softmax
model = LogisticRegressionCV(
    solver='lbfgs',           # Efficient for multiclass problems
    multi_class='multinomial',# Uses softmax across all classes
    cv=5,
    max_iter=500
)
model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_scaled)
print(classification_report(y_test, y_pred))
