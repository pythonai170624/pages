# רגרסיה לוגיסטית - מציאת המקדמים β₀ ו-β₁

## הנתונים שלנו

נתחיל עם טבלת הנתונים הבאה המציגה את הקשר בין שעות לימוד ותוצאות מבחן (עבר/נכשל):

| שעות לימוד | תוצאה (1=עבר, 0=נכשל) |
|------------|----------------------|
| 1          | 0                    |
| 2          | 0                    |
| 3          | 0                    |
| 4          | 0                    |
| 5          | 1                    |
| 6          | 1                    |
| 7          | 1                    |
| 8          | 1                    |
| 9          | 1                    |

אנו רוצים למצוא מודל שיחזה את ההסתברות שתלמיד יעבור את המבחן בהתבסס על שעות הלימוד שלו.

## המודל הלוגיסטי

המודל הלוגיסטי מוגדר כך:

$$P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}$$

כאשר:
- $P(Y=1|X)$ היא ההסתברות שהתוצאה תהיה 1 (עבר) בהינתן X (שעות לימוד)
- $\beta_0$ הוא קבוע החיתוך (intercept)
- $\beta_1$ הוא המקדם עבור המשתנה הבלתי תלוי X
- $e$ הוא בסיס הלוגריתם הטבעי (בערך 2.71828)

## Maximum Likelihood Estimation (MLE)

כדי למצוא את הערכים של $\beta_0$ ו-$\beta_1$, נשתמש בשיטת Maximum Likelihood Estimation. בשיטה זו, אנו מחפשים את הערכים של $\beta_0$ ו-$\beta_1$ שממקסמים את ההסתברות לצפות בנתונים שלנו.

### שלב 1: הגדרת פונקציית הנראות (Likelihood Function)

עבור כל תצפית $i$ בנתונים שלנו, ההסתברות לתוצאה $y_i$ בהינתן $x_i$ היא:

$$P(Y_i=y_i|X_i=x_i) = 
\begin{cases}
P(Y_i=1|X_i=x_i) & \text{אם } y_i = 1 \\
1 - P(Y_i=1|X_i=x_i) & \text{אם } y_i = 0
\end{cases}$$

אפשר לכתוב זאת בצורה קומפקטית יותר:

$$P(Y_i=y_i|X_i=x_i) = P(Y_i=1|X_i=x_i)^{y_i} \times (1-P(Y_i=1|X_i=x_i))^{1-y_i}$$

פונקציית הנראות היא המכפלה של ההסתברויות לכל התצפיות:

$$L(\beta_0, \beta_1) = \prod_{i=1}^{n} P(Y_i=1|X_i=x_i)^{y_i} \times (1-P(Y_i=1|X_i=x_i))^{1-y_i}$$

### שלב 2: המרה ללוג-נראות (Log-Likelihood)

כדי להקל על החישובים, נשתמש בלוג-נראות:

$$\ln L(\beta_0, \beta_1) = \sum_{i=1}^{n} [y_i \ln(P(Y_i=1|X_i=x_i)) + (1-y_i) \ln(1-P(Y_i=1|X_i=x_i))]$$

נסמן $p_i = P(Y_i=1|X_i=x_i) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_i)}}$ ואז נקבל:

$$\ln L(\beta_0, \beta_1) = \sum_{i=1}^{n} [y_i \ln(p_i) + (1-y_i) \ln(1-p_i)]$$

### שלב 3: גזירה והשוואה לאפס

כדי למצוא את הערכים של $\beta_0$ ו-$\beta_1$ שממקסמים את פונקציית הלוג-נראות, נגזור אותה ביחס ל-$\beta_0$ ו-$\beta_1$ ונשווה לאפס:

$$\frac{\partial \ln L}{\partial \beta_0} = \sum_{i=1}^{n} [y_i - p_i] = 0$$

$$\frac{\partial \ln L}{\partial \beta_1} = \sum_{i=1}^{n} [x_i(y_i - p_i)] = 0$$

המשוואות הללו אינן ניתנות לפתרון אנליטי ישיר, ולכן נדרשות שיטות איטרטיביות.

## יישום מעשי: איך למצוא את β₀ ו-β₁ בפייתון

### גישה 1: שימוש בספריית scikit-learn

הדרך הפשוטה ביותר היא להשתמש בספריית scikit-learn:

```python
import numpy as np
from sklearn.linear_model import LogisticRegression

# הנתונים שלנו
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)  # שעות לימוד
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])  # תוצאות (0=נכשל, 1=עבר)

# יצירת מודל רגרסיה לוגיסטית
model = LogisticRegression(solver='lbfgs', random_state=42)
model.fit(X, y)

# הדפסת המקדמים
print(f"β₀ (Intercept): {model.intercept_[0]:.4f}")
print(f"β₁ (Coefficient): {model.coef_[0][0]:.4f}")
```

התוצאה של קוד זה תהיה בערך:
```
β₀ (Intercept): -5.6143
β₁ (Coefficient): 1.2832
```

### גישה 2: מימוש ידני של Maximum Likelihood Estimation

נוכל גם לממש את שיטת ה-MLE באופן ידני באמצעות פונקציות אופטימיזציה:

```python
import numpy as np
from scipy import optimize

# הנתונים שלנו
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])

# פונקציית הסיגמואיד
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# פונקציית לוג-נראות שלילית (כי אנחנו רוצים למזער)
def negative_log_likelihood(params):
    beta0, beta1 = params
    p = sigmoid(beta0 + beta1 * X)
    return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))

# מציאת המקדמים באמצעות מזעור הלוג-נראות השלילית
initial_guess = [0, 0]  # ניחוש התחלתי
result = optimize.minimize(negative_log_likelihood, initial_guess, method='BFGS')

# הדפסת התוצאות
beta0, beta1 = result.x
print(f"β₀ (Intercept): {beta0:.4f}")
print(f"β₁ (Coefficient): {beta1:.4f}")
```

התוצאה של קוד זה תהיה בערך:
```
β₀ (Intercept): -5.6155
β₁ (Coefficient): 1.2836
```

### גישה 3: שימוש ב-statsmodels לקבלת סטטיסטיקות מפורטות יותר

הספרייה statsmodels מספקת מידע סטטיסטי מפורט יותר:

```python
import numpy as np
import statsmodels.api as sm

# הנתונים שלנו
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])

# הוספת עמודת אחדות ל-X עבור קבוע החיתוך
X_with_constant = sm.add_constant(X)

# יצירת מודל רגרסיה לוגיסטית והתאמתו לנתונים
model = sm.Logit(y, X_with_constant)
result = model.fit()

# הדפסת סיכום מפורט
print(result.summary())

# הדפסת המקדמים
print(f"β₀ (Intercept): {result.params[0]:.4f}")
print(f"β₁ (Coefficient): {result.params[1]:.4f}")
```

התוצאה תכלול סיכום מפורט עם המקדמים, טעויות תקן, ערכי p, ועוד.

## הסבר על האלגוריתמים

האלגוריתמים לפתרון בעיית ה-MLE הם:

1. **Newton-Raphson**: משתמש בגזירה ראשונה ושנייה (הסיאן) של פונקציית הלוג-נראות. מתכנס מהר אבל דורש חישוב הסיאן בכל צעד.

2. **BFGS (Broyden-Fletcher-Goldfarb-Shanno)**: שיטת אופטימיזציה קוואזי-ניוטונית שמקרבת את ההסיאן במקום לחשב אותו ישירות. 

3. **L-BFGS**: גרסה מוגבלת-זיכרון של BFGS שמתאימה לבעיות גדולות.

4. **IRLS (Iteratively Reweighted Least Squares)**: שיטה ייחודית לרגרסיה לוגיסטית שממירה את בעיית ה-MLE לסדרה של בעיות ריבועים פחותים ממושקלים.

## חישוב התאמת המודל והסבר המשמעות

### התאמת המודל לנתונים

אחרי שמצאנו את המקדמים β₀ = -5.6143 ו-β₁ = 1.2832, נוכל לכתוב את משוואת הרגרסיה הלוגיסטית:

$$P(\text{pass}) = \frac{1}{1 + e^{-(-5.6143 + 1.2832 \times \text{hours})}}$$

### פירוש המקדמים

- **β₀ (Intercept)**: מייצג את הלוגריתם של יחס הסיכויים כאשר X=0 (כלומר, אפס שעות לימוד). המשמעות הישירה לרוב פחות חשובה מבחינה פרקטית.

- **β₁ (Coefficient)**: מייצג את השינוי בלוגריתם של יחס הסיכויים כאשר X עולה ביחידה אחת. בדוגמה שלנו, כל שעת לימוד נוספת מגדילה את הלוג-יחס הסיכויים ב-1.2832.

אם נחשב $e^{1.2832} \approx 3.61$, זה אומר שכל שעת לימוד נוספת מכפילה את יחס הסיכויים (odds) להצלחה פי 3.61 בערך.

### גבול ההחלטה

כדי למצוא את גבול ההחלטה (כלומר, כמה שעות לימוד נדרשות כדי שההסתברות להצלחה תהיה 0.5), נפתור:

$$0.5 = \frac{1}{1 + e^{-(-5.6143 + 1.2832 \times \text{hours})}}$$

$$1 + e^{-(-5.6143 + 1.2832 \times \text{hours})} = 2$$

$$e^{-(-5.6143 + 1.2832 \times \text{hours})} = 1$$

$$-5.6143 + 1.2832 \times \text{hours} = 0$$

$$\text{hours} = \frac{5.6143}{1.2832} \approx 4.38$$

לפי המודל שלנו, תלמיד צריך ללמוד לפחות 4.38 שעות כדי שהסיכוי שלו לעבור את המבחן יהיה גדול מ-50%.

## סיכום

מציאת המקדמים β₀ ו-β₁ ברגרסיה לוגיסטית מתבצעת באמצעות שיטת Maximum Likelihood Estimation. בניגוד לרגרסיה לינארית, אין פתרון סגור לשיטה זו, ולכן משתמשים באלגוריתמים איטרטיביים כמו Newton-Raphson או BFGS.

בפרקטיקה, ספריות כמו scikit-learn, scipy, או statsmodels מספקות פונקציות מוכנות למציאת המקדמים. המקדמים שמצאנו (β₀ = -5.6143, β₁ = 1.2832) מציינים את נקודת החיתוך ואת השיפוע של פונקציית הלוגיט, וניתן להשתמש בהם לחישוב ההסתברות להצלחה עבור ערכי קלט שונים.
