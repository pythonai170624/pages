# ×”×’×“×¨×•×ª SVM, Kernel, Kernel Function, ×•-Kernel Trick

## Support Vector Machine (SVM)
SVM ×”×•× ××œ×’×•×¨×™×ª× ×œ××™×“×” ××•× ×—×™×ª (supervised learning) ×”××©××© ×œ×¡×™×•×•×’ (classification) ×•×¨×’×¨×¡×™×” (regression). ×”××œ×’×•×¨×™×ª× ××—×¤×© ×”×™×¤×¨×¤×œ×Ÿ ××•×¤×˜×™××œ×™ (××™×©×•×¨ ×”×¤×¨×“×”) ×©×™×¤×¨×™×“ ×‘×™×Ÿ ×§×‘×•×¦×•×ª ×©×•× ×•×ª ×©×œ × ×ª×•× ×™×. ×”×™×¤×¨×¤×œ×Ÿ ××•×¤×˜×™××œ×™ ×”×•× ×–×” ×©×™×•×¦×¨ ××ª ×”××¨×•×•×— (margin) ×”××¨×‘×™ ×‘×™×Ÿ ×”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ××›×œ ×§×‘×•×¦×”, ×”×™×“×•×¢×•×ª ×›×•×•×§×˜×•×¨×™ ×ª××™×›×” (support vectors).

××ª××˜×™×ª, ×¢×‘×•×¨ × ×ª×•× ×™× ×œ×™× ××¨×™×™×, ×”××•×“×œ ××™×•×¦×’ ×¢×œ ×™×“×™:
- $f(x) = w^T x + b$
- ×›××©×¨ $w$ ×”×•× ×•×§×˜×•×¨ ×”××©×§×œ×•×ª
- x ×”×•× ×•×§×˜×•×¨ ×ª×›×•× ×•×ª ×”×§×œ×˜
- b ×”×•× ×¢×¨×š ×”×”×¡×˜×” (bias)
- ×”× ×•×¡×—× ××ª××™××” ×œ×”×¨×‘×” ××™××“×™× ×•×œ× ×¨×§ ×œ×“×•-××™××“

# ××™×š ××•×¦××™× ××ª ×•×§×˜×•×¨ ×”××©×§×œ×™× \( w \) ×‘Ö¾SVM

## ğŸ¯ ×”××˜×¨×” ×©×œ SVM
×œ××¦×•× ××ª ×”×§×• (××• ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ) ×©××¤×¨×™×“ ×”×›×™ ×˜×•×‘ ×‘×™×Ÿ ×©×ª×™ ×§×‘×•×¦×•×ª, ×›×š ×©×”××¨×—×§ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ ××›×œ ×¦×“ (×”Ö¾**support vectors**) ××œ ×”×§×• ×™×”×™×” **×”×›×™ ×’×“×•×œ ×©××¤×©×¨**.

---

## ğŸ”¢ ××™×š ××•×¦××™× ××ª \( w \)?
×”××•×“×œ ××’×“×™×¨ ×‘×¢×™×” ××ª××˜×™×ª ×©×œ **××•×¤×˜×™××™×–×¦×™×”** (××¦×™××ª ××§×¡×™××•×/××™× ×™××•×)

### 1. × ×•×¡×—×ª ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ:
$$
f(x) = w^T x + b
$$

××:

$$f(x) \geq 1$$

â†’ ×”×“×’×™××” ×©×™×™×›×ª ×œ××—×œ×§×” ×”×—×™×•×‘×™×ª (label = +1)

$$f(x) \leq -1$$ 

â†’ ×”×“×’×™××” ×©×™×™×›×ª ×œ××—×œ×§×” ×”×©×œ×™×œ×™×ª (label = -1)

---

### 2. ×ª× ××™ ×”×”×¤×¨×“×”:
×œ×›×œ ×“×•×’××” Xi Yi:

$$
y_i (w^T x_i + b) \geq 1
$$

**××” ×–×” Yi**

×”- Yi ×–×” ×”×ª×•×•×™×ª (label) ×©×œ ×”×“×•×’××” ×”Ö¾ i

×›×œ Xi ×”×•× ×•×§×˜×•×¨

×›×œ Yi ×”×•× ××¡×¤×¨ ×©××•××¨ ×œ××™×–×” ×§×‘×•×¦×” ×©×™×™×›×ª ×”×“×•×’××, ×œ×§×‘×•×¦×” ×”×—×™×•×‘×™×ª ××• ×œ×§×‘×•×¦×” ×”×©×œ×™×œ×™×ª


### 3. ×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” (Objective Function):
×›×“×™ ×œ××§×¡× ××ª ×”××¨×—×§ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª, × ××–×¢×¨ ××ª ×’×•×“×œ \( w \):

$$
\min \left( \frac{1}{2} \|w\|^2 \right)
$$

×ª×—×ª ×”×”×’×‘×œ×”:

$$
y_i (w^T x_i + b) \geq 1 \quad \forall i
$$

## â“ ×œ××” ×¦×¨×™×š ×œ×”×§×˜×™×Ÿ ××ª \( \|w\| \) ×‘Ö¾SVM?


## âœ¨ ×”×”×¡×‘×¨: ×›×œ ×”×¡×•×“ × ××¦× ×‘Ö¾**Margin** â€“ ×”××¨×•×•×— ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª

### ×”××¨×—×§ ×©×œ × ×§×•×“×” ××”××™×©×•×¨:
×œ×¤×™ ×”× ×•×¡×—×”:

$$
\text{Distance from hyperplane} = \frac{|w^T x + b|}{\|w\|}
$$

### ×”××˜×¨×” ×©×œ SVM:
×œ××¦×•× ××™×©×•×¨ ×©××¤×¨×™×“ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×¢× **×”××¨×—×§ ×”×›×™ ×’×“×•×œ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨** â€” ×›×œ×•××¨, ××¨×•×•×— (margin) ××§×¡×™××œ×™.

### ×ª× ××™ ×”×”×¤×¨×“×”:

$$
y_i(w^T x_i + b) \geq 1
$$

×”× ×§×•×“×•×ª ×”×›×™ ×§×¨×•×‘×•×ª ×œ××™×©×•×¨ ×”×Ÿ ×”Ö¾**Support Vectors**, ×©××§×™×™××•×ª:

$$
y_i(w^T x_i + b) = 1
$$

## âœ… ×”××¨×—×§ ×©×œ×”×Ÿ ××”××™×©×•×¨:

$$
\text{margin} = \frac{1}{\|w\|}
$$


### ×•×œ×›×Ÿ:
- ×›×›×œ ×©Ö¾\( \|w\| \) **×§×˜×Ÿ ×™×•×ª×¨**, ×”××¨×•×•×— **×’×“×•×œ ×™×•×ª×¨**.
- ×›×œ×•××¨: ×× × ×§×˜×™×Ÿ ××ª \( \|w\| \), ×× ×—× ×• **××¨×—×™×§×™×** ××ª ×”××™×©×•×¨ ××”× ×§×•×“×•×ª ×”×§×¨×•×‘×•×ª ×‘×™×•×ª×¨ â€” ×•×–×” ×‘×“×™×•×§ ××” ×©×× ×—× ×• ×¨×•×¦×™×!

---

## ğŸ§  ×•×œ×›×Ÿ ×‘×¤×•× ×§×¦×™×™×ª ×”××˜×¨×” ×©×œ SVM:
×× ×—× ×• **×××–×¢×¨×™×** ××ª:

$$
\frac{1}{2} \|w\|^2
$$


×›×“×™ ×œ××¦×•× ××ª ×”×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ ×¢× **margin ××§×¡×™××œ×™** ×•×œ×•×•×“× ×”×¤×¨×“×” ×˜×•×‘×” ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª.

---

### ğŸ–¼ï¸ ×”××—×©×” ×‘×¦×™×•×¨:
- ××™×©×•×¨×™× ×¢× \( \|w\| \) ×’×“×•×œ ××¤×¨×™×“×™× ×§×¨×•×‘ ××“×™ ×œ× ×§×•×“×•×ª â€“ ×¨×¢.  
- ××™×©×•×¨×™× ×¢× \( \|w\| \) ×§×˜×Ÿ ×™×•×¦×¨×™× **×¨×•×•×— ×’×“×•×œ** â€” ×˜×•×‘!



---

## ğŸ¤– ××™×š ×¤×•×ª×¨×™× ××ª ×–×” ×‘×¤×•×¢×œ?

1. ××©×ª××©×™× ×‘×©×™×˜×” ××ª××˜×™×ª ×‘×©× **Lagrange Multipliers**.
2. ×¤×•×ª×¨×™× ××ª ×”×‘×¢×™×” ×”×›×¤×•×œ×” (Dual Problem).
3. ×”×¤×ª×¨×•×Ÿ ××‘×•×¡×¡ ×¨×§ ×¢×œ ×”Ö¾**Support Vectors** (×”× ×§×•×“×•×ª ×”×›×™ ×§×¨×•×‘×•×ª ×œ×”×™×¤×¨Ö¾×¤×œ×™×™×Ÿ).
4. ××”× ××—×©×‘×™× ××ª \( w \) ×›×š:
$$
w = \sum_i \alpha_i y_i x_i
$$

×›××©×¨:
- \( \alpha_i \) ×”× ×¤×¨××˜×¨×™× ×©×§×•×‘×¢×™× ××ª ×—×©×™×‘×•×ª ×›×œ ×“×•×’××”.
- ×¨×§ ×¢×‘×•×¨ ×”Ö¾support vectors ×™×© \( \alpha_i \neq 0 \).

---

## ğŸ’¡ ×¡×™×›×•×
- ×× ×—× ×• ×œ× ××—×©×‘×™× ××ª \( w \) ×™×©×™×¨×•×ª, ××œ× ×¤×•×ª×¨×™× ×‘×¢×™×™×ª ××•×¤×˜×™××™×–×¦×™×”.
- ×”××˜×¨×” ×”×™× ×œ××¦×•× ××ª ×”×§×• ×©××¤×¨×™×“ ×”×›×™ ×˜×•×‘ ×‘×™×Ÿ ×”×§×‘×•×¦×•×ª ×¢× **margin** ××§×¡×™××œ×™.
- ×”×ª×•×¦××”: ××©×•×•××” ×©××‘×•×¡×¡×ª ×¨×§ ×¢×œ ×”Ö¾support vectors.









---
---
---


## Kernel (×’×¨×¢×™×Ÿ)
×§×¨× ×œ ×”×•× ×¤×•× ×§×¦×™×” ××ª××˜×™×ª ×”×××¤×©×¨×ª ×œ-SVM ×œ×˜×¤×œ ×‘× ×ª×•× ×™× ×œ×-×œ×™× ××¨×™×™×. ×”×•× ×××¤×©×¨ ×—×™×©×•×‘ ×©×œ ××›×¤×œ×ª ×”× ×§×•×“×•×ª ×‘××¨×—×‘ ×ª×›×•× ×•×ª ×’×‘×•×”-×××“×™ ××‘×œ×™ ×œ×—×©×‘ ×‘××¤×•×¨×© ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” ×©×œ ×•×§×˜×•×¨×™ ×”×§×œ×˜ ×œ××¨×—×‘ ×–×”

## Kernel Function (×¤×•× ×§×¦×™×™×ª ×’×¨×¢×™×Ÿ)
×¤×•× ×§×¦×™×ª ×”×§×¨× ×œ, ×”××¡×•×× ×ª ×œ×¨×•×‘ ×›-

$K(x, y)$

××—×©×‘×ª ××ª ××›×¤×œ×ª ×”× ×§×•×“×•×ª (dot product) ×©×œ ×©× ×™ ×•×§×˜×•×¨×™× x ×•-y ×œ××—×¨ ×©×¢×‘×¨×• ×˜×¨× ×¡×¤×•×¨××¦×™×” ×œ××¨×—×‘ ×ª×›×•× ×•×ª ×’×‘×•×” ×™×•×ª×¨, ××‘×œ×™ ×œ×—×©×‘ ×‘××¤×•×¨×© ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” ×¢×¦××”:

$K(x, y) = \phi(x) \cdot \phi(y)$

×›××©×¨ $\phi$ ×”×™× ×¤×•× ×§×¦×™×™×ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” ×œ××¨×—×‘ ×”×’×‘×•×” ×™×•×ª×¨

### Common types of Kernel Functions:

1. **Linear Kernel**:
   $K(x, y) = x \cdot y$

2. **Polynomial Kernel**:
   $K(x, y) = (\gamma x \cdot y + c)^d$
   where $\gamma > 0$, $c \geq 0$, and $d$ is an integer representing the degree of the polynomial.

3. **Radial Basis Function (RBF) or Gaussian Kernel**:
   $K(x, y) = \exp(-\gamma \|x - y\|^2)$
   where $\gamma > 0$, typically $\gamma = \frac{1}{2\sigma^2}$.

4. **Sigmoid Kernel**:
   $K(x, y) = \tanh(\gamma x \cdot y + c)$
   where $\gamma > 0$ and $c \geq 0$.

# Kernel Functions with Examples

**What is Î³ (Gamma)?**

Gamma is a hyperparameter that appears in several kernel functions, including the Polynomial kernel, RBF/Gaussian kernel, and Sigmoid kernel. It controls different aspects of the kernel's behavior:

**The "exp"** in the RBF/Gaussian kernel formula refers to the exponential function, which is commonly written as "exp" in mathematics and programming

The exponential function exp(x) is equivalent to e^x, where "e" is Euler's number (approximately 2.71828...), a mathematical constant that forms the base of natural logarithms

## 1. Linear Kernel
**Formula**: $K(x, y) = x \cdot y$

**Example**:
For two 2D vectors $x = [1, 2]$ and $y = [3, 4]$:

$K(x, y) = x \cdot y = 1 \times 3 + 2 \times 4 = 3 + 8 = 11$

**Use case**: Linear kernels work well when the data is already linearly separable. They're computationally efficient but cannot handle non-linear relationships in data.

## 2. Polynomial Kernel
**Formula**: $K(x, y) = (\gamma x \cdot y + c)^d$

where $\gamma > 0$, $c \geq 0$, and $d$ is the polynomial degree.

**Example**:
For vectors $x = [1, 2]$ and $y = [3, 4]$, with $\gamma = 1$, $c = 1$, and $d = 2$:

$K(x, y) = (1 \times (1 \times 3 + 2 \times 4) + 1)^2 = (11 + 1)^2 = 12^2 = 144$

**Use case**: Polynomial kernels are useful for problems where training data is not linearly separable. The degree $d$ determines the flexibility of the decision boundary. Common choices are $d = 2$ (quadratic) or $d = 3$ (cubic).

## 3. Radial Basis Function (RBF) / Gaussian Kernel
**Formula**: $K(x, y) = \exp(-\gamma \|x - y\|^2)$

where $\gamma > 0$, typically $\gamma = \frac{1}{2\sigma^2}$.

**Example**:
For vectors $x = [1, 2]$ and $y = [3, 4]$ with $\gamma = 0.5$:

1. Calculate the squared Euclidean distance: 
   $\|x - y\|^2 = (1-3)^2 + (2-4)^2 = 4 + 4 = 8$

2. Apply the RBF formula:
   $K(x, y) = \exp(-0.5 \times 8) = \exp(-4) \approx 0.018$

**Use case**: RBF kernels are versatile and work well for most types of data. They're especially effective when the relationship between classes is non-linear. The parameter $\gamma$ controls the "reach" of a single training example's influence.

## 4. Sigmoid Kernel
**Formula**: $K(x, y) = \tanh(\gamma x \cdot y + c)$

tanh = Hyperbolic Tangent 

where $\gamma > 0$ and $c \geq 0$.

**Example**:
For vectors $x = [1, 2]$ and $y = [3, 4]$, with $\gamma = 0.1$ and $c = 0$:

$K(x, y) = \tanh(0.1 \times (1 \times 3 + 2 \times 4)) = \tanh(0.1 \times 11) = \tanh(1.1) \approx 0.8$

**Use case**: The sigmoid kernel comes from neural networks (it's similar to using a neural network with one hidden layer). It's less commonly used in SVMs than RBF kernels but can be effective for specific problems.

## Choosing the Right Kernel

The choice of kernel depends on the specific problem:

- **Linear kernel**: When data is linearly separable
- **Polynomial kernel**: When you need a more flexible decision boundary with clear degree of separation
- **RBF kernel**: Most versatile, works well for most datasets when properly tuned
- **Sigmoid kernel**: Works for specific types of problems, often related to neural networks

In practice, it's common to try different kernels and use cross-validation to determine which one performs best for your specific dataset.

## Visual Intuition

To understand how kernels transform data:

1. **Linear**: Data remains in the same space, separated by a straight line
2. **Polynomial**: Data is mapped to a higher-dimensional space where curved boundaries in original space become linear boundaries
3. **RBF**: Essentially creates a "bump" around each data point, with the width controlled by $\gamma$
4. **Sigmoid**: Creates a decision boundary similar to that of a neural network

The kernel trick allows us to compute these separations without explicitly transforming the data to higher dimensions, making SVMs computationally efficient even for complex decision boundaries.

## Kernel Trick (×˜×¨×™×§ ×”×’×¨×¢×™×Ÿ)
×”-Kernel Trick ×”×•× ×”×˜×›× ×™×§×” ×©×××¤×©×¨×ª ×œ-SVM ×œ×”×ª××•×“×“ ×¢× ×‘×¢×™×•×ª ×¡×™×•×•×’ ×œ×-×œ×™× ××¨×™×•×ª ××‘×œ×™ ×œ×—×©×‘ ×‘××¤×•×¨×© ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” ×œ××¨×—×‘ ×’×‘×•×”-×××“×™. ×”×¨×¢×™×•×Ÿ ×”×‘×¡×™×¡×™ ×”×•×:

1. ×‘××§×•× ×œ×”×¤×¢×™×œ ×˜×¨× ×¡×¤×•×¨××¦×™×” $\phi$ ×¢×œ ×›×œ ×•×§×˜×•×¨ ×§×œ×˜ $x$ ×•-$y$ ×‘× ×¤×¨×“
2. ×•××– ×œ×—×©×‘ ××ª ××›×¤×œ×ª ×”× ×§×•×“×•×ª ×©×œ×”× $\phi(x) \cdot \phi(y)$
3. ×× ×—× ×• ××—×©×‘×™× ×™×©×™×¨×•×ª ××ª $K(x, y)$ ×©× ×•×ª×Ÿ ××ª ××•×ª×” ×ª×•×¦××”

×–×” ×—×•×¡×š ×–××Ÿ ×—×™×©×•×‘ ××©××¢×•×ª×™, ×‘××™×•×—×“ ×›××©×¨ ××¨×—×‘ ×”×ª×›×•× ×•×ª ×”×’×‘×•×”-×××“×™ ×™×›×•×œ ×œ×”×™×•×ª ××™× ×¡×•×¤×™ (×›××• ×‘-RBF Kernel).

### ×”×™×ª×¨×•× ×•×ª ×©×œ Kernel Trick:
- ×××¤×©×¨ ×œ-SVM ×œ×”×ª××•×“×“ ×¢× × ×ª×•× ×™× ×œ× ×œ×™× ××¨×™×™×
- ×—×•×¡×š ×‘×–××Ÿ ×—×™×©×•×‘ ×•×‘×©×™××•×© ×‘×–×™×›×¨×•×Ÿ
- ×××¤×©×¨ ×¢×‘×•×“×” ×‘××¨×—×‘×™ ×ª×›×•× ×•×ª ××™× ×¡×•×¤×™×™×
- ××©×¤×¨ ××ª ×”×“×™×•×§ ×‘×‘×¢×™×•×ª ×¡×™×•×•×’ ××•×¨×›×‘×•×ª

### ×“×•×’××” ×¤×©×•×˜×”:
×‘×•××• × ×ª××¨ ××§×¨×” ×©×œ × ×ª×•× ×™× ×©×œ× × ×™×ª× ×™× ×œ×”×¤×¨×“×” ×œ×™× ××¨×™×ª, ×›××• ×‘×¢×™×™×ª XOR (××• ×‘×¢×™×™×ª ×”××¢×’×œ - × ×§×•×“×•×ª ×‘×ª×•×š ××¢×’×œ ××•×œ × ×§×•×“×•×ª ××—×•×¥ ×œ××¢×’×œ):

1. ×‘××¨×—×‘ ×”××§×•×¨×™ (×”×“×•-×××“×™) ××™×Ÿ ××¤×©×¨×•×ª ×œ××¦×•× ×§×• ×™×©×¨ ×©×™×¤×¨×™×“ ×‘×™×Ÿ ×©×ª×™ ×”×§×‘×•×¦×•×ª
2. ×× × ×©×ª××© ×‘×˜×¨× ×¡×¤×•×¨××¦×™×” ×›××• $\phi(x,y) = (x, y, x^2 + y^2)$, ×× ×—× ×• ×××¤×™× ××ª ×”× ×ª×•× ×™× ×œ××¨×—×‘ ×ª×œ×ª-×××“×™
3. ×‘××¨×—×‘ ×”×ª×œ×ª-×××“×™ ×”×–×”, × ×™×ª×Ÿ ×œ×”×¤×¨×™×“ ××ª ×”× ×ª×•× ×™× ×‘×××¦×¢×•×ª ××™×©×•×¨ (×”×™×¤×¨×¤×œ×Ÿ)
4. ×‘××§×•× ×œ×—×©×‘ ×‘××¤×•×¨×© ××ª ×”×˜×¨× ×¡×¤×•×¨××¦×™×” ×”×–×•, ×× ×—× ×• ×™×›×•×œ×™× ×œ×”×©×ª××© ×‘-Kernel (×œ×“×•×’××” Gaussian Kernel) ×©××©×™×’ ××ª ××•×ª×” ×ª×•×¦××”

×”-Kernel Trick ×××¤×©×¨ ×œ× ×• ×œ×¢×‘×•×“ ×¢× ××¨×—×‘×™× ×‘×¢×œ×™ ×××“×™× ×’×‘×•×”×™×, ×œ×¢×ª×™× ××¤×™×œ×• ××™× ×¡×•×¤×™×™×, ××‘×œ×™ ×œ×©×œ× ××ª ×”××—×™×¨ ×”×—×™×©×•×‘×™ ×©×œ ×¢×‘×•×“×” ×‘××¨×—×‘×™× ××œ×•.

